[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Geocomputation with Python",
    "section": "Welcome",
    "text": "Welcome\nThis is the online home of Geocomputation with Python, a book on reproducible geographic data analysis with open source software.\nInspired by the Free and Open Source Software for Geospatial (FOSS4G) movement this is an open source book. Find the code underlying the geocompy project on GitHub, ensuring that the content is reproducible, transparent, and accessible. Making the book open source allows you or anyone else, to interact with the project by opening issues, making typo fixes and more, for the benefit of everyone.\nThe book’s website is built by GitHub Actions, which runs the code every time we make a change, ensuring code correctness and reproducibility. The current build status as follows:\n\n\n\n\n\nYou can run the code in the book using GitHub CodeSpaces as follows (requires a GitHub account):\n\n\n\n\n\nFor details on reproducing the book, see the README in the project’s GitHub repo: https://github.com/geocompx/geocompy."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Geocomputation with Python (geocompy) is motivated by the need for an introductory, yet rigorous and up-to-date, resource geographic data with the most popular programming language in the world. A unique selling point of the book is its cohesive and joined-up coverage of both vector and raster geographic data models and consistent learning curve. We aim to minimize surprises, with each section and chapter building on the previous. If you’re just starting out with Python for working with geographic data, this book is an excellent place to start.\nThere are many resources on Python on ‘GeoPython’ but none that fill this need for an introductory resource that provides strong foundations for future work. We want to avoid reinventing the wheel and provide something that fills an ‘ecological niche’ in the wider free and open source software for geospatial (FOSS4G) ecosystem. Key features include:\n\nDoing basic operations well\nIntegration of vector and raster datasets and operations\nClear explanation of each line of code in the book to minimize surprises\nExcercises at the end of each chapter with reproducible and open solutions\nProvision of lucid example datasets and meaningful operations to illustrate the applied nature of geographic research\n\nThis book is complementary with, and adds value to, other projects in the ecosystem, as highlighted in the following comparison between Geocomputation with Python and related GeoPython books:\n\nLearning Geospatial Analysis with Python and Geoprocessing with Python are books in this space that focus on processing spatial data using low-level Python interfaces for GDAL, such as the gdal, gdalnumeric, and ogr packages from osgeo. This approach requires writing more lines of code. We believe our approach is more “Pythonic” and future-proof, in light of development of packages such as geopandas and rasterio.\nIntroduction to Python for Geographic Data Analysis (in progress) seeks to provide a general introduction to ‘GIS in Python’, with parts focusing on Python essentials, using Python with GIS, and case studies. Compared with this book, which is also open source, and is hosted at pythongis.org, Geocomputation with Python has a narrower scope (not covering spatial network analysis, for example) and more coverage of raster data processing and raster-vector interoperability.\nGeographic Data Science with Python is an ambitious project with chapters dedicated to advanced topics, with Chapter 4 on Spatial Weights getting into complex topics relatively early, for example.\nPython for Geospatial Data Analysis introduces a wide range of approaches to working with geospatial data using Python, including automation of proprietary and open-source GIS software, as well as standalone open source Python packages (which is what we focus on and explain comprehensively in our book). Geocompy is shorter, simpler and more introductory, and cover raster and vector data with equal importance (1 to 4).\n\nAnother unique feature of the book is that it is part of a wider community. Geocomputation with Python is a sister project of Geocomputation with R, a book on geographic data analysis, visualization, and modeling using the R programming language that has 60+ contributors and an active community, not least in the associated Discord group. Links with the vibrant ‘R-spatial’ community, and other communities such as GeoRust and JuliaGeo, will lead to many opportunities for mutual benefit across open source ecosystems."
  },
  {
    "objectID": "02-spatial-data.html#introduction",
    "href": "02-spatial-data.html#introduction",
    "title": "1  Geographic data in Python",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThis chapter introduces key Python packages and data structures for working with the two major types of spatial data, namely:\n\nshapely and geopandas — for working with vector layers\nrasterio and xarray — for working with rasters\n\nAs we will see in the example code presented later in this chapter, shapely and geopandas are related:\n\nshapely is a “low-level” package for working with individual vector geometry objects\ngeopandas is a “high-level” package for working with geometry columns (GeoSeries objects), which internally contain shapely geometries, and vector layers (GeoDataFrame objects)\n\nThe geopandas ecosystem provides a comprehensive approach for working with vector layers in Python, with many packages building on it. This is not the case for raster data, however: there are several partially overlapping packages for working with raster data, each with its own advantages and disadvantages. In this book we focus on the most prominent one:\n\nrasterio — a spatial-oriented package, focused on “simple” raster formats (such as GeoTIFF), representing a raster using a combination of a numpy array, and a metadata object (dict) specifying the spatial referencing of the array\n\nAnother raster-related package worth mentioning is xarray. It is a general-purpose package for working with labeled arrays, thus advantageous for processing “complex” raster format (such as NetCDF), representing a raster using its own native classes, namely xarray.Dataset and xarray.DataArray\nThis chapter will briefly explain the fundamental geographic data models: vector and raster. Before demonstrating their implementation in Python, we will introduce the theory behind each data model and the disciplines in which they predominate.\nThe vector data model represents the world using points, lines, and polygons. These have discrete, well-defined borders, meaning that vector datasets usually have a high level of precision (but not necessarily accuracy).  The raster data model divides the surface up into cells of constant size. Raster datasets are the basis of background images used in web-mapping and have been a vital source of geographic data since the origins of aerial photography and satellite-based remote sensing devices. Rasters aggregate spatially specific features to a given resolution, meaning that they are consistent over space and scalable (many worldwide raster datasets are available).\nWhich to use? The answer likely depends on your domain of application, and the datasets you have access to:\n\nVector datasets and methods dominate the social sciences because human settlements and and processes (e.g. transport infrastructure) tend to have discrete borders\nRaster datasets and methods dominate many environmental sciences because of the reliance on remote sensing data\n\nThere is much overlap in some fields and raster and vector datasets can be used together: ecologists and demographers, for example, commonly use both vector and raster data. Furthermore, it is possible to convert between the two forms  Whether your work involves more use of vector or raster datasets, it is worth understanding the underlying data models before using them, as discussed in subsequent chapters. This book focusses on approaches that build on geopandas and rasterio packages to work with vector data and raster datasets, respectively."
  },
  {
    "objectID": "02-spatial-data.html#sec-vector-data",
    "href": "02-spatial-data.html#sec-vector-data",
    "title": "1  Geographic data in Python",
    "section": "1.2 Vector data",
    "text": "1.2 Vector data\nThe geographic vector data model is based on points located within a coordinate reference system (CRS). Points can represent self-standing features (e.g., the location of a bus stop), or they can be linked together to form more complex geometries such as lines and polygons. Most point geometries contain only two dimensions (3-dimensional CRSs contain an additional \\(z\\) value, typically representing height above sea level).\nIn this system, London, for example, can be represented by the coordinates (-0.1, 51.5). This means that its location is -0.1 degrees east and 51.5 degrees north of the origin. The origin, in this case, is at 0 degrees longitude (the Prime Meridian) and 0 degree latitude (the Equator) in a geographic (‘lon/lat’) CRS.  The same point could also be approximated in a projected CRS with ‘Easting/Northing’ values of (530000, 180000) in the British National Grid, meaning that London is located 530 \\(km\\) East and 180 \\(km\\) North of the origin of the CRS.   The location of National Grid’s origin, in the sea beyond South West Peninsular, ensures that most locations in the UK have positive Easting and Northing values.\n\ngeopandas provides classes for geographic vector data and a consistent command-line interface for reproducible geographic data analysis in Python. geopandas provides an interface to three mature libraries for geocomputation which, in combination, represent a strong foundation on which many geographic applications (including QGIS and R’s spatial ecosystem) builds:\n\nGDAL, for reading, writing, and manipulating a wide range of geographic data formats, covered in Chapter 8\nPROJ, a powerful library for coordinate system transformations, which underlies the content covered in Chapter 7\nGEOS, a planar geometry engine for operations such as calculating buffers and centroids on data with a projected CRS, covered in Chapter 5\n\nTight integration with these geographic libraries makes reproducible geocomputation possible: an advantage of using a higher level language such as Python to access these libraries is that you do not need to know the intricacies of the low level components, enabling focus on the methods rather than the implementation. This section introduces geopandas classes in preparation for subsequent chapters (Chapters 5 and 8 cover the GEOS and GDAL interface, respectively).\n\n1.2.1 Vector data classes\nThe main classes for working with geographic vector data in Python are hierarchical, meaning the highest level ‘vector layer’ class is composed of simpler ‘geometry column’ and individual ‘geometry’ components. This section introduces them in order, starting with the highest level class. For many applications, the high level vector layer class, which are essentially a data frame with geometry columns, are all that’s needed. However, it’s important to understand the structure of vector geographic objects and their component pieces for more advanced applications. The three main vector geographic data classes in Python are:\n\nGeoDataFrame, a class representing vector layers, with a geometry column (class GeoSeries) as one of the columns\nGeoSeries, a class that is used to represent the geometry column in GeoDataFrame objects\nshapely geometry objects which represent individual geometries, such as a point or a polygon\n\nThe first two classes (GeoDataFrame and GeoSeries) are defined in geopandas. The third class is defined in the shapely package, which deals with individual geometries, and is a main dependency of the geopandas package.\n\n\n1.2.2 Vector layers\nThe most commonly used geographic vector data structure is the vector layer. There are several approaches for working with vector layers in Python, ranging from low-level packages (e.g., osgeo, fiona) to the relatively high-level geopandas package that is the focus of this section. Before writing and running code for creating and working with geographic vector objects, we therefore import geopandas (by convention as gpd for more concise code) and shapely:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nimport shapely.geometry\nimport shapely.wkt\n\nWe also limit the maximum number of printed rows to four, to save space, using the 'display.max_rows' option of pandas:\n\npd.set_option('display.max_rows', 4)\n\nProjects often start by importing an existing vector layer saved as an ESRI Shapefile (.shp), a GeoPackage (.gpkg) file, or other geographic file format. The function read_file() in the following line of code imports a GeoPackage file named world.gpkg located in the data directory of Python’s working directory into a GeoDataFrame named gdf:\n\ngdf = gpd.read_file('data/world.gpkg')\n\nAs result is an object of type (class) GeoDataFrame with 177 rows (features) and 11 columns, as shown in the output of the following code:\n\ntype(gdf)\ngdf.shape\n\n(177, 11)\n\n\nThe GeoDataFrame class is an extension of the DataFrame class from the popular pandas package. This means we can treat a vector layer as a table, and process it using the ordinary, i.e., non-spatial, established function methods. For example, standard data frame subsetting methods can be used. The code below creates a subset of the gdf dataset containing only the country name and the geometry:\n\ngdf = gdf[['name_long', 'geometry']]\ngdf\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n    \n  \n  \n    \n      0\n      Fiji\n      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n    \n    \n      1\n      Tanzania\n      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      175\n      Trinidad and Tobago\n      MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n    \n    \n      176\n      South Sudan\n      MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n    \n  \n\n177 rows × 2 columns\n\n\n\nThe following expression creates a subset based on a condition, such as equality of the value in the 'name_long' column to the string 'Egypt':\n\ngdf[gdf['name_long'] == 'Egypt']\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n    \n  \n  \n    \n      163\n      Egypt\n      MULTIPOLYGON (((36.86623 22.00000, 36.69069 22...\n    \n  \n\n\n\n\nFinally, to get a sense of the spatial component of the vector layer, it can be plotted using the .plot method, as follows:\n\ngdf.plot();\n\n\n\n\nor using .explore to get an interactive plot:\n\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nAnd consequently, a subset can be plotted using:\n\ngdf[gdf['name_long'] == 'Egypt'].explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n1.2.3 Geometry columns\nA vital column in a GeoDataFrame is the geometry column, of class GeoSeries. The geometry column contains the geometric part of the vector layer. In the case of the gdf object, the geometry column contains 'MultiPolygon's associated with each country:\n\ngdf['geometry']\n\n0      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n1      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n                             ...                        \n175    MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n176    MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\nName: geometry, Length: 177, dtype: geometry\n\n\nThe geometry column also contains the spatial reference information, if any:\n\ngdf['geometry'].crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nMany geometry operations, such as calculating the centroid, buffer, or bounding box of each feature involve just the geometry. Applying this type of operation on a GeoDataFrame is therefore basically a shortcut to applying it on the GeoSeries object in the geometry column. The two following commands therefore return exactly the same result, a GeoSeries with country centroids:\n\ngdf.centroid\n\n/tmp/ipykernel_177/2017122361.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.centroid\n\n\n0      POINT (163.85312 -17.31631)\n1        POINT (34.75299 -6.25773)\n                  ...             \n175     POINT (-61.33037 10.42824)\n176       POINT (30.19862 7.29289)\nLength: 177, dtype: geometry\n\n\n\ngdf['geometry'].centroid\n\n/tmp/ipykernel_177/3996546279.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf['geometry'].centroid\n\n\n0      POINT (163.85312 -17.31631)\n1        POINT (34.75299 -6.25773)\n                  ...             \n175     POINT (-61.33037 10.42824)\n176       POINT (30.19862 7.29289)\nLength: 177, dtype: geometry\n\n\nNote that .centroid, and other similar operators in geopandas such as .buffer (Section 4.3.3) or .convex_hull, return only the geometry (i.e., a GeoSeries), not a GeoDataFrame with the original attribute data. In case we want the latter, we can create a copy of the GeoDataFrame and then “overwrite” its geometry (or, we can overwrite the geometries directly in case we don’t need the original ones, as in gdf['geometry']=gdf.centroid):\n\ngdf2 = gdf.copy()\ngdf2['geometry'] = gdf.centroid\ngdf2\n\n/tmp/ipykernel_177/4249144945.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf2['geometry'] = gdf.centroid\n\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n    \n  \n  \n    \n      0\n      Fiji\n      POINT (163.85312 -17.31631)\n    \n    \n      1\n      Tanzania\n      POINT (34.75299 -6.25773)\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      175\n      Trinidad and Tobago\n      POINT (-61.33037 10.42824)\n    \n    \n      176\n      South Sudan\n      POINT (30.19862 7.29289)\n    \n  \n\n177 rows × 2 columns\n\n\n\nAnother useful property of the geometry column is the geometry type, as shown in the following code. Note that the types of geometries contained in a geometry column (and, thus, a vector layer) are not necessarily the same for every row. Accordingly, the .type property returns a Series (of type string), rather than a single value:\n\ngdf['geometry'].type\n\n0      MultiPolygon\n1      MultiPolygon\n           ...     \n175    MultiPolygon\n176    MultiPolygon\nLength: 177, dtype: object\n\n\nTo summarize the occurrence of different geometry types in a geometry column, we can use the pandas method called value_counts:\n\ngdf['geometry'].type.value_counts()\n\nMultiPolygon    177\ndtype: int64\n\n\nIn this case, we see that the gdf layer contains only 'MultiPolygon' geometries. It is possible to have multiple geometry types in a single GeoSeries and a GeoDataFrame can have multiple GeoSeries:\n\ngdf['centroids'] = gdf.centroid\ngdf['polygons'] = gdf.geometry\ngdf\n\n/tmp/ipykernel_177/104973583.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf['centroids'] = gdf.centroid\n\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n      centroids\n      polygons\n    \n  \n  \n    \n      0\n      Fiji\n      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n      POINT (163.85312 -17.31631)\n      MULTIPOLYGON (((-180.00000 -16.55522, -179.917...\n    \n    \n      1\n      Tanzania\n      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n      POINT (34.75299 -6.25773)\n      MULTIPOLYGON (((33.90371 -0.95000, 31.86617 -1...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      175\n      Trinidad and Tobago\n      MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n      POINT (-61.33037 10.42824)\n      MULTIPOLYGON (((-61.68000 10.76000, -61.66000 ...\n    \n    \n      176\n      South Sudan\n      MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n      POINT (30.19862 7.29289)\n      MULTIPOLYGON (((30.83385 3.50917, 31.24556 3.7...\n    \n  \n\n177 rows × 4 columns\n\n\n\nTo switch the geometry column from one `GeoSeries column to another, we use set_geometry:\n\ngdf.set_geometry('centroids', inplace=True)\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\ngdf.set_geometry('polygons', inplace=True)\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n1.2.4 The Simple Features standard\nGeometries are the basic building blocks of vector layers. Although the Simple Features standard defines about 20 types of geometries, we will focus on the seven most commonly used types: POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION. Find the whole list of possible feature types in the PostGIS manual. \nWell-known binary (WKB) and well-known text (WKT) are the standard encodings for simple feature geometries.  WKB representations are usually hexadecimal strings easily readable for computers. This is why GIS and spatial databases use WKB to transfer and store geometry objects. WKT, on the other hand, is a human-readable text markup description of simple features. Both formats are exchangeable, and if we present one, we will naturally choose the WKT representation.\nThe foundation of each geometry type is the point. A point is simply a coordinate in 2D, 3D, or 4D space such as: \nPOINT (5 2)\nA linestring is a sequence of points with a straight line connecting the points, for example: \nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\nA polygon is a sequence of points that form a closed, non-intersecting ring. Closed means that the first and the last point of a polygon have the same coordinates (see right panel in Figure …).\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\nSo far we have created geometries with only one geometric entity per feature. However, the Simple Features standard allows multiple geometries to exist within a single feature, using “multi” versions of each geometry type, as illustrated below:\nMULTIPOINT (5 2, 1 3, 3 4, 3 2)\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\nMULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2)))\n\nFinally, a geometry collection can contain any combination of geometries including (multi)points and linestrings: \nGEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))\n\n\n1.2.5 Geometries\nEach element in the geometry column is a geometry object, of class shapely. For example, here is one specific geometry selected by implicit index (that of Canada):\n\ngdf['geometry'].iloc[3]\n\n\n\n\nWe can also select a specific geometry based on the 'name_long' attribute:\n\ngdf[gdf['name_long'] == 'Egypt']['geometry'].iloc[0]\n\n\n\n\nThe shapely package is compatible with the Simple Features standard. Accordingly, seven types of geometries are supported. The following section demonstrates creating a shapely geometry of each type from scratch. In the first example (a 'Point') we demonstrate two types of inputs for geometry creation:\n\na list of coordinates\na string in the WKT format and\n\nIn the examples for the remaining geometries we use the former approach.\nCreating a 'Point' geometry from a list of coordinates uses the shapely.geometry.Point function:\n\npoint = shapely.geometry.Point([5, 2])\npoint\n\n\n\n\nAlternatively, we can use the shapely.wkt.loads (stands for “load a WKT string”) to transform a WKT string to a shapely geometry object. Here is an example of creating the same 'Point' geometry from WKT:\n\npoint = shapely.wkt.loads('POINT (5 2)')\npoint\n\n\n\n\nHere is an example of a 'MultiPoint' geometry from a list of coordinate tuples:\n\nmultipoint = shapely.geometry.MultiPoint([(5,2), (1,3), (3,4), (3,2)])\nmultipoint\n\n\n\n\nHere is an example of a 'LineString' geometry from a list of coordinate tuples:\n\nlinestring = shapely.geometry.LineString([(1,5), (4,4), (4,1), (2,2), (3,2)])\nlinestring\n\n\n\n\nHere is an example of a 'MultiLineString' geometry. Note that there is one list of coordinates for each line in the MultiLineString:\n\nlinestring = shapely.geometry.MultiLineString([[(1,5), (4,4), (4,1), (2,2), (3,2)], [(1,2), (2,4)]])\nlinestring\n\n\n\n\nHere is an example of a 'Polygon' geometry. Note that there is one list of coordinates that defines the exterior outer hull of the polygon, followed by a list of lists of coordinates that define the potential holes in the polygon:\n\npolygon = shapely.geometry.Polygon([(1,5), (2,2), (4,1), (4,4), (1,5)], [[(2,4), (3,4), (3,3), (2,3), (2,4)]])\npolygon\n\n\n\n\nHere is an example of a 'MultiPolygon' geometry:\n\nmultipolygon = shapely.geometry.MultiPolygon([\n    shapely.geometry.Polygon([(1,5), (2,2), (4,1), (4,4), (1,5)]), \n    shapely.geometry.Polygon([(0,2), (1,2), (1,3), (0,3), (0,2)])\n])\nmultipolygon\n\n\n\n\nAnd, finally, here is an example of a 'GeometryCollection' geometry:\n\nmultipoint = shapely.geometry.GeometryCollection([\n    shapely.geometry.MultiPoint([(5,2), (1,3), (3,4), (3,2)]),\n    shapely.geometry.MultiLineString([[(1,5), (4,4), (4,1), (2,2), (3,2)], [(1,2), (2,4)]])\n])\nmultipoint\n\n\n\n\nshapely geometries act as atomic units of vector data, as spatial operations on a geometry return a single new geometry. For example, the following expression calculates the difference between the buffered multipolygon (using distance of 0.2) and itself:\n\nmultipolygon.buffer(0.2).difference(multipolygon)\n\n\n\n\nAs demonstrated above, a shapely geometry object is automatically evaluated to a small image of the geometry (when using an interface capable of displaying it, such as a Jupyter Notebook). To print the WKT string instead, we can use the print function:\n\nprint(linestring)\n\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\n\n\nWe can determine the geometry type using the .geom_type property, which returns a string:\n\nlinestring.geom_type\n\n'MultiLineString'\n\n\nFinally, it is important to note that raw coordinates of shapely geometries are accessible through a combination of the .coords, .geoms, .exterior, and .interiors, properties (depending on the geometry type). These access methods are helpful when we need to develop our own spatial operators for specific tasks. For example, the following expression returns the list of all coordinates of the polygon geometry exterior:\n\nlist(polygon.exterior.coords)\n\n[(1.0, 5.0), (2.0, 2.0), (4.0, 1.0), (4.0, 4.0), (1.0, 5.0)]\n\n\n\n\n1.2.6 Vector layer from scratch\nIn the previous sections we started with a vector layer (GeoDataFrame), from an existing Shapefile, and “decomposed” it to extract the geometry column (GeoSeries, Section 1.2.3) and separate geometries (shapely, see Section 1.2.5). In this section, we will demonstrate the opposite process, constructing a GeoDataFrame from shapely geometries, combined into a GeoSeries. This will:\n\nHelp you better understand the structure of a GeoDataFrame, and\nMay come in handy when you need to programmatically construct simple vector layers, such as a line between two given points, etc.\n\nVector layers consist of two main parts: geometries and non-geographic attributes. Figure 1.1 shows how a GeoDataFrame object is created—geometries come from a GeoSeries object (which consists of shapely geometries), while attributes are taken from Series objects.\n\n\n\nFigure 1.1: Creating a GeoDataFrame from scratch\n\n\nThe final result, a vector layer (GeoDataFrame) is therefore a hierarchical structure (Figure 1.2), containing the geometry column (GeoSeries), which in turn contains geometries (shapely). Each of the “internal” components can be accessed, or “extracted”, which is sometimes necessary, as we will see later on.\n\n\n\nFigure 1.2: Structure of a GeoDataFrame\n\n\nNon-geographic attributes represent the name of the feature or other attributes such as measured values, groups, and other things. To illustrate attributes, we will represent a temperature of 25°C in London on June 21st, 2017. This example contains a geometry (the coordinates), and three attributes with three different classes (place name, temperature and date). Objects of class GeoDataFrame represent such data by combining the attributes (Series) with the simple feature geometry column (GeoSeries). First, we create a point geometry, which we know how to do from Section 1.2.5:\n\nlnd_point = shapely.geometry.Point(0.1, 51.5)\nlnd_point\n\n\n\n\nNext, we create a GeoSeries (of length 1), containing the point. Note that a GeoSeries stores a CRS definition, in this case WGS84 (defined using its EPSG code 4326). Also note that the shapely geometries go into a list, to illustrate that there can be more than one (unlike in this example):\n\nlnd_geom = gpd.GeoSeries([lnd_point], crs=4326)\nlnd_geom\n\n0    POINT (0.10000 51.50000)\ndtype: geometry\n\n\nNext, we combine the GeoSeries with other attributes into a dict. The geometry column is a GeoSeries, named geometry. The other attributes (if any) may be defined using list or Series objects. Here, for simplicity, we use the list option for defining the three attributes name, temperature, and date. Again, note that the list can be of length >1, in case we are creating a layer with more than one feature:\n\nd = {\n  'name': ['London'],\n  'temperature': [25],\n  'date': ['2017-06-21'],\n  'geometry': lnd_geom\n}\n\nFinally, the dict can be coverted to a GeoDataFrame:\n\nlnd_layer = gpd.GeoDataFrame(d)\nlnd_layer\n\n\n\n\n\n  \n    \n      \n      name\n      temperature\n      date\n      geometry\n    \n  \n  \n    \n      0\n      London\n      25\n      2017-06-21\n      POINT (0.10000 51.50000)\n    \n  \n\n\n\n\nWhat just happened? First, the coordinates were used to create the simple feature geometry (shapely). Second, the geometry was converted into a simple feature geometry column (GeoSeries), with a CRS. Third, attributes were combined with GeoSeries. This results in an GeoDataFrame object, named lnd_layer.\nJust to illustrate how does creating a layer with more than one feature looks like, here is an example where we create a layer with two points, London and Paris:\n\nlnd_point = shapely.geometry.Point(0.1, 51.5)\nparis_point = shapely.geometry.Point(2.3, 48.9)\ntowns_geom = gpd.GeoSeries([lnd_point, paris_point], crs=4326)\nd = {\n  'name': ['London', 'Paris'],\n  'temperature': [25, 27],\n  'date': ['2017-06-21', '2017-06-21'],\n  'geometry': towns_geom\n}\ntowns_layer = gpd.GeoDataFrame(d)\ntowns_layer\n\n\n\n\n\n  \n    \n      \n      name\n      temperature\n      date\n      geometry\n    \n  \n  \n    \n      0\n      London\n      25\n      2017-06-21\n      POINT (0.10000 51.50000)\n    \n    \n      1\n      Paris\n      27\n      2017-06-21\n      POINT (2.30000 48.90000)\n    \n  \n\n\n\n\nThe following expression creates an interactive map with the result:\n\ntowns_layer.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nAlternatively, we can first create a pandas.DataFrame and turn it into a GeoDataFrame like this:\n\ntowns_table = pd.DataFrame({\n  'name': ['London', 'Paris'],\n  'temperature': [25, 27],\n  'date': ['2017-06-21', '2017-06-21'],\n  'x': [0.1, 2.3],\n  'y': [51.5, 48.9]\n})\ntowns_geom = gpd.points_from_xy(towns_table['x'], towns_table['y'])\ntowns_layer = gpd.GeoDataFrame(towns_table, geometry=towns_geom, crs=4326)\n\nwhich gives the same result towns_layer:\n\ntowns_layer\n\n\n\n\n\n  \n    \n      \n      name\n      temperature\n      date\n      x\n      y\n      geometry\n    \n  \n  \n    \n      0\n      London\n      25\n      2017-06-21\n      0.1\n      51.5\n      POINT (0.10000 51.50000)\n    \n    \n      1\n      Paris\n      27\n      2017-06-21\n      2.3\n      48.9\n      POINT (2.30000 48.90000)\n    \n  \n\n\n\n\nThis approach is particularly useful when we need to read data from a CSV file, e.g., using pandas.read_csv, and want to turn the resulting DataFrame into a GeoDataFrame.\n\n\n1.2.7 Derived numeric properties\nVector layers are characterized by two essential derived numeric properties:\n\nLength (.length)—Applicable to lines\nArea (.area)—Applicable to polygons\n\nArea and length can be calculated for any data structures discussed above, either a shapely geometry, in which case the returned value is a number:\n\nlinestring.length\n\n11.63441361516796\n\n\n\nmultipolygon.area\n\n8.0\n\n\nor for GeoSeries or DataFrame, in which case the returned value is a numeric Series:\n\ngdf.area\n\n/tmp/ipykernel_177/138307179.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.area\n\n\n0       1.639506\n1      76.301964\n         ...    \n175     0.639000\n176    51.196106\nLength: 177, dtype: float64\n\n\nLike all numeric calculations in geopandas, the results assume a planar CRS and are returned in its native units. This means that length and area measurements for geometries in WGS84 (crs=4326) are returned in decimal degrees and essentially meaningless, thus the warning in the above command.\nTo obtain true length and area measurements, the geometries first need to be transformed to a projected CRS (see Section 6.8) applicable to the area of interest. For example, the area of Slovenia can be calculated in the UTM zone 33N CRS (crs=32633). The result is in the CRS units, namely \\(m^2\\):\n\ngdf[gdf['name_long'] == 'Slovenia'].to_crs(32633).area\n\n150    1.910410e+10\ndtype: float64"
  },
  {
    "objectID": "02-spatial-data.html#raster-data",
    "href": "02-spatial-data.html#raster-data",
    "title": "1  Geographic data in Python",
    "section": "1.3 Raster data",
    "text": "1.3 Raster data\n\n1.3.1 Introduction\nAs mentioned above, working with rasters in Python is less organized around one comprehensive package (compared to the case for vector layers and geopandas). Instead, several packages provide alternative subsets of method for working with raster data.\nThe two most notable approaches for working with rasters in Python are provided by the rasterio and xarray packages. As we will see shortly, they differ in their scope and underlying data models. Specifically, rasterio represents rasters as numpy arrays associated with a separate object holding the spatial metadata. The xarray package, however, represents rasters with the native DataArray object, which is an extension of numpy array designed to hold axis labels and attributes, in the same object, together with the array of raster values.\nBoth packages are not exhaustive in the same way geopandas is. For example, when working with rasterio, on the one hand, more packages may be needed to accomplish common tasks such as zonal statistics (package rasterstats) or calculating topographic indices (package richdem). On the other hand, xarray was extended to accommodate spatial operators missing from the core package itself, with the rioxarray and xarray-spatial packages.\nIn the following two sections, we introduce rasterio, which is the raster-related package we are going to work with through the rest of the book.\n\n\n1.3.2 Using rasterio\nTo work with the rasterio package, we first need to import it. We also import numpy, since the underlying raster data are stored in numpy arrays. To effectively work with those, we expose all numpy functions. Finally, we import the show function from the rasterio.plot sub-module for quick visualization of rasters.\n\nimport numpy as np\nimport rasterio\nfrom rasterio.plot import show\nimport subprocess\n\nRasters are typically imported from existing files. When working with rasterio, importing a raster is actually a two-step process:\n\nFirst, we open a raster file “connection” using rasterio.open\nSecond, we read raster values from the connection using the .read method\n\nThis separation is analogous to basic Python functions for reading from files, such as open and .readline to read from a text file. The rationale is that we do not always want to read all information from the file into memory, which is particularly important as rasters size can be larger than RAM size. Accordingly, the second step (.read) is selective. For example, we may want to read just one raster band rather than reading all bands.\nIn the first step, we pass a file path to the rasterio.open function to create a DatasetReader file connection. For this example, we use a single-band raster representing elevation in Zion National Park:\n\nsrc = rasterio.open('data/srtm.tif')\nsrc\n\n<open DatasetReader name='data/srtm.tif' mode='r'>\n\n\nTo get a first impression of the raster values, we can plot the raster using the show function:\n\nshow(src);\n\n\n\n\nThe DatasetReader contains the raster metadata, that is, all of the information other than the raster values. Let us examine it:\n\nsrc.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 65535.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nImportantly, we can see:\n\nThe raster data type (dtype)\nRaster dimensions (width, height, and count, i.e., number of layers)\nRaster Coordinate Reference System (crs)\nThe raster affine transformation matrix (transform)\n\nThe last item (i.e., transform) deserves more attention. To position a raster in geographical space, in addition to the CRS we must specify the raster origin (\\(x_{min}\\), \\(y_{max}\\)) and resolution (\\(delta_{x}\\), \\(delta_{y}\\)). In the transform matrix notation, these data items are stored as follows:\nAffine(delta_x, 0.0, x_min,\n       0.0, delta_y, y_max)\nNote that, by convention, raster y-axis origin is set to the maximum value (\\(y_{max}\\)) rather than the minimum, and, accordingly, the y-axis resolution (\\(delta_{y}\\)) is negative.\nFinally, the .read method of the DatasetReader is used to read the actual raster values. Importantly, we can read:\n\nA particular layer, passing a numeric index (as in .read(1))\nA subset of layers, passing a list of indices (as in .read([1,2]))\nAll layers (as in .read())\n\nNote that the layer indices start from 1, contrary to the Python convention of the first index being 0.\nThe resulting object is a numpy array, with either two or three dimensions:\n\nThree dimensions, when reading more than one layer (e.g., .read() or .read([1,2])). In such case, the dimensions pattern is (layers, rows, columns)\nTwo dimensions, when reading one specific layer (e.g., .read(1))\n\nLet’s read the first (and only) layer from the srtm.tif raster, using the file connection object src, for example:\n\nsrc.read(1)\n\narray([[1728, 1718, 1715, ..., 2654, 2674, 2685],\n       [1737, 1727, 1717, ..., 2649, 2677, 2693],\n       [1739, 1734, 1727, ..., 2644, 2672, 2695],\n       ...,\n       [1326, 1328, 1329, ..., 1777, 1778, 1775],\n       [1320, 1323, 1326, ..., 1771, 1770, 1772],\n       [1319, 1319, 1322, ..., 1768, 1770, 1772]], dtype=uint16)\n\n\nThe result is a two-dimensional numpy array.\nThe relation between a rasterio file connection and the derived properties is summarized in Figure 1.3. The file connection (created with rasterio.open) gives access to the two components of raster data: the metadata (via the .meta property) and the values (via the .read method).\n\n\n\nFigure 1.3: Creating a GeoDataFrame from scratch\n\n\n\n\n1.3.3 Raster from scratch\nIn this section, we are going to demonstrate creation of rasters from scratch. We are going to create two small rasters, elev and grain, which we are going to use in examples later on in the book. Unlike creating a vector layer, creating a raster from scratch is rarely needed in practive because aligning a raster with the right spatial extent is difficult to do programmatically (GIS software is a better fit for the job). Nevertheless, the examples will be useful to become more familiar with the rasterio data structures.\nA raster is basically an array combined with georeferencing information, namely:\n\nA transformation matrix (linking pixel indices with coordinates)\nA CRS definition\n\nTherefore, to create a raster, we first need to have an array with the values, then supplement it with the georeferencing information. Let’s create the arrays elev and grain. The elev array is a 6 by 6 array with sequential values from 1 to 36. It can be created as follows:\n\nelev = np.arange(1, 37, dtype=np.uint8).reshape(6, 6)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nThe grain array represents a categorical raster with values 0, 1, 2, corresponding to categories “clay”, “silt”, “sand”, respectively. We will create if from a specific arrangement of pixel values, as follows:\n\nv = [\n  1, 0, 1, 2, 2, 2, \n  0, 2, 0, 0, 2, 1, \n  0, 2, 2, 0, 0, 2, \n  0, 0, 1, 1, 1, 1, \n  1, 1, 1, 2, 1, 1, \n  2, 1, 2, 2, 0, 2\n]\ngrain = np.array(v, dtype=np.uint8).reshape(6, 6)\ngrain\n\narray([[1, 0, 1, 2, 2, 2],\n       [0, 2, 0, 0, 2, 1],\n       [0, 2, 2, 0, 0, 2],\n       [0, 0, 1, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1],\n       [2, 1, 2, 2, 0, 2]], dtype=uint8)\n\n\nNote that in both cases we are using the uint8 (unsigned integer in 8 bits, i.e., 0-255) data type, which is minimally sufficient to represent all possible values of the given rasters. This is the recommended approach for a minimal memory footprint.\nWhat is missing is the raster transform (see Section 1.3.2). In this case, since the rasters are arbitrary, we also set up an arbitrary transformation matrix, where:\n\nthe origin (\\(x_{min}\\), \\(y_{max}\\)) is at -1.5,1.5, and\nand resolution (\\(delta_{x}\\), \\(delta_{y}\\)) is 0.5,-0.5.\n\nIn terms of code, we do that as follows, using rasterio.transform.from_origin:\n\nnew_transform = rasterio.transform.from_origin(\n    west=-1.5, \n    north=1.5, \n    xsize=0.5, \n    ysize=0.5\n)\nnew_transform\n\nAffine(0.5, 0.0, -1.5,\n       0.0, -0.5, 1.5)\n\n\nNote that, confusingly, \\(delta_{y}\\) is defined in rasterio.transform.from_origin using a positive value (0.5), even though it is eventuially negative (-0.5)!\nThe raster can now be plotted in its coordinate system, passing the array along with the transformation matrix to show:\n\nshow(elev, transform=new_transform);\n\n\n\n\nThe grain raster can be plotted the same way, as we are going to use the same transformation matrix for it as well:\n\nshow(grain, transform=new_transform);\n\n\n\n\nAt this point, we can work with the raster using rasterio:\n\nPassing the transformation matrix wherever true raster pixel coordinates are important (such as in function show above)\nKeeping in mind that any other layer we use in the analysis is in the same CRS of those coordinates\n\nFinally, to export the raster for permanent storage, along with the CRS definition, we need to go through the following steps:\n\nCreate a raster file connection (where we set the transform and the CRS, among other settings)\nWrite the array with raster values into the connection\nClose the connection\n\nIn the case of elev, we do it as follows:\n\nnew_dataset = rasterio.open(\n    'output/elev.tif', 'w', \n    driver='GTiff',\n    height=elev.shape[0],\n    width=elev.shape[1],\n    count=1,\n    dtype=elev.dtype,\n    crs=4326,\n    transform=new_transform\n)\nnew_dataset.write(elev, 1)\nnew_dataset.close()\n\nNote that the CRS we (arbitrarily) set for the elev raster is WGS84, defined using crs=4326 according to the EPSG code.\nHere is how we export the grain raster as well, using almost the exact same code just with elev replaced with grain:\n\nnew_dataset = rasterio.open(\n    'output/grain.tif', 'w', \n    driver='GTiff',\n    height=grain.shape[0],\n    width=grain.shape[1],\n    count=1,\n    dtype=grain.dtype,\n    crs=4326,\n    transform=new_transform\n)\nnew_dataset.write(grain, 1)\nnew_dataset.close()\n\nDon’t worry if the raster export code is unclear. We will elaborate on the details of raster output in @read-write.\nAs a result, the files elev.tif and grain.tif are written into the output directory. These are identical to the elev.tif and grain.tif files in the data directory which we use later on in the examples (for example, Section 2.4.1).\nNote that the transform matrices and dimensions of elev and grain are identical. This means that the rasters are overlapping, and can be combined into one two-band raster, combined in raster algebra operations (Section 3.4.2), etc."
  },
  {
    "objectID": "02-spatial-data.html#sec-coordinate-reference-systems",
    "href": "02-spatial-data.html#sec-coordinate-reference-systems",
    "title": "1  Geographic data in Python",
    "section": "1.4 Coordinate Reference Systems",
    "text": "1.4 Coordinate Reference Systems\nVector and raster spatial data types share concepts intrinsic to spatial data. Perhaps the most fundamental of these is the Coordinate Reference System (CRS), which defines how the spatial elements of the data relate to the surface of the Earth (or other bodies). CRSs are either geographic or projected, as introduced at the beginning of this chapter (see …). This section explains each type, laying the foundations for Chapter 6, which provides a deep dive into setting, transforming, and querying CRSs.\n\n1.4.1 Geographic coordinate systems\nGeographic coordinate systems identify any location on the Earth’s surface using two values—longitude and latitude (see left panel of Figure 1.4). Longitude is a location in the East-West direction in angular distance from the Prime Meridian plane, while latitude is an angular distance North or South of the equatorial plane. Distances in geographic CRSs are therefore not measured in meters. This has important consequences, as demonstrated in Section 7.\nA spherical or ellipsoidal surface represents the surface of the Earth in geographic coordinate systems. Spherical models assume that the Earth is a perfect sphere of a given radius—they have the advantage of simplicity, but, at the same time, they are inaccurate: the Earth is not a sphere! Ellipsoidal models are defined by two parameters: the equatorial radius and the polar radius. These are suitable because the Earth is compressed: the equatorial radius is around 11.5 km longer than the polar radius. \nEllipsoids are part of a broader component of CRSs: the datum. This contains information on what ellipsoid to use and the precise relationship between the Cartesian coordinates and location on the Earth’s surface. There are two types of datum—geocentric (such as WGS84) and local (such as NAD83). You can see examples of these two types of datums in Figure …. Black lines represent a geocentric datum, whose center is located in the Earth’s center of gravity and is not optimized for a specific location. In a local datum, shown as a purple dashed line, the ellipsoidal surface is shifted to align with the surface at a particular location. These allow local variations on Earth’s surface, such as large mountain ranges, to be accounted for in a local CRS. \n\n\n1.4.2 Projected coordinate reference systems\nAll projected CRSs are based on a geographic CRS, described in the previous section, and rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (x and y) values in a projected CRS. Projected CRSs are based on Cartesian coordinates on an implicitly flat surface (see right panel of Figure 1.4).  They have an origin, x and y axes, and a linear unit of measurement such as meters.\nThis transition cannot be done without adding some deformations. Therefore, some properties of the Earth’s surface are distorted in this process, such as area, direction, distance, and shape. A projected coordinate system can preserve only one or two of those properties. Projections are often named based on a property they preserve: equal-area preserves area, azimuthal preserves direction, equidistant preserves distance, and conformal preserves local shape.\nThere are three main groups of projection types: conic, cylindrical, and planar (azimuthal). In a conic projection, the Earth’s surface is projected onto a cone along a single line of tangency or two lines of tangency. Distortions are minimized along the tangency lines and rise with the distance from those lines in this projection. Therefore, it is best suited for maps of mid-latitude areas. A cylindrical projection maps the surface onto a cylinder. This projection could also be created by touching the Earth’s surface along a single line of tangency or two lines of tangency. Cylindrical projections are used most often when mapping the entire world. A planar projection projects data onto a flat surface touching the globe at a point or along a line of tangency. It is typically used in mapping polar regions.\nLike most open-source geospatial software, the geopandas and rasterio packages use the PROJ software for CRS definition and calculations. The pyproj package is a low-level interface to PROJ. Using its functions, we can examine the list of supported projections:\n\nimport pyproj\nepsg_codes = pyproj.get_codes('EPSG', 'CRS')  ## List of supported EPSG codes\nepsg_codes[:5]  ## print first five\n\n['2000', '20000', '20001', '20002', '20003']\n\n\n\npyproj.CRS.from_epsg(4326)  ## Printout of WGS84 CRS (EPSG:4326)\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nA quick summary of different projections, their types, properties, and suitability can be found in “Map Projections” (1993) and at https://www.geo-projections.com/. We will expand on CRSs and explain how to project from one CRS to another in Chapter 7. But, for now, it is sufficient to know:\n\nThat coordinate systems are a key component of geographic objects\nKnowing which CRS your data is in, and whether it is in geographic (lon/lat) or projected (typically meters), is important and has consequences for how Python handles spatial and geometry operations\nCRSs of geopandas (vector layer or geometry column) and rasterio (raster) objects can be queried with the .crs property\n\nHere is a demonstration of the last bullet point, where we import a vector layer and figure out its CRS (in this case, a projected CRS, namely UTM Zone 12):\n\nzion = gpd.read_file('data/zion.gpkg')\nzion.crs\n\n<Bound CRS: PROJCS[\"UTM Zone 12, Northern Hemisphere\",GEOGCS[\" ...>\nName: UTM Zone 12, Northern Hemisphere\nAxis Info [cartesian]:\n- [east]: Easting (Meter)\n- [north]: Northing (Meter)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: Transformation from GRS 1980(IUGG, 1980) to WGS84\n- method: Position Vector transformation (geog2D domain)\nDatum: unknown\n- Ellipsoid: GRS80\n- Prime Meridian: Greenwich\nSource CRS: UTM Zone 12, Northern Hemisphere\n\n\nAnd here is an illustration of the layer in the original projected CRS and in a geographic CRS (Figure 1.4):\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,4))\nzion.to_crs(4326).plot(ax=axes[0], edgecolor='black', color='lightgrey')\nzion.plot(ax=axes[1], edgecolor='black', color='lightgrey')\naxes[0].set_axisbelow(True)  ## Plot grid below other elements\naxes[1].set_axisbelow(True)\naxes[0].grid()  ## Add grid\naxes[1].grid()\naxes[0].set_title('WGS 84')\naxes[1].set_title('UTM zone 12N');\n\n\n\n\nFigure 1.4: Examples of geographic (WGS 84; left) and projected (NAD83 / UTM zone 12N; right) coordinate systems for a vector data type.\n\n\n\n\nWe are going to elaborate on reprojection from one CRS to another (.to_crs in the above code section) in Chapter 6."
  },
  {
    "objectID": "02-spatial-data.html#units",
    "href": "02-spatial-data.html#units",
    "title": "1  Geographic data in Python",
    "section": "1.5 Units",
    "text": "1.5 Units\nAn essential feature of CRSs is that they contain information about spatial units. Clearly, it is vital to know whether a house’s measurements are in feet or meters, and the same applies to maps. It is a good cartographic practice to add a scale bar or some other distance indicator onto maps to demonstrate the relationship between distances on the page or screen and distances on the ground. Likewise, it is important for the user to be aware of the units in which the geometry coordinates are, to ensure that subsequent calculations are done in the right context.\nPython spatial data structures in geopandas and rasterio do not natively support the concept of measurement. The coordinates of a vector layer or a raster are plain numbers, referring to an arbitrary plane. For example, according to the .transform matrix of srtm.tif we can see that the raster resolution is 0.000833 and that its CRS is WGS84 (EPSG: 4326). We may know (or can find out) that the units of WGS84 are decimal degrees. However, that information is not encoded in any numeric calculation.\n\nsrc.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 65535.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nConsequently, we need to be aware of the CRS units we are working with. Typically, these are decimal degrees, in a geographic CRS, or \\(m\\), in a projected CRS, although there are exceptions. Geometric calculations such as length, area, or distance, return plain numbers in the same units of the CRS (such as \\(m\\) or \\(m^2\\)). It is up to the user to determine which units the result is given in and treat the result accordingly. For example, if the area output was in \\(m^2\\) and we need the result in \\(km^2\\), then we need to divide the result by \\(1000^2\\)."
  },
  {
    "objectID": "02-spatial-data.html#exercises",
    "href": "02-spatial-data.html#exercises",
    "title": "1  Geographic data in Python",
    "section": "1.6 Exercises",
    "text": "1.6 Exercises\n…"
  },
  {
    "objectID": "03-attribute-operations.html#prerequisites",
    "href": "03-attribute-operations.html#prerequisites",
    "title": "2  Attribute data operations",
    "section": "2.1 Prerequisites",
    "text": "2.1 Prerequisites\nPackages…\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\n\nSample data…\n\n\nAttempting to get the data\n\n\n\nworld = gpd.read_file('data/world.gpkg')\nsrc_elev = rasterio.open('data/elev.tif')\nsrc_multi_rast = rasterio.open('data/landsat.tif')"
  },
  {
    "objectID": "03-attribute-operations.html#introduction",
    "href": "03-attribute-operations.html#introduction",
    "title": "2  Attribute data operations",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nAttribute data is non-spatial information associated with geographic (geometry) data. A bus stop provides a simple example: its position would typically be represented by latitude and longitude coordinates (geometry data), in addition to its name. The Elephant & Castle / New Kent Road stop in London, for example has coordinates of -0.098 degrees longitude and 51.495 degrees latitude which can be represented as POINT (-0.098 51.495) in the Simple Feature representation described in Chapter 1. Attributes such as the name attribute of the POINT feature (to use Simple Features terminology) are the topic of this chapter.\nAnother example is the elevation value (attribute) for a specific grid cell in raster data. Unlike the vector data model, the raster data model stores the coordinate of the grid cell indirectly, meaning the distinction between attribute and spatial information is less clear. To illustrate the point, think of a pixel in the 3rd row and the 4th column of a raster matrix. Its spatial location is defined by its index in the matrix: move from the origin four cells in the x direction (typically east and right on maps) and three cells in the y direction (typically south and down). The raster’s resolution defines the distance for each x- and y-step which is specified in a header. The header is a vital component of raster datasets which specifies how pixels relate to geographic coordinates (see also Chapter @spatial-operations).\nThis chapter teaches how to manipulate geographic objects based on attributes such as the names of bus stops in a vector dataset and elevations of pixels in a raster dataset. For vector data, this means techniques such as subsetting and aggregation (see Section 2.3.1 and Section 2.3.2). Section 2.3.3 and Section 2.3.4 demonstrate how to join data onto simple feature objects using a shared ID and how to create new variables, respectively. Each of these operations has a spatial equivalent: [ operator for subsetting a (Geo)DataFrame using a boolean Series, for example, is applicable both for subsetting objects based on their attribute and spatial relations derived using methods such as .intersects; you can also join attributes in two geographic datasets using spatial joins. This is good news: skills developed in this chapter are cross-transferable. Chapter 3 extends the methods presented here to the spatial world.\nAfter a deep dive into various types of vector attribute operations in the next section, raster attribute data operations are covered in Section 2.4, which demonstrates how to create raster layers containing continuous and categorical attributes and extracting cell values from one or more layer (raster subsetting). Section 2.4.2 provides an overview of ‘global’ raster operations which can be used to summarize entire raster datasets."
  },
  {
    "objectID": "03-attribute-operations.html#vector-attribute-manipulation",
    "href": "03-attribute-operations.html#vector-attribute-manipulation",
    "title": "2  Attribute data operations",
    "section": "2.3 Vector attribute manipulation",
    "text": "2.3 Vector attribute manipulation\nAs mentioned in Section 1.2.2, vector layers (GeoDataFrame, from package geopandas) are basically extended tables (DataFrame from package pandas), the difference being that a vector layer has a geometry column. Since GeoDataFrame extends DataFrame, all ordinary table-related operations from package pandas are supported for vector layers as well, as shown below.\n\n2.3.1 Vector attribute subsetting\npandas supports several subsetting interfaces, though the most recommended ones are:\n\n.loc, which uses pandas indices, and\n.iloc, which uses (implicit) numpy-style numeric indices.\n\nIn both cases the method is followed by square brackets, and two indices, separated by a comma. Each index can comprise:\n\nA specific value, as in 1\nA slice, as in 0:3\nA list, as in [0,2,4]\n:—indicating “all” indices\n\nAn exception to this rule is selecting columns using a list, as in df[['a','b']], instead of df.loc[:, ['a','b']], to select columns 'a' and 'b' from df.\nHere are few examples of subsetting the GeoDataFrame of world countries.\nSubsetting rows by position, e.g., the first three rows:\n\nworld.iloc[0:3, :]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.960\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n  \n\n3 rows × 11 columns\n\n\n\nwhich is equivalent to:\n\nworld.iloc[:3]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.960\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n  \n\n3 rows × 11 columns\n\n\n\nas well as:\n\nworld.head(3)\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.960\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n  \n\n3 rows × 11 columns\n\n\n\nSubsetting columns by position, e.g., the first three columns:\n\nworld.iloc[:, 0:3]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n    \n  \n\n177 rows × 3 columns\n\n\n\nSubsetting rows and columns by position:\n\nworld.iloc[0:3, 0:3]\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n    \n  \n\n\n\n\nSubsetting columns by name:\n\nworld[['name_long', 'geometry']]\n\n\n\n\n\n  \n    \n      \n      name_long\n      geometry\n    \n  \n  \n    \n      0\n      Fiji\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      Tanzania\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      Western Sahara\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      174\n      Kosovo\n      MULTIPOLYGON (((20.59025 41.855...\n    \n    \n      175\n      Trinidad and Tobago\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n    \n      176\n      South Sudan\n      MULTIPOLYGON (((30.83385 3.5091...\n    \n  \n\n177 rows × 2 columns\n\n\n\n“Slice” of columns between given ones:\n\nworld.loc[:, 'name_long':'pop']\n\n\n\n\n\n  \n    \n      \n      name_long\n      continent\n      region_un\n      ...\n      type\n      area_km2\n      pop\n    \n  \n  \n    \n      0\n      Fiji\n      Oceania\n      Oceania\n      ...\n      Sovereign country\n      19289.970733\n      885806.0\n    \n    \n      1\n      Tanzania\n      Africa\n      Africa\n      ...\n      Sovereign country\n      932745.792357\n      52234869.0\n    \n    \n      2\n      Western Sahara\n      Africa\n      Africa\n      ...\n      Indeterminate\n      96270.601041\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      Kosovo\n      Europe\n      Europe\n      ...\n      Sovereign country\n      11230.261672\n      1821800.0\n    \n    \n      175\n      Trinidad and Tobago\n      North America\n      Americas\n      ...\n      Sovereign country\n      7737.809855\n      1354493.0\n    \n    \n      176\n      South Sudan\n      Africa\n      Africa\n      ...\n      Sovereign country\n      624909.099086\n      11530971.0\n    \n  \n\n177 rows × 7 columns\n\n\n\nSubsetting by a list of boolean values (0 and 1 or True and False):\n\nx = [1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0] \nworld.iloc[:, x]\n\n\n\n\n\n  \n    \n      \n      name_long\n      name_long\n      iso_a2\n      ...\n      name_long\n      iso_a2\n      iso_a2\n    \n  \n  \n    \n      0\n      Fiji\n      Fiji\n      FJ\n      ...\n      Fiji\n      FJ\n      FJ\n    \n    \n      1\n      Tanzania\n      Tanzania\n      TZ\n      ...\n      Tanzania\n      TZ\n      TZ\n    \n    \n      2\n      Western Sahara\n      Western Sahara\n      EH\n      ...\n      Western Sahara\n      EH\n      EH\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      Kosovo\n      Kosovo\n      XK\n      ...\n      Kosovo\n      XK\n      XK\n    \n    \n      175\n      Trinidad and Tobago\n      Trinidad and Tobago\n      TT\n      ...\n      Trinidad and Tobago\n      TT\n      TT\n    \n    \n      176\n      South Sudan\n      South Sudan\n      SS\n      ...\n      South Sudan\n      SS\n      SS\n    \n  \n\n177 rows × 11 columns\n\n\n\nWe can remove specific rows by id using the .drop method, e.g., dropping rows 2, 3, and 5:\n\nworld.drop([2, 3, 5])\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.960000\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163000\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      4\n      US\n      United States\n      North America\n      ...\n      78.841463\n      51921.984639\n      MULTIPOLYGON (((-171.73166 63.7...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n      ...\n      71.097561\n      8698.291559\n      MULTIPOLYGON (((20.59025 41.855...\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n      ...\n      55.817000\n      1935.879400\n      MULTIPOLYGON (((30.83385 3.5091...\n    \n  \n\n174 rows × 11 columns\n\n\n\nOr remove specific columns using the .drop method and axis=1 (i.e., columns):\n\nworld.drop(['name_long', 'continent'], axis=1)\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      region_un\n      subregion\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Oceania\n      Melanesia\n      ...\n      69.960000\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Africa\n      Eastern Africa\n      ...\n      64.163000\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      EH\n      Africa\n      Northern Africa\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Europe\n      Southern Europe\n      ...\n      71.097561\n      8698.291559\n      MULTIPOLYGON (((20.59025 41.855...\n    \n    \n      175\n      TT\n      Americas\n      Caribbean\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n    \n      176\n      SS\n      Africa\n      Eastern Africa\n      ...\n      55.817000\n      1935.879400\n      MULTIPOLYGON (((30.83385 3.5091...\n    \n  \n\n177 rows × 9 columns\n\n\n\nWe can rename columns using the .rename method:\n\nworld[['name_long', 'pop']].rename(columns={'pop': 'population'})\n\n\n\n\n\n  \n    \n      \n      name_long\n      population\n    \n  \n  \n    \n      0\n      Fiji\n      885806.0\n    \n    \n      1\n      Tanzania\n      52234869.0\n    \n    \n      2\n      Western Sahara\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      174\n      Kosovo\n      1821800.0\n    \n    \n      175\n      Trinidad and Tobago\n      1354493.0\n    \n    \n      176\n      South Sudan\n      11530971.0\n    \n  \n\n177 rows × 2 columns\n\n\n\nThe standard numpy comparison operators can be used in boolean subsetting, as illustrated in Table Table 2.1.\n\n\nTable 2.1: Comparison operators that return Booleans (True/False).\n\n\nSymbol\nName\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n>, <\nGreater/Less than\n\n\n>=, <=\nGreater/Less than or equal\n\n\n&, |, ~\nLogical operators: And, Or, Not\n\n\n\n\nThe following example demonstrates logical vectors for subsetting by creating a new GeoDataFrame object called small_countries that contains only those countries whose surface area is smaller than 10,000 km2:\n\nidx_small = world['area_km2'] < 10000  ## a logical 'Series'\nsmall_countries = world[idx_small]\nsmall_countries\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      45\n      PR\n      Puerto Rico\n      North America\n      ...\n      79.390122\n      35066.046376\n      MULTIPOLYGON (((-66.28243 18.51...\n    \n    \n      79\n      PS\n      Palestine\n      Asia\n      ...\n      73.126000\n      4319.528283\n      MULTIPOLYGON (((35.39756 31.489...\n    \n    \n      89\n      VU\n      Vanuatu\n      Oceania\n      ...\n      71.709000\n      2892.341604\n      MULTIPOLYGON (((166.79316 -15.6...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      160\n      NaN\n      Northern Cyprus\n      Asia\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((32.73178 35.140...\n    \n    \n      161\n      CY\n      Cyprus\n      Asia\n      ...\n      80.173000\n      29786.365653\n      MULTIPOLYGON (((32.73178 35.140...\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n  \n\n7 rows × 11 columns\n\n\n\nThe intermediary idx_small (short for index representing small countries) is a boolean Series that can be used to subset the seven smallest countries in the world by surface area. A more concise command, which omits the intermediary object, generates the same result:\n\nsmall_countries = world[world['area_km2'] < 10000]\nsmall_countries\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      45\n      PR\n      Puerto Rico\n      North America\n      ...\n      79.390122\n      35066.046376\n      MULTIPOLYGON (((-66.28243 18.51...\n    \n    \n      79\n      PS\n      Palestine\n      Asia\n      ...\n      73.126000\n      4319.528283\n      MULTIPOLYGON (((35.39756 31.489...\n    \n    \n      89\n      VU\n      Vanuatu\n      Oceania\n      ...\n      71.709000\n      2892.341604\n      MULTIPOLYGON (((166.79316 -15.6...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      160\n      NaN\n      Northern Cyprus\n      Asia\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((32.73178 35.140...\n    \n    \n      161\n      CY\n      Cyprus\n      Asia\n      ...\n      80.173000\n      29786.365653\n      MULTIPOLYGON (((32.73178 35.140...\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n  \n\n7 rows × 11 columns\n\n\n\nThe various methods shown above can be chained for any combination with several subsetting steps, e.g.:\n\nworld[world['continent'] == 'Asia']  \\\n    .loc[:, ['name_long', 'continent']]  \\\n    .iloc[0:5, :]\n\n\n\n\n\n  \n    \n      \n      name_long\n      continent\n    \n  \n  \n    \n      5\n      Kazakhstan\n      Asia\n    \n    \n      6\n      Uzbekistan\n      Asia\n    \n    \n      8\n      Indonesia\n      Asia\n    \n    \n      24\n      Timor-Leste\n      Asia\n    \n    \n      76\n      Israel\n      Asia\n    \n  \n\n\n\n\nWe can also combine indexes:\n\nidx_small = world['area_km2'] < 10000\nidx_asia = world['continent'] == 'Asia'\nworld.loc[idx_small & idx_asia, ['name_long', 'continent', 'area_km2']]\n\n\n\n\n\n  \n    \n      \n      name_long\n      continent\n      area_km2\n    \n  \n  \n    \n      79\n      Palestine\n      Asia\n      5037.103826\n    \n    \n      160\n      Northern Cyprus\n      Asia\n      3786.364506\n    \n    \n      161\n      Cyprus\n      Asia\n      6207.006191\n    \n  \n\n\n\n\n\n\n2.3.2 Vector attribute aggregation\nAggregation involves summarizing data based on one or more grouping variables (typically values in a column;geographic aggregation is covered in the next chapter). A classic example of this attribute-based aggregation is calculating the number of people per continent based on country-level data (one row per country). The world dataset contains the necessary ingredients: the columns pop and continent, the population and the grouping variable, respectively. The aim is to find the sum() of country populations for each continent, resulting in a smaller data frame. (Since aggregation is a form of data reduction, it can be a useful early step when working with large datasets). This aggregation can be achieved using a combination of .groupby and .sum:\n\nworld_agg1 = world[['continent', 'pop']].groupby('continent').sum()\nworld_agg1\n\n\n\n\n\n  \n    \n      \n      pop\n    \n    \n      continent\n      \n    \n  \n  \n    \n      Africa\n      1.154947e+09\n    \n    \n      Antarctica\n      0.000000e+00\n    \n    \n      Asia\n      4.311408e+09\n    \n    \n      ...\n      ...\n    \n    \n      Oceania\n      3.775783e+07\n    \n    \n      Seven seas (open ocean)\n      0.000000e+00\n    \n    \n      South America\n      4.120608e+08\n    \n  \n\n8 rows × 1 columns\n\n\n\nIf you dislike the scientific notation used by default to display the population sums, you can change the Pandas display format for float values like this:\n\npd.set_option('display.float_format', '{:.0f}'.format)\nworld_agg1\n\n\n\n\n\n  \n    \n      \n      pop\n    \n    \n      continent\n      \n    \n  \n  \n    \n      Africa\n      1154946633\n    \n    \n      Antarctica\n      0\n    \n    \n      Asia\n      4311408059\n    \n    \n      ...\n      ...\n    \n    \n      Oceania\n      37757833\n    \n    \n      Seven seas (open ocean)\n      0\n    \n    \n      South America\n      412060811\n    \n  \n\n8 rows × 1 columns\n\n\n\nThe result is a (non-spatial) table with eight rows, one per continent, and two columns reporting the name and population of each continent.\nIf we want to include the geometry in the aggregation result, we can use the .dissolve method. That way, in addition to the summed population, we also get the associated geometry per continent, i.e., the union of all countries. Note that we use the by parameter to choose which column(s) are used for grouping, and the aggfunc parameter to choose the aggregation function for non-geometry columns:\n\nworld_agg2 = world[['continent', 'pop', 'geometry']] \\\n    .dissolve(by='continent', aggfunc='sum') \\\n    .reset_index()\nworld_agg2\n\n\n\n\n\n  \n    \n      \n      continent\n      geometry\n      pop\n    \n  \n  \n    \n      0\n      Africa\n      MULTIPOLYGON (((-11.43878 6.785...\n      1154946633\n    \n    \n      1\n      Antarctica\n      MULTIPOLYGON (((-61.13898 -79.9...\n      0\n    \n    \n      2\n      Asia\n      MULTIPOLYGON (((48.67923 14.003...\n      4311408059\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      5\n      Oceania\n      MULTIPOLYGON (((147.91405 -43.2...\n      37757833\n    \n    \n      6\n      Seven seas (open ocean)\n      POLYGON ((68.93500 -48.62500, 6...\n      0\n    \n    \n      7\n      South America\n      MULTIPOLYGON (((-68.63999 -55.5...\n      412060811\n    \n  \n\n8 rows × 3 columns\n\n\n\nFigure 2.1 shows the result:\n\nworld_agg2.plot(column='pop', legend=True);\n\n\n\n\nFigure 2.1: Continents with summed population\n\n\n\n\nThe resulting world_agg2 object is a GeoDataFrame containing 8 features representing the continents of the world (and the open ocean).\nOther options for the aggfunc parameter in .dissolve include:\n\n'first'\n'last'\n'min'\n'max'\n'sum'\n'mean'\n'median'\n\nAdditionally, we can pass custom functions.\nAs a more complex example, here is how we can calculate the total population, area, and count of countries, per continent:\n\nworld_agg3 = world.dissolve(\n    by='continent', aggfunc={\n         \"name_long\": \"count\",\n         \"pop\": \"sum\",\n         'area_km2': \"sum\"\n     }).rename(columns={'name_long': 'n'})\nworld_agg3\n\n\n\n\n\n  \n    \n      \n      geometry\n      n\n      pop\n      area_km2\n    \n    \n      continent\n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      MULTIPOLYGON (((-11.43878 6.785...\n      51\n      1154946633\n      29946198\n    \n    \n      Antarctica\n      MULTIPOLYGON (((-61.13898 -79.9...\n      1\n      0\n      12335956\n    \n    \n      Asia\n      MULTIPOLYGON (((48.67923 14.003...\n      47\n      4311408059\n      31252459\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Oceania\n      MULTIPOLYGON (((147.91405 -43.2...\n      7\n      37757833\n      8504489\n    \n    \n      Seven seas (open ocean)\n      POLYGON ((68.93500 -48.62500, 6...\n      1\n      0\n      11603\n    \n    \n      South America\n      MULTIPOLYGON (((-68.63999 -55.5...\n      13\n      412060811\n      17762592\n    \n  \n\n8 rows × 4 columns\n\n\n\nFigure Figure 2.2 visualizes the resulting layer (world_agg3) of continents with the three aggregated attributes.\n\nfig, axes = plt.subplots(2, 2, figsize=(9, 5))\nworld_agg3.plot(column='pop', edgecolor='black', legend=True, ax=axes[0][0])\nworld_agg3.plot(column='area_km2', edgecolor='black', legend=True, ax=axes[0][1])\nworld_agg3.plot(column='n', edgecolor='black', legend=True, ax=axes[1][0])\naxes[0][0].set_title('Summed population')\naxes[0][1].set_title('Summed area')\naxes[1][0].set_title('Count of countries')\nfig.delaxes(axes[1][1]);\n\n\n\n\nFigure 2.2: Continent properties, calculated using spatial aggregation using different functions\n\n\n\n\nLet’s proceed with the last result to demonstrate other table-related operations. Given the world_agg3 continent summary (Figure 2.2), we:\n\ndrop the geometry columns,\ncalculate population density of each continent,\narrange continents by the number countries they contain, and\nkeep only the 3 most populous continents.\n\n\nworld_agg4 = world_agg3.drop(columns=['geometry'])\nworld_agg4['density'] = world_agg4['pop'] / world_agg4['area_km2']\nworld_agg4 = world_agg4.sort_values(by='n', ascending=False)\nworld_agg4 = world_agg4.head(3)\nworld_agg4\n\n\n\n\n\n  \n    \n      \n      n\n      pop\n      area_km2\n      density\n    \n    \n      continent\n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      51\n      1154946633\n      29946198\n      39\n    \n    \n      Asia\n      47\n      4311408059\n      31252459\n      138\n    \n    \n      Europe\n      39\n      669036256\n      23065219\n      29\n    \n  \n\n\n\n\n\n\n2.3.3 Vector attribute joining\nCombining data from different sources is a common task in data preparation. Joins do this by combining tables based on a shared ‘key’ variable. pandas has a function named pd.merge for joining (Geo)DataFrames based on common column(s). The pd.merge function follows conventions used in the database language SQL (Grolemund and Wickham 2016). The pd.merge function works the same on DataFrame and GeoDataFrame objects. The result of pd.merge can be either a DataFrame or a GeoDataFrame object, depending on the inputs.\nA common type of attribute join on spatial data is to join DataFrames to GeoDataFrames. To achieve this, we use pd.merge with a GeoDataFrame as the first argument and add columns to it from a DataFrame specified as the second argument. In the following example, we combine data on coffee production with the world dataset. The coffee data is in a DataFrame called coffee_data imported from a CSV file of major coffee-producing nations:\n\ncoffee_data = pd.read_csv('data/coffee_data.csv')\ncoffee_data\n\n\n\n\n\n  \n    \n      \n      name_long\n      coffee_production_2016\n      coffee_production_2017\n    \n  \n  \n    \n      0\n      Angola\n      NaN\n      NaN\n    \n    \n      1\n      Bolivia\n      3\n      4\n    \n    \n      2\n      Brazil\n      3277\n      2786\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      44\n      Zambia\n      3\n      NaN\n    \n    \n      45\n      Zimbabwe\n      1\n      1\n    \n    \n      46\n      Others\n      23\n      26\n    \n  \n\n47 rows × 3 columns\n\n\n\nIts three columns are:\n\nname_long country name\ncoffee_production_2016 and coffee_production_2017 contain estimated values for coffee production in units of 60-kg bags per year.\n\nA left join, which preserves the first dataset, merges world with coffee_data, based on the common 'name_long' column:\n\nworld_coffee = pd.merge(world, coffee_data, on='name_long', how='left')\nworld_coffee\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      geometry\n      coffee_production_2016\n      coffee_production_2017\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      MULTIPOLYGON (((-180.00000 -16....\n      NaN\n      NaN\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      MULTIPOLYGON (((33.90371 -0.950...\n      81\n      66\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      MULTIPOLYGON (((-8.66559 27.656...\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n      ...\n      MULTIPOLYGON (((20.59025 41.855...\n      NaN\n      NaN\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      MULTIPOLYGON (((-61.68000 10.76...\n      NaN\n      NaN\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n      ...\n      MULTIPOLYGON (((30.83385 3.5091...\n      NaN\n      NaN\n    \n  \n\n177 rows × 13 columns\n\n\n\nThe result is a GeoDataFrame object identical to the original world object, but with two new variables (coffee_production_2016 and coffee_production_2017) on coffee production. This can be plotted as a map, as illustrated in Figure 2.3:\n\nbase = world_coffee.plot(color='white', edgecolor='lightgrey')\ncoffee_map = world_coffee.plot(ax=base, column='coffee_production_2017')\ncoffee_map.set_title('Coffee production');\n\n\n\n\nFigure 2.3: World coffee production, thousand 60-kg bags by country, in 2017 (source: International Coffee Organization).\n\n\n\n\nTo work, attribute-based joins need a ‘key variable’ in both datasets (on parameter of pd.merge). In the above example, both world_coffee and world DataFrames contained a column called name_long. (By default pd.merge uses all columns with matching names. However, it is recommended to explicitly specify the names of the columns to be used for matching, like we did in the last example.)\nIn case where column names are not the same, you can use left_on and right_on to specify the respective columns.\nNote that the result world_coffee has the same number of rows as the original dataset world. Although there are only 47 rows in coffee_data, all 177 country records are kept intact in world_coffee. Rows in the original dataset with no match are assigned np.nan values for the new coffee production variables. This is a characteristic of a left join (specified with how='left') and is what we typically want to do.\nWhat if we only want to keep countries that have a match in the key variable? In that case an inner join can be used:\n\npd.merge(world, coffee_data, on='name_long', how='inner')\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      geometry\n      coffee_production_2016\n      coffee_production_2017\n    \n  \n  \n    \n      0\n      TZ\n      Tanzania\n      Africa\n      ...\n      MULTIPOLYGON (((33.90371 -0.950...\n      81\n      66\n    \n    \n      1\n      PG\n      Papua New Guinea\n      Oceania\n      ...\n      MULTIPOLYGON (((141.00021 -2.60...\n      114\n      74\n    \n    \n      2\n      ID\n      Indonesia\n      Asia\n      ...\n      MULTIPOLYGON (((104.36999 -1.08...\n      742\n      360\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      42\n      ET\n      Ethiopia\n      Africa\n      ...\n      MULTIPOLYGON (((47.78942 8.0030...\n      215\n      283\n    \n    \n      43\n      UG\n      Uganda\n      Africa\n      ...\n      MULTIPOLYGON (((33.90371 -0.950...\n      408\n      443\n    \n    \n      44\n      RW\n      Rwanda\n      Africa\n      ...\n      MULTIPOLYGON (((30.41910 -1.134...\n      36\n      42\n    \n  \n\n45 rows × 13 columns\n\n\n\nAn alternative way to join two (Geo)DataFrames is the aptly called join function:\n\nworld.join(coffee_data.set_index('name_long'), on='name_long', how='inner')\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      geometry\n      coffee_production_2016\n      coffee_production_2017\n    \n  \n  \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      MULTIPOLYGON (((33.90371 -0.950...\n      81\n      66\n    \n    \n      7\n      PG\n      Papua New Guinea\n      Oceania\n      ...\n      MULTIPOLYGON (((141.00021 -2.60...\n      114\n      74\n    \n    \n      8\n      ID\n      Indonesia\n      Asia\n      ...\n      MULTIPOLYGON (((104.36999 -1.08...\n      742\n      360\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      165\n      ET\n      Ethiopia\n      Africa\n      ...\n      MULTIPOLYGON (((47.78942 8.0030...\n      215\n      283\n    \n    \n      168\n      UG\n      Uganda\n      Africa\n      ...\n      MULTIPOLYGON (((33.90371 -0.950...\n      408\n      443\n    \n    \n      169\n      RW\n      Rwanda\n      Africa\n      ...\n      MULTIPOLYGON (((30.41910 -1.134...\n      36\n      42\n    \n  \n\n45 rows × 13 columns\n\n\n\nNote that in this case, we need to set the index of coffee_data to the name_long values to avoid error messages.\n\n\n2.3.4 Creating attributes and removing spatial information\nOften, we would like to create a new column based on already existing columns. For example, we want to calculate population density for each country. For this we need to divide a population column, here pop, by an area column, here area_km2. Note that we are working on a copy of world named world2 so that we do not modify the original layer:\n\nworld2 = world.copy()\nworld2['pop_dens'] = world2['pop'] / world2['area_km2']\nworld2\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      gdpPercap\n      geometry\n      pop_dens\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      8222\n      MULTIPOLYGON (((-180.00000 -16....\n      46\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      2402\n      MULTIPOLYGON (((33.90371 -0.950...\n      56\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n      ...\n      8698\n      MULTIPOLYGON (((20.59025 41.855...\n      162\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      31182\n      MULTIPOLYGON (((-61.68000 10.76...\n      175\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n      ...\n      1936\n      MULTIPOLYGON (((30.83385 3.5091...\n      18\n    \n  \n\n177 rows × 12 columns\n\n\n\nTo paste (i.e., concatenate) together existing columns, we can use the ordinary Python string operator +, as if we are working with individual strings rather than Series. For example, we want to combine the continent and region_un columns into a new column named con_reg, using ':' as a separator. Subsequesntly, we remove the original columns using .drop:\n\nworld2['con_reg'] = world['continent'] + ':' + world2['region_un']\nworld2 = world2.drop(['continent', 'region_un'], axis=1)\nworld2\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      subregion\n      ...\n      geometry\n      pop_dens\n      con_reg\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Melanesia\n      ...\n      MULTIPOLYGON (((-180.00000 -16....\n      46\n      Oceania:Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Eastern Africa\n      ...\n      MULTIPOLYGON (((33.90371 -0.950...\n      56\n      Africa:Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Northern Africa\n      ...\n      MULTIPOLYGON (((-8.66559 27.656...\n      NaN\n      Africa:Africa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Southern Europe\n      ...\n      MULTIPOLYGON (((20.59025 41.855...\n      162\n      Europe:Europe\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      Caribbean\n      ...\n      MULTIPOLYGON (((-61.68000 10.76...\n      175\n      North America:Americas\n    \n    \n      176\n      SS\n      South Sudan\n      Eastern Africa\n      ...\n      MULTIPOLYGON (((30.83385 3.5091...\n      18\n      Africa:Africa\n    \n  \n\n177 rows × 11 columns\n\n\n\nThe resulting sf object has a new column called con_reg representing the continent and region of each country, e.g., 'South America:Americas' for Argentina and other South America countries. The opposite operation, splitting one column into multiple columns based on a separator string, is done using the .str.split method. As a result we go back to the previous state of two separate continent and region_un columns (only that their position is now last, since they are newly created):\n\nworld2[['continent', 'region_un']] = world2['con_reg'] \\\n    .str.split(':', expand=True)\nworld2\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      subregion\n      ...\n      con_reg\n      continent\n      region_un\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Melanesia\n      ...\n      Oceania:Oceania\n      Oceania\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Northern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Southern Europe\n      ...\n      Europe:Europe\n      Europe\n      Europe\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      Caribbean\n      ...\n      North America:Americas\n      North America\n      Americas\n    \n    \n      176\n      SS\n      South Sudan\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n  \n\n177 rows × 13 columns\n\n\n\nRenaming one or more columns can be done using the .rename method combined with the columns argument, which should be a dictionary of the form old_name:new_name. The following command, for example, renames the lengthy name_long column to simply name:\n\nworld2.rename(columns={'name_long': 'name'})\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name\n      subregion\n      ...\n      con_reg\n      continent\n      region_un\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Melanesia\n      ...\n      Oceania:Oceania\n      Oceania\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Northern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Southern Europe\n      ...\n      Europe:Europe\n      Europe\n      Europe\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      Caribbean\n      ...\n      North America:Americas\n      North America\n      Americas\n    \n    \n      176\n      SS\n      South Sudan\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n  \n\n177 rows × 13 columns\n\n\n\nTo change all column names at once, we assign a list of the “new” column names into the .columns property. The list must be of the same length as the number of columns (i.e., world.shape[1]). This is illustrated below, which outputs the same world2 object, but with very short names:\n\nnew_names = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'geom', 'i', 'j', 'k', 'l']\nworld2.columns = new_names\nworld2\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      ...\n      j\n      k\n      l\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Melanesia\n      ...\n      Oceania:Oceania\n      Oceania\n      Oceania\n    \n    \n      1\n      TZ\n      Tanzania\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      2\n      EH\n      Western Sahara\n      Northern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Southern Europe\n      ...\n      Europe:Europe\n      Europe\n      Europe\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      Caribbean\n      ...\n      North America:Americas\n      North America\n      Americas\n    \n    \n      176\n      SS\n      South Sudan\n      Eastern Africa\n      ...\n      Africa:Africa\n      Africa\n      Africa\n    \n  \n\n177 rows × 13 columns\n\n\n\nTo reorder columns, we can pass a modified columns list to the subsetting operator [. For example, the following expressions reorder world2 columns in reverse alphabetical order:\n\nnames = sorted(world2.columns, reverse=True)\nworld2 = world2[names]\nworld2\n\n\n\n\n\n  \n    \n      \n      l\n      k\n      j\n      ...\n      c\n      b\n      a\n    \n  \n  \n    \n      0\n      Oceania\n      Oceania\n      Oceania:Oceania\n      ...\n      Melanesia\n      Fiji\n      FJ\n    \n    \n      1\n      Africa\n      Africa\n      Africa:Africa\n      ...\n      Eastern Africa\n      Tanzania\n      TZ\n    \n    \n      2\n      Africa\n      Africa\n      Africa:Africa\n      ...\n      Northern Africa\n      Western Sahara\n      EH\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      Europe\n      Europe\n      Europe:Europe\n      ...\n      Southern Europe\n      Kosovo\n      XK\n    \n    \n      175\n      Americas\n      North America\n      North America:Americas\n      ...\n      Caribbean\n      Trinidad and Tobago\n      TT\n    \n    \n      176\n      Africa\n      Africa\n      Africa:Africa\n      ...\n      Eastern Africa\n      South Sudan\n      SS\n    \n  \n\n177 rows × 13 columns\n\n\n\nEach of these attribute data operations, even though they are defined in the pandas package and applicable to any DataFrame, preserve the geometry column and the GeoDataFrame class. Sometimes, however, it makes sense to remove the geometry, for example to speed-up aggregation or to export just the attribute data for statistical analysis. To go from GeoDataFrame to DataFrame we need to:\n\nDrop the geometry column\nConvert from GeoDataFrame into a DataFrame\n\nFor example:\n\nworld2 = world2.drop('geom', axis=1)\nworld2 = pd.DataFrame(world2)\nworld2\n\n\n\n\n\n  \n    \n      \n      l\n      k\n      j\n      ...\n      c\n      b\n      a\n    \n  \n  \n    \n      0\n      Oceania\n      Oceania\n      Oceania:Oceania\n      ...\n      Melanesia\n      Fiji\n      FJ\n    \n    \n      1\n      Africa\n      Africa\n      Africa:Africa\n      ...\n      Eastern Africa\n      Tanzania\n      TZ\n    \n    \n      2\n      Africa\n      Africa\n      Africa:Africa\n      ...\n      Northern Africa\n      Western Sahara\n      EH\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      Europe\n      Europe\n      Europe:Europe\n      ...\n      Southern Europe\n      Kosovo\n      XK\n    \n    \n      175\n      Americas\n      North America\n      North America:Americas\n      ...\n      Caribbean\n      Trinidad and Tobago\n      TT\n    \n    \n      176\n      Africa\n      Africa\n      Africa:Africa\n      ...\n      Eastern Africa\n      South Sudan\n      SS\n    \n  \n\n177 rows × 12 columns"
  },
  {
    "objectID": "03-attribute-operations.html#sec-manipulating-raster-objects",
    "href": "03-attribute-operations.html#sec-manipulating-raster-objects",
    "title": "2  Attribute data operations",
    "section": "2.4 Manipulating raster objects",
    "text": "2.4 Manipulating raster objects\n\n2.4.1 Raster subsetting\nWhen using rasterio, raster values are accessible through a numpy array, which can be imported with the .read method:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nThen, we can access any subset of cell values using numpy methods, e.g.:\n\nelev[0, 0]  ## Value at row 1, column 1\n\n1\n\n\nCell values can be modified by overwriting existing values in conjunction with a subsetting operation, e.g. to set the upper left cell of elev to 0:\n\nelev[0, 0] = 0\nelev\n\narray([[ 0,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nMultiple cells can also be modified in this way:\n\nelev[0, 0:3] = 0\nelev\n\narray([[ 0,  0,  0,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\n\n\n2.4.2 Summarizing raster objects\nGlobal summaries of raster values can be calculated by applying numpy summary functions on the array with raster values, e.g. np.mean:\n\nnp.mean(elev)\n\n18.333333333333332\n\n\nNote that “No Data”-safe functions–such as np.nanmean—should be used in case the raster contains “No Data” values which need to be ignored. Before we can demontrate that, we must convert the array from int to float, as int arrays cannot contain np.nan (due to computer memory limitations):\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1\n\narray([[ 0.,  0.,  0.,  4.,  5.,  6.],\n       [ 7.,  8.,  9., 10., 11., 12.],\n       [13., 14., 15., 16., 17., 18.],\n       [19., 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nNow we can insert an np.nan value into the array. (Trying to do so in the original elev array raises an error, try it to see for yourself)\n\nelev1[0, 2] = np.nan\nelev1\n\narray([[ 0.,  0., nan,  4.,  5.,  6.],\n       [ 7.,  8.,  9., 10., 11., 12.],\n       [13., 14., 15., 16., 17., 18.],\n       [19., 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nWith the np.nan value inplace, the summary value becomes unknown:\n\nnp.mean(elev1)\n\nnan\n\n\nTherefore, we need to ignore the “No Data” value(s):\n\nnp.nanmean(elev1)\n\n18.857142857142858\n\n\nRaster value statistics can be visualized in a variety of ways. One approach is to “flatten” the raster values into a one-dimensional array, then use a graphical function such as plt.hist or plt.boxplot (from matplotlib.pyplot). For example:\n\nx = elev.flatten()\nplt.hist(x);"
  },
  {
    "objectID": "03-attribute-operations.html#exercises",
    "href": "03-attribute-operations.html#exercises",
    "title": "2  Attribute data operations",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises"
  },
  {
    "objectID": "04-spatial-operations.html#prerequisites",
    "href": "04-spatial-operations.html#prerequisites",
    "title": "3  Spatial data operations",
    "section": "3.1 Prerequisites",
    "text": "3.1 Prerequisites\nPackages…\n\nimport shapely.geometry\nimport geopandas as gpd\nimport numpy as np\nimport os\nimport rasterio\nimport scipy.ndimage\nfrom rasterio.plot import show\nimport rasterio.merge\n\nLet us load the sample data for this chapter:\n\nnz = gpd.read_file(\"data/nz.gpkg\")\nnz_height = gpd.read_file(\"data/nz_height.gpkg\")\nworld = gpd.read_file('data/world.gpkg')\ncycle_hire = gpd.read_file('data/cycle_hire.gpkg')\ncycle_hire_osm = gpd.read_file('data/cycle_hire_osm.gpkg')\nsrc_elev = rasterio.open(\"data/elev.tif\")\nsrc_multi_rast = rasterio.open(\"data/landsat.tif\")\nsrc_grain = rasterio.open('data/grain.tif')"
  },
  {
    "objectID": "04-spatial-operations.html#introduction",
    "href": "04-spatial-operations.html#introduction",
    "title": "3  Spatial data operations",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction"
  },
  {
    "objectID": "04-spatial-operations.html#sec-spatial-vec",
    "href": "04-spatial-operations.html#sec-spatial-vec",
    "title": "3  Spatial data operations",
    "section": "3.3 Spatial operations on vector data",
    "text": "3.3 Spatial operations on vector data\n\n3.3.1 Spatial subsetting\nSpatial subsetting is the process of taking a spatial object and returning a new object containing only features that relate in space to another object. Analogous to attribute subsetting (covered in Section 2.3.1), subsets of GeoDataFrames can be created with square bracket ([) operator using the syntax x[y], where x is an GeoDataFrame from which a subset of rows/features will be returned, and y is the ‘subsetting object’. y, in turn, can be created using one of the binary geometry relation methods, such as .intersects (see Section 3.3.2).\nTo demonstrate spatial subsetting, we will use the nz and nz_height layers, which contain geographic data on the 16 main regions and 101 highest points in New Zealand, respectively (Figure 3.1), in a projected coordinate system. The following lines of code create an object representing Canterbury (canterbury), then use spatial subsetting to return all high points in the region (canterbury_height):\n\ncanterbury = nz[nz['Name'] == 'Canterbury']\ncanterbury\n\n\n\n\n\n  \n    \n      \n      Name\n      Island\n      Land_area\n      ...\n      Median_income\n      Sex_ratio\n      geometry\n    \n  \n  \n    \n      10\n      Canterbury\n      South\n      44504.499091\n      ...\n      30100\n      0.975327\n      MULTIPOLYGON (((1686901.914 535...\n    \n  \n\n1 rows × 7 columns\n\n\n\n\n# Does each 'nz_height' point intersect with 'canterbury'?\nsel = nz_height.intersects(canterbury['geometry'].iloc[0])\nsel\n\n0      False\n1      False\n       ...  \n99     False\n100    False\nLength: 101, dtype: bool\n\n\n\ncanterbury_height = nz_height[sel]\ncanterbury_height\n\n\n\n\n\n  \n    \n      \n      t50_fid\n      elevation\n      geometry\n    \n  \n  \n    \n      4\n      2362630\n      2749\n      POINT (1378169.600 5158491.453)\n    \n    \n      5\n      2362814\n      2822\n      POINT (1389460.041 5168749.086)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      93\n      2380300\n      2711\n      POINT (1654213.379 5349962.973)\n    \n    \n      94\n      2380308\n      2885\n      POINT (1654898.622 5350462.779)\n    \n  \n\n70 rows × 3 columns\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[0])\nnz_height.plot(ax=base, color='None', edgecolor='red')\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[1])\ncanterbury.plot(ax=base, color='grey', edgecolor='none')\ncanterbury_height.plot(ax=base, color='None', edgecolor='red')\naxes[0].set_title('Original')\naxes[1].set_title('Subset (intersects)');\n\n\n\n\nFigure 3.1: Spatial subsetting of points by intersection with polygon\n\n\n\n\nLike in attribute subsetting (Section 2.3.1), we are using a boolean series (sel), of the same length as the number of rows in the filtered table (nz_height), created based on a condition applied on itself. The difference is that the condition is not a comparison of attribute values, but an evaluation of a spatial relation. Namely, we evaluate whether each geometry of nz_height intersects with canterbury geometry, using the .intersects method.\nVarious topological relations can be used for spatial subsetting which determine the type of spatial relationship that features in the target object must have with the subsetting object to be selected. These include touches, crosses or within, as we will see shortly in Section 3.3.2. The most commonly used method .intersects method which we used in the last example is a ‘catch all’ topological relation, that will return features in the target that touch, cross or are within the source ‘subsetting’ object. Alternatively, we can evaluate other methods, such as .disjoint to obtain all points that do not intersect with Canterbury:\n\n# Is each 'nz_height' point disjoint from 'canterbury'?\nsel = nz_height.disjoint(canterbury['geometry'].iloc[0])\ncanterbury_height2 = nz_height[sel]\n\nas shown in Figure 3.2:\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[0])\nnz_height.plot(ax=base, color='None', edgecolor='red')\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[1])\ncanterbury.plot(ax=base, color='grey', edgecolor='none')\ncanterbury_height2.plot(ax=base, color='None', edgecolor='red');\naxes[0].set_title('Original')\naxes[1].set_title('Subset (disjoint)');\n\n\n\n\nFigure 3.2: Spatial subsetting of points disjoint from a polygon\n\n\n\n\nIn case we need to subset according to several geometries at once, e.g., find out which points intersect with both Canterbury and Southland, we can dissolve the filtering subset before applying the .intersects (or any other) operator:\n\ncanterbury_southland = nz[(nz['Name'] == 'Canterbury') | (nz['Name'] == 'Southland')]\ncanterbury_southland = canterbury_southland.unary_union\ncanterbury_southland\n\n\n\n\n\nsel = nz_height.intersects(canterbury_southland)\ncanterbury_southland_height = nz_height[sel]\ncanterbury_southland_height\n\n\n\n\n\n  \n    \n      \n      t50_fid\n      elevation\n      geometry\n    \n  \n  \n    \n      0\n      2353944\n      2723\n      POINT (1204142.603 5049971.287)\n    \n    \n      4\n      2362630\n      2749\n      POINT (1378169.600 5158491.453)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      93\n      2380300\n      2711\n      POINT (1654213.379 5349962.973)\n    \n    \n      94\n      2380308\n      2885\n      POINT (1654898.622 5350462.779)\n    \n  \n\n71 rows × 3 columns\n\n\n\nThe resulting subset is shown in Figure 3.3:\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[0])\nnz_height.plot(ax=base, color='None', edgecolor='red')\nbase = nz.plot(color='white', edgecolor='lightgrey', ax=axes[1])\ngpd.GeoSeries(canterbury_southland).plot(ax=base, color='grey', edgecolor='none')\ncanterbury_southland_height.plot(ax=base, color='None', edgecolor='red')\naxes[0].set_title('Original')\naxes[1].set_title('Subset (intersects)');\n\n\n\n\nFigure 3.3: Spatial subsetting of points by intersection with more that one polygon\n\n\n\n\nThe next section further explores different types of spatial relation, also known as binary predicates (of which .intersects and .disjoint are two examples), that can be used to identify whether or not two features are spatially related or not.\n\n\n3.3.2 Topological relations\n\nTopological relations describe the spatial relationships between objects. “Binary topological relationships”, to give them their full name, are logical statements (in that the answer can only be True or False) about the spatial relationships between two objects defined by ordered sets of points (typically forming points, lines and polygons) in two or more dimensions (Egenhofer and Herring 1990). That may sound rather abstract and, indeed, the definition and classification of topological relations is based on mathematical foundations first published in book form in 1966 (Spanier 1995), with the field of algebraic topology continuing into the 21st century (Dieck 2008).\nDespite their mathematical origins, topological relations can be understood intuitively with reference to visualizations of commonly used functions that test for common types of spatial relationships.  Figure 4.2 shows a variety of geometry pairs and their associated relations. The third and fourth pairs in Figure 4.2 (from left to right and then down) demonstrate that, for some relations, order is important: while the relations equals, intersects, crosses, touches and overlaps are symmetrical, meaning that if function(x, y) is true, function(y, x) will also by true, relations in which the order of the geometries are important such as contains and within are not. Notice that each geometry pair has a “DE-9IM” string such as FF2F11212, described in the next section.\n\n\n\nTopological relations between vector geometries, inspired by Figures 1 and 2 in Egenhofer and Herring (1990). The relations for which the function(x, y) is true are printed for each geometry pair, with x represented in pink and y represented in blue.  The nature of the spatial relationship for each pair is described by the Dimensionally Extended 9-Intersection Model string.\n\n\nIn shapely, functions testing for different types of topological relations are known as “relationships”. To see how topological relations work in practice, let’s create a simple reproducible example, building on the relations illustrated in Figure 4.2 and consolidating knowledge of how vector geometries are represented from a previous chapter (Section 1.2.3 and Section 1.2.5):\n\npoints = gpd.GeoSeries([\n  shapely.geometry.Point(0.2,0.1), \n  shapely.geometry.Point(0.7,0.2), \n  shapely.geometry.Point(0.4,0.8)\n])\nline = gpd.GeoSeries([\n  shapely.geometry.LineString([(0.4,0.2), (1,0.5)])\n])\npoly = gpd.GeoSeries([\n  shapely.geometry.Polygon([(0,0), (0,1), (1,1), (1,0.5), (0,0)])\n])\n\nThe resulting GeoSeries named points, line, and poly are visualized as follows:\n\nax = gpd.GeoSeries(poly).plot(color='grey', edgecolor='red')\nax = gpd.GeoSeries(line).plot(ax=ax, color='black')\npoints.plot(ax=ax, color='none', edgecolor='black')\nfor x, y, label in zip(points.x, points.y, [1,2,3]):\n    ax.annotate(label, xy=(x, y), xytext=(3, 3), weight='bold', textcoords=\"offset points\")\n\n\n\n\nFigure 3.4: Points, line and polygon objects arranged to illustrate topological relations\n\n\n\n\nA simple query is: which of the points in points intersect in some way with polygon poly? The question can be answered by inspection (points 1 and 3 are touching and within the polygon, respectively). This question can be answered with the .intersects method, as follows. Note that we evaluate the relation between each geometry in a GeoSeries (points) and a single shapely geometry (poly.iloc[0]). When both inputs are GeoSeries, a pairwise evaluation takes place between geometries aligned by index (align=True, the default) or by position (align=False):\n\npoints.intersects(poly.iloc[0])\n\n0     True\n1    False\n2     True\ndtype: bool\n\n\nThe result is a boolean Series. Its contents should match your intuition: positive (True) results are returned for the first and third point, and a negative result (False) for the second are outside the polygon’s border.\nAgain, the above output is a Series where each value represents a feature in the first input (points). In case we need to obtain a matrix of pairwise results, we can use .apply as follows. In this case, the result is a DataFrame, where each row represents a points geometry and each column represents a poly geometry (there is just one, in this case)L\n\npoints.apply(lambda x: poly.intersects(x))\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      True\n    \n    \n      1\n      False\n    \n    \n      2\n      True\n    \n  \n\n\n\n\nThe .intersects method returns True even in cases where the features just touch: intersects is a ‘catch-all’ topological operation which identifies many types of spatial relation, as illustrated in Figure 4.2. More restrictive questions include which points lie within the polygon, and which features are on or contain a shared boundary with it? These can be answered as follows:\n\npoints.within(poly.iloc[0])\n\n0    False\n1    False\n2     True\ndtype: bool\n\n\n\npoints.touches(poly.iloc[0])\n\n0     True\n1    False\n2    False\ndtype: bool\n\n\nNote that although the first point touches the boundary polygon, it is not within it; the third point is within the polygon but does not touch any part of its border. The opposite of .intersects is .disjoint, which returns only objects that do not spatially relate in any way to the selecting object:\n\npoints.disjoint(poly.iloc[0])\n\n0    False\n1     True\n2    False\ndtype: bool\n\n\nAnother useful type of relation is “within distance”, where we detect features that intersect with the target buffered by particular distance. Buffer distance determines how close target objects need to be before they are selected. This can be done by literally buffering (Section 1.2.5) the target geometry, and evaluating intersection (.intersects, see above). Another way is to calculate the distances and compare them to the distance threshold:\n\npoints.distance(poly.iloc[0]) < 0.2\n\n0    True\n1    True\n2    True\ndtype: bool\n\n\nNote that although point 2 is more than 0.2 units of distance from the nearest vertex of poly, it is still selected when the distance is set to 0.2. This is because distance is measured to the nearest edge, in this case the part of the the polygon that lies directly above point 2 in Figure 4.2. We can verify the actual distance between point 2 and the polygon is 0.13, as follows:\n\npoints.iloc[1].distance(poly.iloc[0])\n\n0.13416407864998736\n\n\n\n\n3.3.3 DE-9IM strings\n…\n\n\n3.3.4 Spatial joining\nJoining two non-spatial datasets relies on a shared ‘key’ variable, as described in Section 2.3.3. Spatial data joining applies the same concept, but instead relies on spatial relations, described in the previous section. As with attribute data, joining adds new columns to the target object (the argument x in joining functions), from a source object (y).\nThe process is illustrated by the following example: imagine you have ten points randomly distributed across the Earth’s surface and you ask, for the points that are on land, which countries are they in? Implementing this idea in a reproducible example will build your geographic data handling skills and show how spatial joins work. The starting point is to create points that are randomly scattered over the Earth’s surface:\n\nnp.random.seed(11)  ## set seed for reproducibility\nbb = world.total_bounds  ## the world's bounds\nx = np.random.uniform(low=bb[0], high=bb[2], size=10)\ny = np.random.uniform(low=bb[1], high=bb[3], size=10)\nrandom_points = gpd.points_from_xy(x, y, crs=4326)\nrandom_points = gpd.GeoSeries(random_points)\nrandom_points = gpd.GeoDataFrame({'geometry': random_points})\nrandom_points\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POINT (-115.10291 36.78178)\n    \n    \n      1\n      POINT (-172.98891 -71.02938)\n    \n    \n      ...\n      ...\n    \n    \n      8\n      POINT (159.05039 -34.99599)\n    \n    \n      9\n      POINT (126.28622 -62.49509)\n    \n  \n\n10 rows × 1 columns\n\n\n\nThe scenario illustrated in Figure 3.5 shows that the random_points object (top left) lacks attribute data, while the world (top right) has attributes, including country names shown for a sample of countries in the legend. Spatial joins are implemented with gpd.sjoin, as illustrated in the code chunk below. The output is the random_joined object which is illustrated in Figure 3.5 (bottom left). Before creating the joined dataset, we use spatial subsetting to create world_random, which contains only countries that contain random points, to verify the number of country names returned in the joined dataset should be four (see the top right panel of Figure 3.5).\n\n# Subset\nworld_random = world[world.intersects(random_points.unary_union)]\nworld_random\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      4\n      US\n      United States\n      North America\n      ...\n      78.841463\n      51921.984639\n      MULTIPOLYGON (((-171.73166 63.7...\n    \n    \n      18\n      RU\n      Russian Federation\n      Europe\n      ...\n      70.743659\n      25284.586202\n      MULTIPOLYGON (((-180.00000 64.9...\n    \n    \n      52\n      ML\n      Mali\n      Africa\n      ...\n      57.007000\n      1865.160622\n      MULTIPOLYGON (((-11.51394 12.44...\n    \n    \n      159\n      AQ\n      Antarctica\n      Antarctica\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-180.00000 -89....\n    \n  \n\n4 rows × 11 columns\n\n\n\n\n# Spatial join\nrandom_joined = gpd.sjoin(random_points, world, how='left')\nrandom_joined\n\n\n\n\n\n  \n    \n      \n      geometry\n      index_right\n      iso_a2\n      ...\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      POINT (-115.10291 36.78178)\n      4.0\n      US\n      ...\n      318622525.0\n      78.841463\n      51921.984639\n    \n    \n      1\n      POINT (-172.98891 -71.02938)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8\n      POINT (159.05039 -34.99599)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n    \n      9\n      POINT (126.28622 -62.49509)\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n    \n  \n\n10 rows × 12 columns\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(9, 5))\nbase = world.plot(color='white', edgecolor='lightgrey', ax=axes[0][0])\nrandom_points.plot(ax=base, color='None', edgecolor='red')\nbase = world.plot(color='white', edgecolor='lightgrey', ax=axes[0][1])\nworld_random.plot(ax=base, column='name_long')\nbase = world.plot(color='white', edgecolor='lightgrey', ax=axes[1][0])\nrandom_joined.geometry.plot(ax=base, color='grey');\nrandom_joined.plot(ax=base, column='name_long', legend=True)\nfig.delaxes(axes[1][1]);\n\n\n\n\nFigure 3.5: Illustration of a spatial join. A new attribute variable is added to random points (top left) from source world object (top right) resulting in the data represented in the final panel.\n\n\n\n\n\n\n3.3.5 Non-overlapping joins\nSometimes two geographic datasets do not touch but still have a strong geographic relationship. The datasets cycle_hire and cycle_hire_osm, provide a good example. Plotting them shows that they are often closely related but they do not touch, as shown in Figure 3.6, which is created with the code below:\n\nbase = cycle_hire.plot(edgecolor='blue', color='none')\ncycle_hire_osm.plot(ax=base, edgecolor='red', color='none');\n\n\n\n\nFigure 3.6: The spatial distribution of cycle hire points in London based on official data (blue) and OpenStreetMap data (red).\n\n\n\n\nWe can check if any of the points are the same by creating a pairwise boolean matrix of .intersects relations, then evaluating whether any of the values in it is True. Note that the .to_numpy method is applied to go from a DataFrame to a numpy array, for which .any gives a global rather than a row-wise summary, which is what we want in this case:\n\nm = cycle_hire['geometry'].apply(\n  lambda x: cycle_hire_osm['geometry'].intersects(x)\n)\nm.to_numpy().any()\n\nFalse\n\n\nImagine that we need to join the capacity variable in cycle_hire_osm onto the official ‘target’ data contained in cycle_hire. This is when a non-overlapping join is needed. Spatial join (gpd.sjoin) along with buffered geometries can be used to do that. This is demonstrated below, using a threshold distance of 20 m. Note that we transform the data to a projected CRS (27700) to use real buffer distances, in meters.\n\ncrs = 27700\ncycle_hire2 = cycle_hire.copy().to_crs(crs)\ncycle_hire2['geometry'] = cycle_hire2.buffer(20)\nz = gpd.sjoin(\n  cycle_hire2, \n  cycle_hire_osm.to_crs(crs)\n)\nz\n\n\n\n\n\n  \n    \n      \n      id\n      name_left\n      area\n      ...\n      capacity\n      cyclestreets_id\n      description\n    \n  \n  \n    \n      0\n      1\n      River Street\n      Clerkenwell\n      ...\n      9.0\n      NaN\n      NaN\n    \n    \n      1\n      2\n      Phillimore Gardens\n      Kensington\n      ...\n      27.0\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      729\n      765\n      Ranelagh Gardens\n      Fulham\n      ...\n      29.0\n      NaN\n      NaN\n    \n    \n      737\n      773\n      Tallis Street\n      Temple\n      ...\n      14.0\n      NaN\n      NaN\n    \n  \n\n458 rows × 12 columns\n\n\n\nThe result z shows that there are 438 points in the target object cycle_hire within the threshold distance of cycle_hire_osm. Note that the number of rows in the joined result is greater than the target. This is because some cycle hire stations in cycle_hire have multiple matches in cycle_hire_osm. To aggregate the values for the overlapping points and return the mean, we can use the aggregation methods learned in @attr, resulting in an object with the same number of rows as the target. We also go back from buffers to points using .centroid:\n\nz = z[['id', 'capacity', 'geometry']] \\\n    .dissolve(by='id', aggfunc='mean') \\\n    .reset_index()\nz['geometry'] = z.centroid\nz\n\n\n\n\n\n  \n    \n      \n      id\n      geometry\n      capacity\n    \n  \n  \n    \n      0\n      1\n      POINT (531203.517 182832.066)\n      9.0\n    \n    \n      1\n      2\n      POINT (525208.067 179391.922)\n      27.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      436\n      765\n      POINT (524652.998 175817.001)\n      29.0\n    \n    \n      437\n      773\n      POINT (531435.032 180916.010)\n      14.0\n    \n  \n\n438 rows × 3 columns\n\n\n\nThe capacity of nearby stations can be verified by comparing a plot of the capacity of the source cycle_hire_osm data with the results in this new object z (Figure 3.7):\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,3))\ncycle_hire_osm.plot(column='capacity', legend=True, ax=axes[0])\nz.plot(column='capacity', legend=True, ax=axes[1])\naxes[0].set_title('Input')\naxes[1].set_title('Join result');\n\n\n\n\nFigure 3.7: Non-overlapping join input (left) and result (right)\n\n\n\n\n\n\n3.3.6 Spatial aggregation\nAs with attribute data aggregation, spatial data aggregation condenses data: aggregated outputs have fewer rows than non-aggregated inputs. Statistical aggregating functions, such as mean, average, or sum, summarise multiple values of a variable, and return a single value per grouping variable. Section 2.3.2 demonstrated how the .groupby method, combined with summary functions such as .sum, condense data based on attribute variables. This section shows how grouping by spatial objects can be acheived using spatial joins combined with non-spatial aggregation.\nReturning to the example of New Zealand, imagine you want to find out the average height of high points in each region. It is the geometry of the source (nz) that defines how values in the target object (nz_height) are grouped. This can be done in three steps:\n\nFiguring out which nz region each nz_height point falls in—using gpd.sjoin\nSummarizing the average elevation per region—using .groupby and .mean\nJoining the result back to nz—using pd.merge\n\nFirst, we ‘attach’ the region classification of each point, using spatial join. Note that we are using the minimal set of columns required: the geometries (for the spatial join to work), the point elevation (to later calculate an average), and the region name (to use as key when joining the results back to nz).\n\nnz_height2 = gpd.sjoin(\n  nz_height[['elevation', 'geometry']], \n  nz[['Name', 'geometry']], \n  how='left'\n)\nnz_height2\n\n\n\n\n\n  \n    \n      \n      elevation\n      geometry\n      index_right\n      Name\n    \n  \n  \n    \n      0\n      2723\n      POINT (1204142.603 5049971.287)\n      12\n      Southland\n    \n    \n      1\n      2820\n      POINT (1234725.325 5048309.302)\n      11\n      Otago\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      99\n      2720\n      POINT (1822262.592 5650428.656)\n      2\n      Waikato\n    \n    \n      100\n      2732\n      POINT (1822492.184 5650492.304)\n      2\n      Waikato\n    \n  \n\n101 rows × 4 columns\n\n\n\nSecond, we calculate the average elevation:\n\nnz_height3 = nz_height2.groupby('Name')[['elevation']].mean()\nnz_height3\n\n\n\n\n\n  \n    \n      \n      elevation\n    \n    \n      Name\n      \n    \n  \n  \n    \n      Canterbury\n      2994.600000\n    \n    \n      Manawatu-Wanganui\n      2777.000000\n    \n    \n      ...\n      ...\n    \n    \n      Waikato\n      2734.333333\n    \n    \n      West Coast\n      2889.454545\n    \n  \n\n7 rows × 1 columns\n\n\n\nThe third and final step is joining the averages with the nz layer:\n\nnz_height4 = pd.merge(nz[['Name', 'geometry']], nz_height3, on='Name', how='left')\nnz_height4\n\n\n\n\n\n  \n    \n      \n      Name\n      geometry\n      elevation\n    \n  \n  \n    \n      0\n      Northland\n      MULTIPOLYGON (((1745493.196 600...\n      NaN\n    \n    \n      1\n      Auckland\n      MULTIPOLYGON (((1803822.103 590...\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      14\n      Nelson\n      MULTIPOLYGON (((1624866.279 541...\n      NaN\n    \n    \n      15\n      Marlborough\n      MULTIPOLYGON (((1686901.914 535...\n      2720.0\n    \n  \n\n16 rows × 3 columns\n\n\n\nWe now have create the nz_height4 layer, which gives the average nz_height elevation value per polygon. The result is shown in Figure 3.8. Note that the missing_kwds part determines the style of geometries where the symbology attribute (elevation) is missing, because the were no nz_height points overlapping with them. The default is to omit them, which is usually not what we want. With {'color':'none','edgecolor':'black'}, those polygons are shown with black outline and no fill.\n\nnz_height4.plot(\n  column='elevation', \n  legend=True,\n  missing_kwds={'color': 'none', 'edgecolor': 'black'}\n);\n\n\n\n\nFigure 3.8: Average height of the top 101 high points across the regions of New Zealand\n\n\n\n\n\n\n3.3.7 Joining incongruent layers\nSpatial congruence is an important concept related to spatial aggregation. An aggregating object (which we will refer to as y) is congruent with the target object (x) if the two objects have shared borders. Often this is the case for administrative boundary data, whereby larger units—such as Middle Layer Super Output Areas (MSOAs) in the UK or districts in many other European countries—are composed of many smaller units.\nIncongruent aggregating objects, by contrast, do not share common borders with the target (Qiu, Zhang, and Zhou 2012). This is problematic for spatial aggregation (and other spatial operations) illustrated in Figure 3.9: aggregating the centroid of each sub-zone will not return accurate results. Areal interpolation overcomes this issue by transferring values from one set of areal units to another, using a range of algorithms including simple area weighted approaches and more sophisticated approaches such as ‘pycnophylactic’ methods (Tobler 1979).\nTo demonstrate, we will create a “synthetic” layer comprising a regular grid of rectangles of size \\(100\\times100\\) \\(km\\), covering the extent of the nz layer:\n\nxmin, ymin, xmax, ymax = nz.total_bounds\nres = 100000\ncols = list(range(int(np.floor(xmin)), int(np.ceil(xmax+res)), res))\nrows = list(range(int(np.floor(ymin)), int(np.ceil(ymax+res)), res))\nrows.reverse()\npolygons = []\nfor x in cols:\n    for y in rows:\n        polygons.append( shapely.geometry.Polygon([(x,y), (x+res, y), (x+res, y-res), (x, y-res)]) )\ngrid = gpd.GeoDataFrame({'geometry': polygons}, crs=nz.crs)\nsel = grid.intersects(shapely.geometry.box(*nz.total_bounds))\ngrid = grid[sel]\ngrid['id'] = grid.index\ngrid\n\n\n\n\n\n  \n    \n      \n      geometry\n      id\n    \n  \n  \n    \n      0\n      POLYGON ((1090143.000 6248536.0...\n      0\n    \n    \n      1\n      POLYGON ((1090143.000 6148536.0...\n      1\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      157\n      POLYGON ((1990143.000 4948536.0...\n      157\n    \n    \n      158\n      POLYGON ((1990143.000 4848536.0...\n      158\n    \n  \n\n150 rows × 2 columns\n\n\n\nas shown in Figure 3.9.\n\nbase = grid.plot(color='none', edgecolor='grey')\nnz.plot(ax=base, column='Population', edgecolor='black', legend=True);\n\n\n\n\nFigure 3.9: The nz layer and a regular grid of rectangles\n\n\n\n\nOur goal, now, is to “transfer” the Population attribute to the rectangular grid polygons, which is an example of a join between incongruent layers. To do that, we basically need to calculate–for each grid cell—the weighted sum of the population in nz polygons coinciding with that cell. The weights in the weighted sum calculation are the ratios between the area of the coinciding “part” out of the entire nz polygon. That is, we (inevitably) assume that the population in each nz polygon is equally distributed across space, therefore a partial nz polygon contains the respective partial population size.\nWe start with calculating the entire area of each nz polygon, as follows, using the .area method (see Section 1.2.7):\n\nnz['area'] = nz.area\nnz\n\n\n\n\n\n  \n    \n      \n      Name\n      Island\n      Land_area\n      ...\n      Sex_ratio\n      geometry\n      area\n    \n  \n  \n    \n      0\n      Northland\n      North\n      12500.561149\n      ...\n      0.942453\n      MULTIPOLYGON (((1745493.196 600...\n      1.289058e+10\n    \n    \n      1\n      Auckland\n      North\n      4941.572557\n      ...\n      0.944286\n      MULTIPOLYGON (((1803822.103 590...\n      4.911565e+09\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      14\n      Nelson\n      South\n      422.195242\n      ...\n      0.925967\n      MULTIPOLYGON (((1624866.279 541...\n      4.080754e+08\n    \n    \n      15\n      Marlborough\n      South\n      10457.745485\n      ...\n      0.957792\n      MULTIPOLYGON (((1686901.914 535...\n      1.046485e+10\n    \n  \n\n16 rows × 8 columns\n\n\n\nNext, we use the .overlay method to calculate the pairwise intersections between nz and grid, hereby named nz_grid. We also calculate the area of the intersections, hereby named area_sub. If an nz polygon was completely within a single grid polygon, then area_sub is going to be equal to area; otherwise, it is going to be smaller:\n\nnz_grid = nz.overlay(grid)\nnz_grid = nz_grid[['id', 'area', 'Population', 'geometry']]\nnz_grid['area_sub'] = nz_grid.area\nnz_grid\n\n\n\n\n\n  \n    \n      \n      id\n      area\n      Population\n      geometry\n      area_sub\n    \n  \n  \n    \n      0\n      64\n      1.289058e+10\n      175500.0\n      POLYGON ((1586362.965 6168009.0...\n      3.231015e+08\n    \n    \n      1\n      80\n      1.289058e+10\n      175500.0\n      POLYGON ((1590143.000 6162776.6...\n      4.612641e+08\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      108\n      87\n      4.080754e+08\n      51400.0\n      POLYGON ((1649908.695 5455398.2...\n      1.716260e+07\n    \n    \n      109\n      87\n      1.046485e+10\n      46200.0\n      MULTIPOLYGON (((1678688.086 545...\n      4.526248e+08\n    \n  \n\n110 rows × 5 columns\n\n\n\nWe are going to elaborate on the .overlay method in…\nThe resulting layer nz_grid is shown in Figure 3.10.\n\nbase = grid.plot(color='none', edgecolor='grey')\nnz_grid.plot(ax=base, column='Population', edgecolor='black', legend=True);\n\n\n\n\nFigure 3.10: The nz layer and a regular grid of rectangles: after an overlay operation\n\n\n\n\nNote that each of the “intersections” still holds the Population attribute of its “origin” feature of nz, as depicted in Figure 3.10. The real population size of each nz_grid feature, however, is smaller (or equal), depending on the geographic area proportion that it occupies out of the original nz feature. To make the “correction”, we first calculate the ratio (area_prop) and then multiply it by the population. The new (lowercase) attribute population now has the correct estimate of population sizes in nz_grid:\n\nnz_grid['area_prop'] = nz_grid['area_sub'] / nz_grid['area']\nnz_grid['population'] = nz_grid['Population'] * nz_grid['area_prop']\nnz_grid\n\n\n\n\n\n  \n    \n      \n      id\n      area\n      Population\n      ...\n      area_sub\n      area_prop\n      population\n    \n  \n  \n    \n      0\n      64\n      1.289058e+10\n      175500.0\n      ...\n      3.231015e+08\n      0.025065\n      4398.897141\n    \n    \n      1\n      80\n      1.289058e+10\n      175500.0\n      ...\n      4.612641e+08\n      0.035783\n      6279.925114\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      108\n      87\n      4.080754e+08\n      51400.0\n      ...\n      1.716260e+07\n      0.042057\n      2161.752203\n    \n    \n      109\n      87\n      1.046485e+10\n      46200.0\n      ...\n      4.526248e+08\n      0.043252\n      1998.239223\n    \n  \n\n110 rows × 7 columns\n\n\n\nWhat is left to be done is to sum (see Section 2.3.2) the population in all parts forming the same grid cell:\n\nnz_grid = nz_grid.groupby('id')['population'].sum().reset_index()\nnz_grid\n\n\n\n\n\n  \n    \n      \n      id\n      population\n    \n  \n  \n    \n      0\n      11\n      67.533590\n    \n    \n      1\n      12\n      15339.996965\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      55\n      149\n      31284.910446\n    \n    \n      56\n      150\n      129.326331\n    \n  \n\n57 rows × 2 columns\n\n\n\nand join (see Section 2.3.3) them back to the grid layer:\n\ngrid = pd.merge(grid, nz_grid[['id', 'population']], on='id', how='left')\ngrid\n\n\n\n\n\n  \n    \n      \n      geometry\n      id\n      population\n    \n  \n  \n    \n      0\n      POLYGON ((1090143.000 6248536.0...\n      0\n      NaN\n    \n    \n      1\n      POLYGON ((1090143.000 6148536.0...\n      1\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      148\n      POLYGON ((1990143.000 4948536.0...\n      157\n      NaN\n    \n    \n      149\n      POLYGON ((1990143.000 4848536.0...\n      158\n      NaN\n    \n  \n\n150 rows × 3 columns\n\n\n\nThe final result grid, with the incongruently-joined population attribute from nz, is shown in Figure 3.11.\n\nbase = grid.plot(column='population', edgecolor='black', legend=True);\nnz.plot(ax=base, color='none', edgecolor='grey', legend=True);\n\n\n\n\nFigure 3.11: The nz layer and a regular grid of rectangles: final result\n\n\n\n\nWe can demonstrate that, expectedly, the summed population in nz and grid is identical, even though the geometry is different (since we created grid to completely cover nz):\n\nnz['Population'].sum()\n\n4787200.0\n\n\n\ngrid['population'].sum()\n\n4787199.999999998\n\n\nThe procedure in this section is known as an area-weighted interpolation of a spatially extensive (e.g., population) variable. An area-weighted interpolation of a spatially intensive variable (e.g., population density) is almost identical, except that we would have to calculate the weighted .mean rather than .sum, to preserve the average rather than the sum.\n\n\n3.3.8 Distance relations\nWhile topological relations are binary — a feature either intersects with another or does not — distance relations are continuous. The distance between two objects is calculated with the distance method. The method is applied on a GeoSeries (or a GeoDataFrame), with the argument being an individual shapely geometry. The result is a Series of pairwise distances.\nThis is illustrated in the code chunk below, which finds the distance between the three highest point in New Zealand:\n\nnz_heighest = nz_height.sort_values(by='elevation', ascending=False).iloc[:3, :]\nnz_heighest\n\n\n\n\n\n  \n    \n      \n      t50_fid\n      elevation\n      geometry\n    \n  \n  \n    \n      64\n      2372236\n      3724\n      POINT (1369317.630 5169132.284)\n    \n    \n      63\n      2372235\n      3717\n      POINT (1369512.866 5168235.616)\n    \n    \n      67\n      2372252\n      3688\n      POINT (1369381.942 5168761.875)\n    \n  \n\n\n\n\nand the geographic centroid of the Canterbury region, created in Section 4.2.1:\n\ncanterbury_centroid = canterbury.centroid.iloc[0]\n\nHere are the distances:\n\nnz_heighest.distance(canterbury_centroid)\n\n64    115539.995747\n63    115390.248038\n67    115493.594066\ndtype: float64\n\n\nTo obtain a distance matrix, i.e., a pairwise set of distances between all combinations of features in objects x and y, we need to use the .apply method. This is illustrated in the command below, which finds the distances between the first three features in nz_height and the Otago and Canterbury regions of New Zealand represented by the object co:\n\nsel = nz['Name'].str.contains('Canter|Otag')\nco = nz[sel]\nco\n\n\n\n\n\n  \n    \n      \n      Name\n      Island\n      Land_area\n      ...\n      Sex_ratio\n      geometry\n      area\n    \n  \n  \n    \n      10\n      Canterbury\n      South\n      44504.499091\n      ...\n      0.975327\n      MULTIPOLYGON (((1686901.914 535...\n      4.532656e+10\n    \n    \n      11\n      Otago\n      South\n      31186.309188\n      ...\n      0.951169\n      MULTIPOLYGON (((1335204.789 512...\n      3.190356e+10\n    \n  \n\n2 rows × 8 columns\n\n\n\nThe distance matrix d is obtained as follows (technically speaking, this is a DataFrame). In plain language, we take the geometry from each each row in nz_height.iloc[:3, :], and apply the .distance method on co with that row as the argument:\n\nd = nz_height.iloc[:3, :].apply(lambda x: co.distance(x['geometry']), axis=1)\nd\n\n\n\n\n\n  \n    \n      \n      10\n      11\n    \n  \n  \n    \n      0\n      123537.158269\n      15497.717252\n    \n    \n      1\n      94282.773074\n      0.000000\n    \n    \n      2\n      93018.560814\n      0.000000\n    \n  \n\n\n\n\nNote that the distance between the second and third features in nz_height and the second feature in co is zero. This demonstrates the fact that distances between points and polygons refer to the distance to any part of the polygon: The second and third points in nz_height are in Otago, which can be verified by plotting them:\n\nbase = co.iloc[[1]].plot(color='none')\nnz_height.iloc[1:3, :].plot(ax=base);"
  },
  {
    "objectID": "04-spatial-operations.html#sec-spatial-ras",
    "href": "04-spatial-operations.html#sec-spatial-ras",
    "title": "3  Spatial data operations",
    "section": "3.4 Spatial operations on raster data",
    "text": "3.4 Spatial operations on raster data\n\n3.4.1 Spatial subsetting\nThe previous chapter (Section Section 2.4) demonstrated how to retrieve values associated with specific cell IDs or row and column combinations. Raster objects can also be extracted by location (coordinates) and other spatial objects. To use coordinates for subsetting, we can use the .sample method of a rasterio file connection object, combined with a list of coordinate tuples. The methods is demonstrated below to find the value of the cell that covers a point located at coordinates of 0.1, 0.1 in elev. The returned object is a generator:\n\nsrc_elev.sample([(0.1, 0.1)])\n\n<generator object sample_gen at 0x7f199eada400>\n\n\nIn case we want all values at once we can apply list. The result is 16:\n\nlist(src_elev.sample([(0.1, 0.1)]))\n\n[array([16], dtype=uint8)]\n\n\nRaster objects can also be subset with another raster object, as demonstrated in the code chunk below:\n…\n\n# ...\n\nAnother common use case of spatial subsetting is using a boolean mask, based on another raster with the same extent and resolution, or the original one, as illustrated in Figure …. To do that, we “erase” the values in the array of one raster, according to another corresponding “mask” raster. For example, let us read the elev.tif raster array:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nand create a correspinding random mask:\n\nnp.random.seed(1)\nmask = np.random.choice([True, False], elev.shape)\nmask\n\narray([[False, False,  True,  True, False, False],\n       [False, False, False,  True,  True, False],\n       [ True, False, False,  True,  True, False],\n       [ True,  True,  True, False,  True,  True],\n       [False,  True,  True,  True, False,  True],\n       [ True,  True, False, False, False, False]])\n\n\nIn the code chunk above, we have created a mask object called mask with values randomly assigned to True and False. Next, we want to keep those values of elev which are False in mask (i.e., they are not masked). In other words, we want to mask elev with mask. The result is stored in a copy named elev1. To be able to store np.nan in the raster, we also need to convert it to float (see Section 2.4.2):\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1[mask] = np.nan\nelev1\n\narray([[ 1.,  2., nan, nan,  5.,  6.],\n       [ 7.,  8.,  9., nan, nan, 12.],\n       [nan, 14., 15., nan, nan, 18.],\n       [nan, nan, nan, 22., nan, nan],\n       [25., nan, nan, nan, 29., nan],\n       [nan, nan, 33., 34., 35., 36.]])\n\n\nThe result is shown in Figure 3.12.\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nshow(elev, ax=axes[0])\nshow(mask, ax=axes[1])\nshow(elev1, ax=axes[2])\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Mask\")\naxes[2].set_title(\"Result\");\n\n\n\n\nFigure 3.12: Original raster (left). Raster mask (middle). Output of masking a raster (right).\n\n\n\n\nThe above approach can be also used to replace some values (e.g., expected to be wrong) with nan:\n\nelev1 = elev.copy()\nelev1 = elev1.astype('float64')\nelev1[elev1 < 20] = np.nan\nelev1\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, 20., 21., 22., 23., 24.],\n       [25., 26., 27., 28., 29., 30.],\n       [31., 32., 33., 34., 35., 36.]])\n\n\nThese operations are in fact Boolean local operations, since we compare cell-wise two rasters. The next subsection explores these and related operations in more detail.\n\n\n3.4.2 Map algebra\nThe term ‘map algebra’ was coined in the late 1970s to describe a “set of conventions, capabilities, and techniques” for the analysis of geographic raster and (although less prominently) vector data (Tomlin 1994). In this context, we define map algebra more narrowly, as operations that modify or summarise raster cell values, with reference to surrounding cells, zones, or statistical functions that apply to every cell.\nMap algebra operations tend to be fast, because raster datasets only implicitly store coordinates, hence the old adage “raster is faster but vector is corrector”. The location of cells in raster datasets can be calculated by using its matrix position and the resolution and origin of the dataset (stored in the header). For the processing, however, the geographic position of a cell is barely relevant as long as we make sure that the cell position is still the same after the processing. Additionally, if two or more raster datasets share the same extent, projection and resolution, one could treat them as matrices for the processing.\nThis is the way that map algebra works with the terra package. First, the headers of the raster datasets are queried and (in cases where map algebra operations work on more than one dataset) checked to ensure the datasets are compatible. Second, map algebra retains the so-called one-to-one locational correspondence, meaning that cells cannot move. This differs from matrix algebra, in which values change position, for example when multiplying or dividing matrices.\nMap algebra (or cartographic modeling with raster data) divides raster operations into four subclasses (Tomlin 1990), with each working on one or several grids simultaneously:\n\nLocal or per-cell operations\nFocal or neighborhood operations. Most often the output cell value is the result of a 3 x 3 input cell block\nZonal operations are similar to focal operations, but the surrounding pixel grid on which new values are computed can have irregular sizes and shapes\nGlobal or per-raster operations; that means the output cell derives its value potentially from one or several entire rasters\n\nThis typology classifies map algebra operations by the number of cells used for each pixel processing step and the type of the output. For the sake of completeness, we should mention that raster operations can also be classified by discipline such as terrain, hydrological analysis, or image classification. The following sections explain how each type of map algebra operations can be used, with reference to worked examples.\n\n\n3.4.3 Local operations\nLocal operations comprise all cell-by-cell operations in one or several layers. Raster algebra is a classical use case of local operations - this includes adding or subtracting values from a raster, squaring and multipling rasters. Raster algebra also allows logical operations such as finding all raster cells that are greater than a specific value (5 in our example below). Local operations are applied using the numpy array operations syntax, as demonstrated below:\nFirst, we need to read raster values:\n\nelev = src_elev.read(1)\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\nNow, any element-wise array operation can be applied. For example:\n\nelev + elev\n\narray([[ 2,  4,  6,  8, 10, 12],\n       [14, 16, 18, 20, 22, 24],\n       [26, 28, 30, 32, 34, 36],\n       [38, 40, 42, 44, 46, 48],\n       [50, 52, 54, 56, 58, 60],\n       [62, 64, 66, 68, 70, 72]], dtype=uint8)\n\n\nFigure 3.13 demonstrates a few more examples.\n\nfig, axes = plt.subplots(ncols=4, figsize=(9,5))\nshow(elev + elev, ax=axes[0], cmap=\"Oranges\")\nshow(elev ** 2, ax=axes[1], cmap=\"Oranges\")\nshow(np.log(elev), ax=axes[2], cmap=\"Oranges\")\nshow(elev > 5, ax=axes[3], cmap=\"Oranges\")\naxes[0].set_title(\"elev+elev\")\naxes[1].set_title(\"elev ** 2\")\naxes[2].set_title(\"np.log(elev)\")\naxes[3].set_title(\"elev > 5\");\n\n\n\n\nFigure 3.13: Examples of different local operations of the elev raster object: adding two rasters, squaring, applying logarithmic transformation, and performing a logical operation.\n\n\n\n\nAnother good example of local operations is the classification of intervals of numeric values into groups such as grouping a digital elevation model into low (class 1), middle (class 2) and high elevations (class 3). Here, we assign the raster values in the ranges 0–12, 12–24 and 24–36 are reclassified to take values 1, 2 and 3, respectively.\n\nrecl = elev.copy()\nrecl[(elev > 0)  & (elev <= 12)] = 1\nrecl[(elev > 12) & (elev <= 24)] = 2\nrecl[(elev > 24) & (elev <= 36)] = 3\n\nThe reclassified result is shown in Figure 3.14.\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\nshow(elev, ax=axes[0], cmap=\"Oranges\")\nshow(recl, ax=axes[1], cmap=\"Oranges\")\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Reclassified\");\n\n\n\n\nFigure 3.14: Reclassifying a continuous raster into three categories.\n\n\n\n\nThe calculation of the normalized difference vegetation index (NDVI) is a well-known local (pixel-by-pixel) raster operation. It returns a raster with values between -1 and 1; positive values indicate the presence of living plants (mostly > 0.2). NDVI is calculated from red and near-infrared (NIR) bands of remotely sensed imagery, typically from satellite systems such as Landsat or Sentinel. Vegetation absorbs light heavily in the visible light spectrum, and especially in the red channel, while reflecting NIR light, explaining the NVDI formula:\n\\[NDVI=\\frac{NIR-Red} {NIR+Red}\\]\nLet’s calculate NDVI for the multispectral satellite file of the Zion National Park.\n\nmulti_rast = src_multi_rast.read()\nnir = multi_rast[3,:,:]\nred = multi_rast[2,:,:]\nndvi = (nir-red)/(nir+red)\n\nConvert values >1 to “No Data”:\n\nndvi[ndvi>1] = np.nan\n\nWhen plotting an RGB image using the show function, the function assumes that:\n\nValues are in the range [0,1] for floats, or [0,255] for integers (otherwise clipped)\nThe order of bands is RGB\n\nTo “prepare” the multi-band raster for show, we therefore reverse the order of bands (which is originally BGR+NIR), and divided by the maximum to set the maximum value at 1:\n\nmulti_rast_rgb = multi_rast[(2,1,0), :, :] / multi_rast.max()\n\nThe result is shown in Figure 3.15.\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\nshow(multi_rast_rgb, ax=axes[0], cmap=\"RdYlGn\")\nshow(ndvi, ax=axes[1], cmap=\"Greens\")\naxes[0].set_title(\"RGB image\")\naxes[1].set_title(\"NDVI\");\n\n\n\n\nFigure 3.15: RGB image (left) and NDVI values (right) calculated for the example satellite file of the Zion National Park.\n\n\n\n\n\n\n3.4.4 Focal operations\nWhile local functions operate on one cell, though possibly from multiple layers, focal operations take into account a central (focal) cell and its neighbors. The neighborhood (also named kernel, filter or moving window) under consideration is typically of size 3-by-3 cells (that is the central cell and its eight surrounding neighbors), but can take on any other (not necessarily rectangular) shape as defined by the user. A focal operation applies an aggregation function to all cells within the specified neighborhood, uses the corresponding output as the new value for the the central cell, and moves on to the next central cell (Figure …). Other names for this operation are spatial filtering and convolution (Burrough, McDonnell, and Lloyd 2015).\nIn Python, the scipy.ndimage package has a comprehensive collection of functions to perform filtering of numpy arrays, such as:\n\nminimum_filter\nmaximum_filter\nuniform_filter (i.e., mean filter)\nmedian_filter etc.\n\nIn this group of functions, we define the shape of the moving window with either one of:\n\nsize—a single number or tuple, implying a filter of those dimensions\nfootprint—a boolean array, representing both the window shape and the identity of elements being included\n\nIn addition to specific built-in filters,\n\nconvolve applies the sum function after multiplying by a custom weights array\ngeneric_filter makes it possible to pass any custom function, where the user can specify any type of custom window-based calculatio.\n\nFor example, here we apply the minimum filter with window size of 3 on elev:\n\nelev\n\narray([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18],\n       [19, 20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29, 30],\n       [31, 32, 33, 34, 35, 36]], dtype=uint8)\n\n\n\nelev_min = scipy.ndimage.minimum_filter(elev, size=3)\nelev_min\n\narray([[ 1,  1,  2,  3,  4,  5],\n       [ 1,  1,  2,  3,  4,  5],\n       [ 7,  7,  8,  9, 10, 11],\n       [13, 13, 14, 15, 16, 17],\n       [19, 19, 20, 21, 22, 23],\n       [25, 25, 26, 27, 28, 29]], dtype=uint8)\n\n\nSpecial care should be given to the edge pixels. How should they be calculated? scipy.ndimage gives several options through the mode parameter:\n\nreflect (the default)\nconstant\nnearest\nmirror\nwrap\n\nSometimes artificially extending raster edges is considered unsuitable. In other words, we may wish the resulting raster to contain pixel values with “complete” windows only, for example to have a uniform sample size or because values in all directions matter (such as in topographic calculations). There is no specific option not to extend edges in scipy.ndimage. However, to get the same effect, the edges of the filtered array can be assigned with nan, in a number of rows and columns according to filter size. For example, when using a filter of size=3, the first “layer” of pixels may be assigned with nan, reflecting the fact that these pixels have incomplete 3*3 neighborhoods:\n\nelev_min = elev_min.astype('float')\nelev_min[:, [0, -1]] = np.nan\nelev_min[[0, -1], :] = np.nan\nelev_min\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan,  1.,  2.,  3.,  4., nan],\n       [nan,  7.,  8.,  9., 10., nan],\n       [nan, 13., 14., 15., 16., nan],\n       [nan, 19., 20., 21., 22., nan],\n       [nan, nan, nan, nan, nan, nan]])\n\n\nWe can quickly check if the output meets our expectations. In our example, the minimum value has to be always the upper left corner of the moving window (remember we have created the input raster by row-wise incrementing the cell values by one starting at the upper left corner).\nFocal functions or filters play a dominant role in image processing. Low-pass or smoothing filters use the mean function to remove extremes. In the case of categorical data, we can replace the mean with the mode, which is the most common value. By contrast, high-pass filters accentuate features. The line detection Laplace and Sobel filters might serve as an example here.\nTerrain processing, the calculation of topographic characteristics such as slope, aspect and flow directions, relies on focal functions. The TerrainAttribute function from package richdem can be used to calculate common metrics, specified through the attrib argument, namely:\n\nslope_riserun Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_percentage Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_degrees Horn (1981) doi: 10.1109/PROC.1981.11918\nslope_radians Horn (1981) doi: 10.1109/PROC.1981.11918\naspect Horn (1981) doi: 10.1109/PROC.1981.11918\ncurvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\nplanform_curvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\nprofile_curvature Zevenbergen and Thorne (1987) doi: 10.1002/esp.3290120107\n\n\n\n3.4.5 Zonal operations\nJust like focal operations, zonal operations apply an aggregation function to multiple raster cells. However, a second raster, usually with categorical values, defines the zonal filters (or ‘zones’) in the case of zonal operations, as opposed to a predefined neighborhood window in the case of focal operation presented in the previous section. Consequently, raster cells defining the zonal filter do not necessarily have to be neighbors. Our grain size raster is a good example, as illustrated in the right panel of Figure 3.2: different grain sizes are spread irregularly throughout the raster. Finally, the result of a zonal operation is a summary table grouped by zone which is why this operation is also known as zonal statistics in the GIS world. This is in contrast to focal operations which return a raster object.\nTo demonstrate, let us get back to the grain and elev rasters (Figure 3.2). To calculate zonal statistics, we use the arrays with raster values. The elev array was already imported earlier:\n\ngrain = src_grain.read(1)\ngrain\n\narray([[1, 0, 1, 2, 2, 2],\n       [0, 2, 0, 0, 2, 1],\n       [0, 2, 2, 0, 0, 2],\n       [0, 0, 1, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1],\n       [2, 1, 2, 2, 0, 2]], dtype=uint8)\n\n\nOur interntion is to calculate the average (or any other summary function, for that matter) of elevation in each zone defined by grain values. First, we can obtain the unique values defining the zones using np.unique:\n\nnp.unique(grain)\n\narray([0, 1, 2], dtype=uint8)\n\n\nNow, we can use dictionary conprehension to “split” the elev array into separate one-dimensional arrays with values per grain group, with keys being the unique grain values:\n\nz = {i: elev[grain == i] for i in np.unique(grain)}\nz\n\n{0: array([ 2,  7,  9, 10, 13, 16, 17, 19, 20, 35], dtype=uint8),\n 1: array([ 1,  3, 12, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32], dtype=uint8),\n 2: array([ 4,  5,  6,  8, 11, 14, 15, 18, 28, 31, 33, 34, 36], dtype=uint8)}\n\n\nAt this stage, we can expand the dictionary comprehension expression to calculate the mean elevation associated with each grain size class. Instead of placing the elevation values (elev[grain==i]) into the dictionary values, we place their mean (elev[grain==i].mean()):\n\nz = {i: elev[grain == i].mean() for i in np.unique(grain)}\nz\n\n{0: 14.8, 1: 21.153846153846153, 2: 18.692307692307693}\n\n\nThis returns the statistics for each category, here the mean elevation for each grain size class. For example, the mean elevation in pixels characterized by grain size 0 is 14.8, and so on.\n\n\n3.4.6 Global operations and distances\n…\n\n\n3.4.7 Map algebra counterparts in vector processing\n…\n\n\n3.4.8 Merging rasters\nSuppose we would like to compute the NDVI (see Section 3.4.3), and additionally want to compute terrain attributes from elevation data for observations within a study area. Such computations rely on remotely sensed information. The corresponding imagery is often divided into scenes covering a specific spatial extent (i.e., “tiles”), and frequently, a study area covers more than one scene. Then, we would need to merge (also known as “mosaic”) the scenes covered by our study area. In the easiest case, we can just merge these scenes, that is put them side by side. However, the procedure is the same regardless of the number of scenes. In case when all scenes are “aligned” (i.e., share the same origin and resolution), this can be thought of as simply gluing them into one big raster; otherwise, all scenes are resampled (see Section 4.4.4) to the grid defined by the first scene.\nFor example, let us merge digital elevation data from two SRTM elevation tiles, for Austria (aut.tif) and Switzerland (ch.tif). Merging can be done using function rasterio.merge.merge, which accepts a list of raster file connections, and returns the new ndarray and a “transform”:\n\nsrc_1 = rasterio.open('data/aut.tif')\nsrc_2 = rasterio.open('data/ch.tif')\nout_image, out_transform = rasterio.merge.merge([src_1, src_2])\n\nBoth inputs and the result are shown in Figure 3.16:\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,6))\nshow(src_1, ax=axes[0]);\nshow(src_2, ax=axes[1]);\nshow(out_image, transform=out_transform, ax=axes[2])\naxes[0].set_title('aut.tif')\naxes[1].set_title('ch.tif')\naxes[2].set_title('Mosaic (aut.tif+ch.tif)');\n\n\n\n\nFigure 3.16: Raster merging\n\n\n\n\nBy default (method='first), areas of overlap retain the value of the first raster. Other possible methods are:\n\nlast—Value of the last raster\nmin—Minimum value\nmax—Maximum value\n\nWhen dealing with non-overlapping tiles, such as aut.tif and ch.tif (above), the method argument has no practical effect. However, it becomes relevant when we want to combine spectral imagery from scenes that were taken on different dates. The above four options for method do not cover the commonly required scenario when we would like to compute the mean value—for example to calculate a seasonal average NDVI image from a set of partially overlapping satellite images (such as Landsat). An alternative worflow to rasterio.merge.merge, for calculating a mosaic as well as “averaging” any overlaps, could be to go through two steps:\n\nResampling all scenes into a common “global” grid (Section 4.4.4), thereby producing a series of “matching” rasters (with the area surrounding each scene set as “No Data”)\nAveraging the rasters through raster algebra (Section 3.4.3), as in np.mean(m,axis=0) or np.nanmean(m,axis=0), where m is the multi-band array, which would return a single-band array of averages"
  },
  {
    "objectID": "04-spatial-operations.html#exercises",
    "href": "04-spatial-operations.html#exercises",
    "title": "3  Spatial data operations",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nWrite a function which accepts and array and an int specifying the number of rows/columns to erase along an array edges. The function needs to return the modified array with np.nan values along its edges."
  },
  {
    "objectID": "05-geometry-operations.html#prerequisites",
    "href": "05-geometry-operations.html#prerequisites",
    "title": "4  Geometry operations",
    "section": "4.1 Prerequisites",
    "text": "4.1 Prerequisites\nPackages…\n\nimport numpy as np\nimport shapely.geometry\nimport geopandas as gpd\nimport topojson as tp\nimport rasterio\nimport rasterio.warp\nimport rasterio.plot\n\nSample data…\n\nseine = gpd.read_file('data/seine.gpkg')\nus_states = gpd.read_file('data/us_states.gpkg')\nnz = gpd.read_file('data/nz.gpkg')"
  },
  {
    "objectID": "05-geometry-operations.html#introduction",
    "href": "05-geometry-operations.html#introduction",
    "title": "4  Geometry operations",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nSo far the book has explained the structure of geographic datasets (Chapter 2), and how to manipulate them based on their non-geographic attributes (Chapter 3) and spatial relations (Chapter 4). This chapter focusses on manipulating the geographic elements of geographic objects, for example by simplifying and converting vector geometries, cropping raster datasets, and converting vector objects into rasters and from rasters into vectors. After reading it—and attempting the exercises at the end—you should understand and have control over the geometry column in sf objects and the extent and geographic location of pixels represented in rasters in relation to other geographic objects.\nSection 4.3 covers transforming vector geometries with ‘unary’ and ‘binary’ operations. Unary operations work on a single geometry in isolation, including simplification (of lines and polygons), the creation of buffers and centroids, and shifting/scaling/rotating single geometries using ‘affine transformations’ (Section 4.3.1 to Section 4.3.4). Binary transformations modify one geometry based on the shape of another, including clipping and geometry unions, covered in Section 4.3.5 and Section 4.3.7, respectively. Type transformations (from a polygon to a line, for example) are demonstrated in Section Section 4.3.8.\nSection 4.4 covers geometric transformations on raster objects. This involves changing the size and number of the underlying pixels, and assigning them new values. It teaches how to change the resolution (also called raster aggregation and disaggregation), the extent and the origin of a raster. These operations are especially useful if one would like to align raster datasets from diverse sources. Aligned raster objects share a one-to-one correspondence between pixels, allowing them to be processed using map algebra operations, described in Section 4.3.2. The final Section 6 connects vector and raster objects. It shows how raster values can be ‘masked’ and ‘extracted’ by vector geometries. Importantly it shows how to ‘polygonize’ rasters and ‘rasterize’ vector datasets, making the two data models more interchangeable."
  },
  {
    "objectID": "05-geometry-operations.html#sec-geo-vec",
    "href": "05-geometry-operations.html#sec-geo-vec",
    "title": "4  Geometry operations",
    "section": "4.3 Geometric operations on vector data",
    "text": "4.3 Geometric operations on vector data\nThis section is about operations that in some way change the geometry of vector layers. It is more advanced than the spatial data operations presented in the previous chapter (in Section 3.3), because here we drill down into the geometry: the functions discussed in this section work on the geometric (GeoSeries) part, either as standalone object or as part of a GeoDataFrame.\n\n4.3.1 Simplification\nSimplification is a process for generalization of vector objects (lines and polygons) usually for use in smaller scale maps. Another reason for simplifying objects is to reduce the amount of memory, disk space and network bandwidth they consume: it may be wise to simplify complex geometries before publishing them as interactive maps. The geopandas package provides the .simplify method, which uses the GEOS implementation of the Douglas-Peucker algorithm to reduce the vertex count. .simplify uses the tolerance to control the level of generalization in map units (see Douglas and Peucker 1973 for details).\nFor example, a simplified geometry of a \"LineString\" geometry, representing the river Seine and tributaries, using tolerance of 2000 meters, can created using the following command:\n\nseine_simp = seine.simplify(2000)  # 2000 m\n\nFigure Figure 4.1 illustrates the input and the result of the simplification:\n\nfig, axes = plt.subplots(ncols=2)\nseine.plot(ax=axes[0])\nseine_simp.plot(ax=axes[1])\naxes[0].set_title('Original')\naxes[1].set_title('Simplified (d=2000 m)');\n\n\n\n\nFigure 4.1: Comparison of the original and simplified geometry of the seine object.\n\n\n\n\nThe resulting seine_simp object is a copy of the original seine but with fewer vertices. This is apparent, with the result being visually simpler (Figure 4.1, right) and consuming less memory than the original object, as verified below:\n\nimport sys\nsys.getsizeof(seine)       ## Original (bytes)\n\n370\n\n\n\nsys.getsizeof(seine_simp)  ## Simplified (bytes)\n\n184\n\n\nSimplification is also applicable for polygons. This is illustrated using us_states, representing the contiguous United States. As we show in Chapter 6, GEOS assumes that the data is in a projected CRS and this could lead to unexpected results when using a geographic CRS. Therefore, the first step is to project the data into some adequate projected CRS, such as US National Atlas Equal Area (epsg = 2163) (on the left in Figure Figure 4.2):\n\nus_states2163 = us_states.to_crs(2163)\n\nThe .simplify method from geopandas works the same way with a \"Polygon\"/\"MultiPolygon\" layer such as us_states2163:\n\nus_states_simp1 = us_states2163.simplify(100000)\n\nA limitation with .simplify is that it simplifies objects on a per-geometry basis. This means the “topology” is lost, resulting in overlapping and “holey” areal units illustrated in Figure Figure 4.2 (middle panel). The toposimplify function from topojson provides an alternative that overcomes this issue. By default it uses the Douglas-Peucker algorithm like the .simplify method. Another algorithm known as Visvalingam-Whyatt, which overcomes some limitations of the Douglas-Peucker algorithm (Visvalingam and Whyatt 1993), is also available in toposimplify. The main advanatage of toposimplify, however, is that it is topologically “aware”. That is, it simplifies the combined borders of the polygons (rather than each polygon on its own), thus ensuring that the overlap is maintained. The following code chunk uses this function to simplify us_states2163:\n\ntopo = tp.Topology(us_states2163, prequantize=False)\nus_states_simp2 = topo.toposimplify(100000).to_gdf()\n\n/usr/local/lib/python3.11/site-packages/topojson/core/dedup.py:107: RuntimeWarning: invalid value encountered in cast\n  data[\"bookkeeping_shared_arcs\"] = array_bk_sarcs.astype(np.int64).tolist()\n\n\nFigure Figure 4.2 demonstrates the two simplification methods applied to us_states2163.\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nus_states2163.plot(ax=axes[0], color='lightgrey', edgecolor='black')\nus_states_simp1.plot(ax=axes[1], color='lightgrey', edgecolor='black')\nus_states_simp2.plot(ax=axes[2], color='lightgrey', edgecolor='black')\naxes[0].set_title(\"Original\")\naxes[1].set_title(\"Simplified (w/ geopandas)\")\naxes[2].set_title(\"Simplified (w/ topojson)\");\n\n\n\n\nFigure 4.2: Polygon simplification in action, comparing the original geometry of the contiguous United States with simplified versions, generated with functions from the geopandas (middle), and topojson (right), packages.\n\n\n\n\n\n\n4.3.2 Centroids\nCentroid operations identify the center of geographic objects. Like statistical measures of central tendency (including mean and median definitions of ‘average’), there are many ways to define the geographic center of an object. All of them create single point representations of more complex vector objects.\nThe most commonly used centroid operation is the geographic centroid. This type of centroid operation (often referred to as ‘the centroid’) represents the center of mass in a spatial object (think of balancing a plate on your finger). Geographic centroids have many uses, for example to create a simple point representation of complex geometries, or to estimate distances between polygons. Centroids of the geometries in a GeoSeries or a GeoDataFrame are accessible through the .centroid property, as demonstrated in the code below, which generates the geographic centroids of regions in New Zealand and tributaries to the River Seine, illustrated with black points in Figure 4.3.\n\nnz_centroid = nz.centroid\nseine_centroid = seine.centroid\n\nSometimes the geographic centroid falls outside the boundaries of their parent objects (think of a doughnut). In such cases point on surface operations can be used to guarantee the point will be in the parent object (e.g., for labeling irregular multipolygon objects such as island states), as illustrated by the red points in Figure 4.3. Notice that these red points always lie on their parent objects. They were created with the representative_point method, as follows:\n\nnz_pos = nz.representative_point()\nseine_pos = seine.representative_point()\n\nThe centroids and points in surface are illustrated in Figure 4.3:\n\nfig, axes = plt.subplots(ncols=2)\nnz.plot(ax=axes[0], color=\"white\", edgecolor=\"lightgrey\")\nnz_centroid.plot(ax=axes[0], color=\"None\", edgecolor=\"black\")\nnz_pos.plot(ax=axes[0], color=\"None\", edgecolor=\"red\")\nseine.plot(ax=axes[1], color=\"grey\")\nseine_pos.plot(ax=axes[1], color=\"None\", edgecolor=\"red\")\nseine_centroid.plot(ax=axes[1], color=\"None\", edgecolor=\"black\");\n\n\n\n\nFigure 4.3: Centroids (black) and points on surface red of New Zealand and Seine datasets.\n\n\n\n\n\n\n4.3.3 Buffers\nBuffers are polygons representing the area within a given distance of a geometric feature: regardless of whether the input is a point, line or polygon, the output is a polygon. Unlike simplification (which is often used for visualization and reducing file size) buffering tends to be used for geographic data analysis. How many points are within a given distance of this line? Which demographic groups are within travel distance of this new shop? These kinds of questions can be answered and visualized by creating buffers around the geographic entities of interest.\nFigure 4.4 illustrates buffers of different sizes (5 and 50 km) surrounding the river Seine and tributaries. These buffers were created with commands below, which show that the .buffer method, applied to a GeoSeries (or GeoDataFrame) requires one important argument: the buffer distance, provided in the units of the CRS (in this case meters):\n\nseine_buff_5km = seine.buffer(5000)\nseine_buff_50km = seine.buffer(50000)\n\nThe 5 and 50 km buffers are visualized in Figure 4.4:\n\nfig, axes = plt.subplots(ncols=2)\nseine_buff_5km.plot(ax=axes[0], color=\"none\", edgecolor=[\"red\", \"green\", \"blue\"])\nseine_buff_50km.plot(ax=axes[1], color=\"none\", edgecolor=[\"red\", \"green\", \"blue\"])\naxes[0].set_title(\"5 km buffer\")\naxes[1].set_title(\"50 km buffer\");\n\n\n\n\nFigure 4.4: Buffers around the Seine dataset of 5 km (left) and 50 km (right). Note the colors, which reflect the fact that one buffer is created per geometry feature.\n\n\n\n\nNote that both .centroid and .buffer return a GeoSeries object, even when the input is a GeoDataFrame:\n\nseine_buff_5km\n\n0    POLYGON ((657550.332 6852587.97...\n1    POLYGON ((517151.801 6930724.10...\n2    POLYGON ((701519.740 6813075.49...\ndtype: geometry\n\n\nIn the common scenario when the original attributes of the input features need to be retained, you can replace the existing geometry with the new GeoSeries as in:\n\nseine_buff_5km = seine.copy()\nseine_buff_5km['geometry'] = seine.buffer(5000)\nseine_buff_5km\n\n\n\n\n\n  \n    \n      \n      name\n      geometry\n    \n  \n  \n    \n      0\n      Marne\n      POLYGON ((657550.332 6852587.97...\n    \n    \n      1\n      Seine\n      POLYGON ((517151.801 6930724.10...\n    \n    \n      2\n      Yonne\n      POLYGON ((701519.740 6813075.49...\n    \n  \n\n\n\n\n\n\n4.3.4 Affine transformations\nAffine transformation is any transformation that preserves lines and parallelism. However, angles or length are not necessarily preserved. Affine transformations include, among others, shifting (translation), scaling and rotation. Additionally, it is possible to use any combination of these. Affine transformations are an essential part of geocomputation. For example, shifting is needed for labels placement, scaling is used in non-contiguous area cartograms, and many affine transformations are applied when reprojecting or improving the geometry that was created based on a distorted or wrongly projected map.\nThe geopandas package implements affine transformation, for objects of classes GeoSeries and GeoDataFrame. In both cases, the method is applied on the GeoSeries part, returning a new GeoSeries of transformed geometries.\nAffine transformations of GeoSeries can be done using the .affine_transform method, which is a wrapper around the shapely.affinity.affine_transform function. According to the documentation, a 2D affine transformation requires a six-parameter list [a,b,d,e,xoff,yoff] which represents the following equations for transforming the coordinates:\n\\[\nx' = a x + b y + x_\\mathrm{off}\n\\]\n\\[\ny' = d x + e y + y_\\mathrm{off}\n\\]\nThere are also simplified GeoSeries methods for specific scenarios:\n\nGeoSeries.translate(xoff=0.0, yoff=0.0, zoff=0.0)\nGeoSeries.scale(xfact=1.0, yfact=1.0, zfact=1.0, origin='center')\nGeoSeries.rotate(angle, origin='center', use_radians=False)\nGeoSeries.skew(angle, origin='center', use_radians=False)\n\nFor example, shifting only requires the \\(x_{off}\\) and \\(y_{off}\\), using .translate. The code below shifts the y-coordinates by 100,000 meters to the north, but leaves the x-coordinates untouched:\n\nnz_shift = nz.translate(0, 100000)\n\nScaling enlarges or shrinks objects by a factor. It can be applied either globally or locally. Global scaling increases or decreases all coordinates values in relation to the origin coordinates, while keeping all geometries topological relations intact.\nGeopandas implements local scaling using the .scale method. Local scaling treats geometries independently and requires points around which geometries are going to be scaled, e.g., centroids. In the example below, each geometry is shrunk by a factor of two around the centroids (middle panel in Figure 4.5). To achieve that, we pass the 0.5 and 0.5 scaling factors (for x and y, respectively), and the \"centroid\" option for the point of origin. (Other than \"centroid\", it is possible to use \"center\" for the bounding box center, or specific point coordinates.)\n\nnz_scale = nz.scale(0.5, 0.5, origin=\"centroid\")\n\nRotating the geometries can be done using the .rotate method. When rotating, we need to specify the rotation angle (positive values imply clockwise rotation) and the origin points (using the same options as in scale). For example, the following expression rotates nz by 30 degrees counter-clockwise, around the geometry centroids:\n\nnz_rotate = nz.rotate(-30, origin=\"centroid\")\n\nFigure 4.5 shows the original layer nz, and the shifting, scaling and rotation results.\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nnz.plot(ax=axes[0], color=\"lightgrey\", edgecolor=\"darkgrey\")\nnz_shift.plot(ax=axes[0], color=\"red\", edgecolor=\"darkgrey\")\nnz.plot(ax=axes[1], color=\"lightgrey\", edgecolor=\"darkgrey\")\nnz_scale.plot(ax=axes[1], color=\"red\", edgecolor=\"darkgrey\")\nnz.plot(ax=axes[2], color=\"lightgrey\", edgecolor=\"darkgrey\")\nnz_rotate.plot(ax=axes[2], color=\"red\", edgecolor=\"darkgrey\")\naxes[0].set_title(\"Shift\")\naxes[1].set_title(\"Scale\")\naxes[2].set_title(\"Rotate\");\n\n\n\n\nFigure 4.5: Illustrations of affine transformations: shift, scale and rotate.\n\n\n\n\n\n\n4.3.5 Clipping\nSpatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features.\nClipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents. To illustrate the concept we will start with a simple example: two overlapping circles with a center point one unit away from each other and a radius of one (Figure 4.6).\n\nx = shapely.geometry.Point((0, 0)).buffer(1)\ny = shapely.geometry.Point((1, 0)).buffer(1)\nshapely.geometry.GeometryCollection([x, y])\n\n\n\n\nFigure 4.6: Overlapping circles.\n\n\n\n\nImagine you want to select not one circle or the other, but the space covered by both x and y. This can be done using the .intersection method from shapely, illustrated using objects named x and y which represent the left- and right-hand circles (Figure 4.7).\n\nx.intersection(y)\n\n\n\n\nFigure 4.7: Overlapping circles with a gray color indicating intersection between them.\n\n\n\n\nThe next lines of code demonstrate how this works for the .difference, .union, and .symmetric_difference operators:\n\nx.difference(y)\n\n\n\n\n\nx.union(y)\n\n\n\n\n\nx.symmetric_difference(y)\n\n\n\n\n\n\n4.3.6 Subsetting and clipping\nClipping objects can change their geometry but it can also subset objects, returning only features that intersect (or partly intersect) with a clipping/subsetting object. To illustrate this point, we will subset points that cover the bounding box of the circles x and y in Figure 4.6. Some points will be inside just one circle, some will be inside both and some will be inside neither. The following code sections generates a simple random distribution of points within the extent of circles x and y, resulting in output illustrated in Figure 4.8. We do this in two steps. First, we figure out the bounds where random points are to be generated:\n\nbounds = x.union(y).bounds\nbounds\n\n(-1.0, -1.0, 2.0, 1.0)\n\n\nSecond, we use np.random.uniform to calculate n random x and y coordinates within the given bounds:\n\nnp.random.seed(1)\nn = 10  ## Number of points to generate\ncoords_x = np.random.uniform(bounds[0], bounds[2], n)\ncoords_y = np.random.uniform(bounds[1], bounds[3], n)\ncoords = list(zip(coords_x, coords_y))\ncoords\n\n[(0.2510660141077219, -0.1616109711934104),\n (1.1609734803264744, 0.370439000793519),\n (-0.9996568755479653, -0.5910955005369651),\n (-0.0930022821044807, 0.7562348727818908),\n (-0.5597323275486609, -0.9452248136041477),\n (-0.7229842156936066, 0.34093502035680445),\n (-0.4412193658669873, -0.16539039526574606),\n (0.03668218112914312, 0.11737965689150331),\n (0.1903024226920098, -0.7192261228095325),\n (0.6164502020100708, -0.6037970218302424)]\n\n\nThird, we transform the list of coordinates into a list of shapely points:\n\npnt = [shapely.geometry.Point(i) for i in coords]\npnt\n\n[<POINT (0.251 -0.162)>,\n <POINT (1.161 0.37)>,\n <POINT (-1 -0.591)>,\n <POINT (-0.093 0.756)>,\n <POINT (-0.56 -0.945)>,\n <POINT (-0.723 0.341)>,\n <POINT (-0.441 -0.165)>,\n <POINT (0.037 0.117)>,\n <POINT (0.19 -0.719)>,\n <POINT (0.616 -0.604)>]\n\n\nand then to a GeoSeries:\n\npnt = gpd.GeoSeries(pnt)\npnt\n\n0     POINT (0.25107 -0.16161)\n1      POINT (1.16097 0.37044)\n2    POINT (-0.99966 -0.59110)\n               ...            \n7      POINT (0.03668 0.11738)\n8     POINT (0.19030 -0.71923)\n9     POINT (0.61645 -0.60380)\nLength: 10, dtype: geometry\n\n\nThe result is shown in Figure 4.8:\n\nbase = pnt.plot(color='none', edgecolor='black')\ngpd.GeoSeries([x]).plot(ax=base, color='none', edgecolor='darkgrey');\ngpd.GeoSeries([y]).plot(ax=base, color='none', edgecolor='darkgrey');\n\n\n\n\nFigure 4.8: Randomly distributed points within the bounding box enclosing circles x and y. The point that intersects with both objects x and y are highlighted.\n\n\n\n\nNow, we get back to our question: how to subset the points to only return the point that intersects with both x and y? The code chunks below demonstrate three ways to achieve the same result. We can calculate a boolean Series, evaluating whether each point of pnt intersects with the intersection of x and y:\n\nsel = pnt.intersects(x.intersection(y))\nsel\n\n0     True\n1    False\n2    False\n     ...  \n7     True\n8    False\n9     True\nLength: 10, dtype: bool\n\n\nthen use it to subset pnt to get the result pnt1:\n\npnt1 = pnt[sel]\npnt1\n\n0    POINT (0.25107 -0.16161)\n7     POINT (0.03668 0.11738)\n9    POINT (0.61645 -0.60380)\ndtype: geometry\n\n\nWe can also find the intersection between the input points represented by pnt, using the intersection of x and y as the subsetting/clipping object. Since the second argument is an individual shapely geometry (x.intersection(y)), we get “pairwise” intersections of each pnt with it:\n\npnt2 = pnt.intersection(x.intersection(y))\npnt2\n\n0    POINT (0.25107 -0.16161)\n1                 POINT EMPTY\n2                 POINT EMPTY\n               ...           \n7     POINT (0.03668 0.11738)\n8                 POINT EMPTY\n9    POINT (0.61645 -0.60380)\nLength: 10, dtype: geometry\n\n\nEmpty geometries can be filtered out to retain the required subset, and to get pnt2 which is identical to pnt1:\n\npnt2 = pnt2[~pnt2.is_empty]  ## Subset non-empty geometries\npnt2\n\n0    POINT (0.25107 -0.16161)\n7     POINT (0.03668 0.11738)\n9    POINT (0.61645 -0.60380)\ndtype: geometry\n\n\nThis second approach will return features that partly intersect with x.intersection(y) but with modified geometries for spatially extensive features that cross the border of the subsetting object. The results are identical, but the implementation differs substantially.\nAlthough the example above is rather contrived and provided for educational rather than applied purposes, and we encourage the reader to reproduce the results to deepen your understanding for handling geographic vector objects in R, it raises an important question: which implementation to use? Generally, more concise implementations should be favored, meaning the first approach above. We will return to the question of choosing between different implementations of the same technique or algorithm in Chapter 11.\n\n\n4.3.7 Geometry unions\nAs we saw in Section 2.3.2, spatial aggregation can silently dissolve the geometries of touching polygons in the same group. This is demonstrated in the code chunk below in which 49 us_states are aggregated into 4 regions using the .dissolve method:\n\nregions = us_states.dissolve(by='REGION', aggfunc='sum').reset_index()\nregions\n\n/usr/local/lib/python3.11/site-packages/geopandas/geodataframe.py:1676: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  aggregated_data = data.groupby(**groupby_kwargs).agg(aggfunc)\n\n\n\n\n\n\n  \n    \n      \n      REGION\n      geometry\n      AREA\n      total_pop_10\n      total_pop_15\n    \n  \n  \n    \n      0\n      Midwest\n      MULTIPOLYGON (((-89.10077 36.94...\n      1.984047e+06\n      66514091.0\n      67546398.0\n    \n    \n      1\n      Norteast\n      MULTIPOLYGON (((-75.61724 39.83...\n      4.357609e+05\n      54909218.0\n      55989520.0\n    \n    \n      2\n      South\n      MULTIPOLYGON (((-81.38550 30.27...\n      2.314087e+06\n      112072990.0\n      118575377.0\n    \n    \n      3\n      West\n      MULTIPOLYGON (((-118.36998 32.8...\n      3.073145e+06\n      68444193.0\n      72264052.0\n    \n  \n\n\n\n\nThe result is shown in Figure 4.9:\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 2.5))\nus_states.plot(ax=axes[0], edgecolor='black', column='total_pop_15', legend=True)\nregions.plot(ax=axes[1], edgecolor='black', column='total_pop_15', legend=True)\naxes[0].set_title('State')\naxes[1].set_title('Region');\n\n\n\n\nFigure 4.9: Spatial aggregation on contiguous polygons, illustrated by aggregating the population of US states into regions, with population represented by color. Note the operation automatically dissolves boundaries between states.\n\n\n\n\nWhat is going on in terms of the geometries? Behind the scenes, .dissolve combines the geometries and dissolve the boundaries between them using the .unary_union method per group. This is demonstrated in the code chunk below which creates a united western US using the standalone unary_union operation:\n\nus_west = us_states[us_states['REGION'] == 'West']\nus_west_union = us_west['geometry'].unary_union\n\nNote that the result is a shapely geometry, as the individual attributes are “lost” as part of dissolving. The result is shown in Figure 4.10.\n\nus_west_union\n\n\n\n\nFigure 4.10: Western US\n\n\n\n\nTo dissolve two (or more) groups of a GeoDataFrame into one geometry, we can either use a combined condition:\n\nsel = (us_states['REGION'] == 'West') | (us_states['NAME'] == 'Texas')\ntexas_union = us_states[sel]\ntexas_union = texas_union['geometry'].unary_union\n\nor concatenate the two separate subsets:\n\nus_west = us_states[us_states['REGION'] == 'West']\ntexas = us_states[us_states['NAME'] == 'Texas']\ntexas_union = pd.concat([us_west, texas]).unary_union\n\nand then dissove using .unary_union. The result is identical in both cases, shown in Figure 4.11.\n\ntexas_union\n\n\n\n\nFigure 4.11: Western US and Texas\n\n\n\n\n\n\n4.3.8 Type transformations\nTransformation of geometries, from one type to another, also known as “geometry casting”, is often required to facilitate spatial analysis. The shapely package can be used for geometry casting. The exact expression(s) depend on the specific transformation we are interested in. In general, you need to figure out the required input of the respective construstor function according to the “destination” geometry (e.g., shapely.geometry.LineString, etc.), then reshape the input of the “source” geometry into the right form to be passed to that function.\nLet’s create a \"MultiPoint\" to illustrate how geometry casting works on shapely geometry objects:\n\nmultipoint = shapely.geometry.MultiPoint([(1,1), (3,3), (5,1)])\nmultipoint\n\n\n\n\nA \"LineString\" can be created using shapely.geometry.LineString from a list of points. Consequently, a \"MultiPoint\" can be converted to a \"LineString\" by extracting the individual points into a list, then passing them to shapely.geometry.LineString:\n\nlinestring = shapely.geometry.LineString(list(multipoint.geoms))\nlinestring\n\n\n\n\nA \"Polygon\" can also be created using function shapely.geometry.Polygon, which acceps a sequence of point coordinates. In principle, the last coordinate must be equal to the first, in order to form a closed shape. However, shapely.geometry.Polygon is able to complete the last coordinate automatically. Therefore:\n\npolygon = shapely.geometry.Polygon([[p.x, p.y] for p in multipoint.geoms])\npolygon\n\n\n\n\nThe source \"MultiPoint\" geometry, and the derived \"LineString\" and \"Polygon\" geometries are shown in Figure 4.12. Note that we convert the shapely geometries to GeoSeries for easier multi-panel plotting:\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\ngpd.GeoSeries(multipoint).plot(ax=axes[0])\ngpd.GeoSeries(linestring).plot(ax=axes[1])\ngpd.GeoSeries(polygon).plot(ax=axes[2])\naxes[0].set_title(\"MultiPoint\")\naxes[1].set_title(\"LineString\")\naxes[2].set_title(\"Polygon\");\n\n\n\n\nFigure 4.12: Examples of linestring and polygon casted from a multipoint geometry.\n\n\n\n\nConversion from multipoint to linestring is a common operation that creates a line object from ordered point observations, such as GPS measurements or geotagged media. This allows spatial operations such as the length of the path traveled. Conversion from multipoint or linestring to polygon is often used to calculate an area, for example from the set of GPS measurements taken around a lake or from the corners of a building lot.\nOur \"LineString\" geometry can be converted bact to a \"MultiPoint\" geometry by passing its coordinates directly to shapely.geometry.MultiPoint:\n\n# 'LineString' -> 'MultiPoint'\nshapely.geometry.MultiPoint(linestring.coords)\n\n\n\n\nThe \"Polygon\" (exterior) coordinates can be passed to shapely.geometry.MultiPoint as well:\n\n# 'Polygon' -> 'MultiPoint'\nshapely.geometry.MultiPoint(polygon.exterior.coords)\n\n\n\n\n…"
  },
  {
    "objectID": "05-geometry-operations.html#sec-geo-ras",
    "href": "05-geometry-operations.html#sec-geo-ras",
    "title": "4  Geometry operations",
    "section": "4.4 Geometric operations on raster data",
    "text": "4.4 Geometric operations on raster data\n\n4.4.1 Geometric intersections\n…\n\n\n4.4.2 Extent and origin\n…\n\n\n4.4.3 Aggregation and disaggregation\nRaster datasets can also differ with regard to their resolution. To match resolutions, or to modify the resolution for other reasons (such as conserving storage space), one can either decrease (aggregate) or increase (disaggregate) the resolution of one raster. As an example, we here change the spatial resolution of dem.tif by a factor of 5 (Figure Figure 4.13).\nRaster aggregation is, in fact, a special case of raster resampling (see Section 4.4.4), where the target raster grid is aligned with the original raster, only with coarser pixels. Raster resampling, is the general case where the new grid is not necessarily an aggregation of the original one, but any other case as well (such as a rotated and/or shifted one, etc.).\nTo aggregate a raster using rasterio, we go through two steps:\n\nReading the raster values (using .read) into an out_shape that is different from the original .shape\nUpdating the transform according to the out_shape\n\nLet’s demonstrate. First, we create a file connection to dem.tif, using rasterio.open:\n\nsrc = rasterio.open('data/dem.tif')\n\nNote the shape of the raster, it has 117 rows and 117 columns:\n\nsrc.read(1).shape\n\n(117, 117)\n\n\nAlso note the transform, which tells us that the raster resolution is 30.85 \\(m\\):\n\nsrc.transform\n\nAffine(30.849999999999604, 0.0, 794599.1076146346,\n       0.0, -30.84999999999363, 8935384.324602526)\n\n\nNow, instead of reading the raster values the usual way, as in src.read(1), we can specify out_shape to read the values into a different shape. Here, we calculate a new shape which is downscaled by a factor of 5, i.e., the number of rows and columns is multiplied by 0.2. We must truncate any “partial” rows and columns, e.g., using int. Each new pixel is now obtained, or “resampled”, from ~5×5=25 “old” raster values. We can choose the resampling method through the resampling parameter. Here we use rasterio.enums.Resampling.average, i.e., the new “large” pixel value is the average of all coinciding small pixels, which makes sense for our elevation data in dem.tif:\n\nfactor = 0.2\nr = src.read(1,\n    out_shape=(\n        int(src.height * factor),\n        int(src.width * factor)\n        ),\n    resampling=rasterio.enums.Resampling.average\n)\n\nThe resulting array r has a smaller .shape, as shown below:\n\nr.shape\n\n(23, 23)\n\n\nOther useful options include:\n\nrasterio.enums.Resampling.nearest—Nearest neighbor resampling\nrasterio.enums.Resampling.bilinear—Bilinear resampling\nrasterio.enums.Resampling.mode—Mode resampling (most common value)\nrasterio.enums.Resampling.min—Minimum resampling\nrasterio.enums.Resampling.max—Maximum resampling\nrasterio.enums.Resampling.med—Median resampling\nrasterio.enums.Resampling.sum—Median resampling\n\nThe second step is to update the transform, taking into account the change in raster shape, as follows:\n\nnew_transform = src.transform * src.transform.scale(\n    (src.width / r.shape[0]),\n    (src.height / r.shape[1])\n)\nnew_transform\n\nAffine(156.93260869565017, 0.0, 794599.1076146346,\n       0.0, -156.9326086956198, 8935384.324602526)\n\n\nThe original raster and the aggregated one, are shown in Figure 4.13:\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\nrasterio.plot.show(src, ax=axes[0])\nrasterio.plot.show(r, transform=new_transform, ax=axes[1]);\n\n\n\n\nFigure 4.13: Original raster (left), and aggregated raster (right).\n\n\n\n\nIn case we need to export the new raster, we need to update the metadata:\n\ndst_kwargs = src.meta.copy()\ndst_kwargs.update({\n    'transform': new_transform,\n    'width': r.shape[0],\n    'height': r.shape[1],\n})\ndst_kwargs\n\n{'driver': 'GTiff',\n 'dtype': 'float32',\n 'nodata': nan,\n 'width': 23,\n 'height': 23,\n 'count': 1,\n 'crs': CRS.from_epsg(32717),\n 'transform': Affine(156.93260869565017, 0.0, 794599.1076146346,\n        0.0, -156.9326086956198, 8935384.324602526)}\n\n\nThen create a new file in writing mode, and write the values in r into that file (see Section 7.8.2):\n\ndst = rasterio.open('output/dem_agg5.tif', 'w', **dst_kwargs)\ndst.write(r, 1)\ndst.close()\n\nDisaggregation…\n…\n\n\n4.4.4 Resampling\n…"
  },
  {
    "objectID": "05-geometry-operations.html#exercises",
    "href": "05-geometry-operations.html#exercises",
    "title": "4  Geometry operations",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises"
  },
  {
    "objectID": "06-raster-vector.html#prerequisites",
    "href": "06-raster-vector.html#prerequisites",
    "title": "5  Raster-vector interactions",
    "section": "5.1 Prerequisites",
    "text": "5.1 Prerequisites\nLet’s import the required packages:\n\nimport numpy as np\nimport shapely.geometry\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\nimport rasterio.mask\nimport rasterstats\nfrom rasterio.plot import show\nimport math\nimport os\n\nand load the sample data:\n\nsrc_srtm = rasterio.open('data/srtm.tif')\nsrc_nlcd = rasterio.open('data/nlcd.tif')\nsrc_grain = rasterio.open('data/grain.tif')\nsrc_elev = rasterio.open('data/elev.tif')\nsrc_dem = rasterio.open('data/dem.tif')\nzion = gpd.read_file('data/zion.gpkg')\nzion_points = gpd.read_file('data/zion_points.gpkg')\ncycle_hire_osm = gpd.read_file('data/cycle_hire_osm.gpkg')\nus_states = gpd.read_file('data/us_states.gpkg')"
  },
  {
    "objectID": "06-raster-vector.html#introduction",
    "href": "06-raster-vector.html#introduction",
    "title": "5  Raster-vector interactions",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction"
  },
  {
    "objectID": "06-raster-vector.html#raster-cropping",
    "href": "06-raster-vector.html#raster-cropping",
    "title": "5  Raster-vector interactions",
    "section": "5.3 Raster cropping",
    "text": "5.3 Raster cropping\nMany geographic data projects involve integrating data from many different sources, such as remote sensing images (rasters) and administrative boundaries (vectors). Often the extent of input raster datasets is larger than the area of interest. In this case raster cropping and masking are useful for unifying the spatial extent of input data. Both operations reduce object memory use and associated computational resources for subsequent analysis steps, and may be a necessary preprocessing step before creating attractive maps involving raster data.\nWe will use two objects to illustrate raster cropping:\n\nThe srtm.tif raster representing elevation (meters above sea level) in south-western Utah\nThe zion.gpkg vector layer representing the Zion National Park\n\nBoth target and cropping objects must have the same projection. The following reprojects the vector layer zion into the CRS of the raster src_srtm:\n\nzion = zion.to_crs(src_srtm.crs)\n\nTo mask the image, i.e., convert all pixels which do not intersect with the zion polygon to “No Data”, we use the rasterio.mask.mask function as follows:\n\nout_image_mask, out_transform_mask = rasterio.mask.mask(\n    src_srtm, \n    zion['geometry'], \n    crop=False, \n    nodata=9999\n)\n\nNote that we need to specify a “No Data” value in agreement with the raster data type. Since srtm.tif is of type uint16, we choose 9999 (a positive integer that is guaranteed not to occur in the raster).\nThe result is the out_image array with the masked values:\n\nout_image_mask\n\narray([[[9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        ...,\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999],\n        [9999, 9999, 9999, ..., 9999, 9999, 9999]]], dtype=uint16)\n\n\nand the new out_transform:\n\nout_transform_mask\n\nAffine(0.0008333333332777796, 0.0, -113.23958321278403,\n       0.0, -0.0008333333332777843, 37.512916763165805)\n\n\nNote that masking (without cropping!) does not modify the raster spatial configuration. Therefore, the new transform is identical to the original:\n\nsrc_srtm.transform\n\nAffine(0.0008333333332777796, 0.0, -113.23958321278403,\n       0.0, -0.0008333333332777843, 37.512916763165805)\n\n\nUnfortunately, the out_image and out_transform object do not contain any information indicating that 9999 represents “No Data”. To associate the information with the raster, we must write it to file along with the corresponding metadata. For example, to write the cropped raster to file, we need to modify the “No Data” setting in the metadata:\n\nout_meta = src_srtm.meta\nout_meta.update(nodata=9999)\nout_meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 9999,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nThen we can write the cropped raster to file:\n\nnew_dataset = rasterio.open('output/srtm_masked.tif', 'w', **out_meta)\nnew_dataset.write(out_image_mask)\nnew_dataset.close()\n\nNow we can re-import the raster:\n\nsrc_srtm_mask = rasterio.open('output/srtm_masked.tif')\n\nThe .meta property contains the nodata entry. Now, any relevant operation (such as plotting) will take “No Data” into account:\n\nsrc_srtm_mask.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': 9999.0,\n 'width': 465,\n 'height': 457,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0008333333332777796, 0.0, -113.23958321278403,\n        0.0, -0.0008333333332777843, 37.512916763165805)}\n\n\nCropping means reducing the raster extent to the extent of the vector layer:\n\nTo crop and mask, we can use the same in rasterio.mask.mask expression shown above for masking, just setting crop=True instead of crop=False.\nTo just crop, without masking, we can derive the extent polygon and then crop using it.\n\nFor example, here is how we can obtain the extent polygon of zion, as a shapely geometry object:\n\nbb = zion.unary_union.envelope\nbb\n\n\n\n\nThe extent can now be used for masking. Here, we are also using the all_touched=True option so that pixels partially overlapping with the extent are included:\n\nout_image_crop, out_transform_crop = rasterio.mask.mask(\n    src_srtm, \n    [bb], \n    crop=True, \n    all_touched=True, \n    nodata=9999\n)\n\nFigure 5.1 shows the original raster, and the cropped and masked results.\n\nfig, axes = plt.subplots(ncols=3, figsize=(9,5))\nshow(src_srtm, ax=axes[0])\nzion.plot(ax=axes[0], color='none', edgecolor='black')\nshow(src_srtm_mask, ax=axes[1])\nzion.plot(ax=axes[1], color='none', edgecolor='black')\nshow(out_image_crop, transform=out_transform_crop, ax=axes[2])\nzion.plot(ax=axes[2], color='none', edgecolor='black')\naxes[0].set_title('Original')\naxes[1].set_title('Mask')\naxes[2].set_title('Crop');\n\n\n\n\nFigure 5.1: Raster masking and cropping"
  },
  {
    "objectID": "06-raster-vector.html#raster-extraction",
    "href": "06-raster-vector.html#raster-extraction",
    "title": "5  Raster-vector interactions",
    "section": "5.4 Raster extraction",
    "text": "5.4 Raster extraction\nRaster extraction is the process of identifying and returning the values associated with a ‘target’ raster at specific locations, based on a (typically vector) geographic ‘selector’ object. The reverse of raster extraction — assigning raster cell values based on vector objects — is rasterization, described in Section 5.5.\nIn the following examples, we use a third-party package called rasterstats, which is specifically aimed at extracting raster values:\n\nto points, via the rasterstats.point_query function, or\nto polygons, via the rasterstats.zonal_stats function.\n\n\n5.4.1 Extraction to points\nThe basic example is of extracting the value of a raster cell at specific points. For this purpose, we will use zion_points, which contain a sample of 30 locations within the Zion National Park (Figure …). The following expression extracts elevation values from srtm:\n\nresult = rasterstats.point_query(\n    zion_points, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform,\n    interpolate='nearest'\n)\n\nThe resulting object is a list of raster values, corresponding to zion_points:\n\nresult[:5]\n\n[1802, 2433, 1886, 1370, 1452]\n\n\nTo create a DataFrame with points’ IDs (one value per vector’s row) and related srtm values for each point, we need to assign it:\n\nzion_points['elev'] = result\nzion_points\n\n\n\n\n\n  \n    \n      \n      geometry\n      elev\n    \n  \n  \n    \n      0\n      POINT (-112.91587 37.20013)\n      1802\n    \n    \n      1\n      POINT (-113.09369 37.39263)\n      2433\n    \n    \n      2\n      POINT (-113.02462 37.33466)\n      1886\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      27\n      POINT (-113.03655 37.23446)\n      1372\n    \n    \n      28\n      POINT (-113.13933 37.39004)\n      1905\n    \n    \n      29\n      POINT (-113.09677 37.24237)\n      1574\n    \n  \n\n30 rows × 2 columns\n\n\n\n\n\n5.4.2 Extraction to lines\nRaster extraction is also applicable with line selectors. The typical line extraction algorithm is to extract one value for each raster cell touched by a line. However, this particular approach is not recommended to obtain values along the transects, as it is hard to get the correct distance between each pair of extracted raster values.\nFor line extraction, a better approach is to split the line into many points (at equal distances along the line) and then extract the values for these points. To demonstrate this, the code below creates zion_transect, a straight line going from northwest to southeast of the Zion National Park (see Section 1.2 for a recap on the vector data model):\n\ncoords = [[-113.2, 37.45], [-112.9, 37.2]]\nzion_transect = shapely.geometry.LineString(coords)\nzion_transect\n\n\n\n\nHere is a printout demonstrating that this is a \"LineString\" geometry representing a straight line between two points:\n\nprint(zion_transect)\n\nLINESTRING (-113.2 37.45, -112.9 37.2)\n\n\nThe line is illustrated in the context of the raster in Figure 5.2.\nThe utility of extracting heights from a linear selector is illustrated by imagining that you are planning a hike. The method demonstrated below provides an ‘elevation profile’ of the route (the line does not need to be straight), useful for estimating how long it will take due to long climbs.\nFirst, we need to create a layer zion_transect_pnt consisting of points along our line (zion_transect), at specified intervals (distance_delta). To do that, we need to transform the line into a projected CRS (so that we work with true distances, in \\(m\\)), such as UTM. This requires going through a GeoSeries, as shapely geometries have no CRS definition nor concept of reprojection (see Section 1.2.6):\n\nzion_transect_utm = gpd.GeoSeries(zion_transect, crs=4326)\nzion_transect_utm = zion_transect_utm.to_crs(32612)\nzion_transect_utm = zion_transect_utm.iloc[0]\n\nThe printout of the new geometry shows this is still a straight line between two points, only with coordinates in a different CRS:\n\nprint(zion_transect_utm)\n\nLINESTRING (305399.67208180577 4147066.650206682, 331380.8917453843 4118750.0947884847)\n\n\nThen, we calculate the distances, along the line, where points are going to be generated, using np.arange. This is a numeric sequence starting at 0, going up to line .length, in steps of 250 (\\(m\\)):\n\ndistances = np.arange(0, zion_transect_utm.length, 250)\ndistances[:7]  ## First 7 distance cutoff points\n\narray([   0.,  250.,  500.,  750., 1000., 1250., 1500.])\n\n\nThe distances cutoffs are used to sample (“interpolate”) points along the line. The shapely method line.interpolate(d) is used to generate the points. The points are then reproject back to the CRS of the raster:\n\nzion_transect_pnt = [zion_transect_utm.interpolate(distance) for distance in distances]\nzion_transect_pnt = gpd.GeoSeries(zion_transect_pnt, crs=32612)\nzion_transect_pnt = zion_transect_pnt.to_crs(4326)\nzion_transect_pnt\n\n0      POINT (-113.20000 37.45000)\n1      POINT (-113.19804 37.44838)\n2      POINT (-113.19608 37.44675)\n                  ...             \n151    POINT (-112.90529 37.20443)\n152    POINT (-112.90334 37.20280)\n153    POINT (-112.90140 37.20117)\nLength: 154, dtype: geometry\n\n\nSecond, we extract elevation values for each point in our transects and combine this information with zion_transect_pnt (after “promoting” it to a GeoDataFrame, to accomodate extra attributes), using the point extraction method shown earlier (Section 5.4.1). We also attach the respective distance cutoff points distances:\n\nresult = rasterstats.point_query(\n    zion_transect_pnt, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform,\n    interpolate='nearest'\n)\nzion_transect_pnt = gpd.GeoDataFrame(geometry=zion_transect_pnt)\nzion_transect_pnt['dist'] = distances\nzion_transect_pnt['elev'] = result\nzion_transect_pnt\n\n\n\n\n\n  \n    \n      \n      geometry\n      dist\n      elev\n    \n  \n  \n    \n      0\n      POINT (-113.20000 37.45000)\n      0.0\n      2001\n    \n    \n      1\n      POINT (-113.19804 37.44838)\n      250.0\n      2037\n    \n    \n      2\n      POINT (-113.19608 37.44675)\n      500.0\n      1949\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      151\n      POINT (-112.90529 37.20443)\n      37750.0\n      1837\n    \n    \n      152\n      POINT (-112.90334 37.20280)\n      38000.0\n      1841\n    \n    \n      153\n      POINT (-112.90140 37.20117)\n      38250.0\n      1819\n    \n  \n\n154 rows × 3 columns\n\n\n\nThe information in zion_transect_pnt, namely the \"dist\" and \"elev\" attributes, can now be used to create elevation profiles, as illustrated in Figure 5.2:\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,4))\nshow(src_srtm, ax=axes[0])\ngpd.GeoSeries(zion_transect).plot(ax=axes[0], color='black')\nzion.plot(ax=axes[0], color='none', edgecolor='darkgrey')\nzion_transect_pnt.set_index('dist')['elev'].plot(ax=axes[1])\naxes[1].set_xlabel('Distance (m)')\naxes[1].set_ylabel('Elevation (m)')\naxes[0].set_title('Line extraction')\naxes[1].set_title('Elevation along the line');\n\n\n\n\nFigure 5.2: Location of a line used for raster extraction (left) and the elevation along this line (right).\n\n\n\n\n\n\n5.4.3 Extraction to polygons\nThe final type of geographic vector object for raster extraction is polygons. Like lines, polygons tend to return many raster values per polygon. Typically, we generate summary statistics for raster values per polygon, for example to characterize a single region or to compare many regions. The generation of raster summary statistics, by polygons, is demonstrated in the code below, which creates a list of summary statistics (in this case a list of length 1, since there is just one polygon), again using rasterstats:\n\nrasterstats.zonal_stats(\n    zion, \n    src_srtm.read(1), \n    nodata = src_srtm.nodata, \n    affine = src_srtm.transform, \n    stats = ['mean', 'min', 'max']\n)\n\n/usr/local/lib/python3.11/site-packages/rasterstats/main.py:156: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n  if 'Point' in geom.type:\n\n\n[{'min': 1122.0, 'max': 2661.0, 'mean': 1818.211830154405}]\n\n\nThe results provide useful summaries, for example that the maximum height in the park is around 2,661 meters above see level (other summary statistics, such as standard deviation, can also be calculated in this way). Because there is only one polygon in the example a data frame with a single row is returned; however, the method works when multiple selector polygons are used.\nNote the stats argument, where we determine what type of statistics are calculated per polygon. Possible values other than 'mean', 'min', 'max' are:\n\n'count'—The number of valid (i.e., excluding “No Data”) pixels\n'nodata'—The number of pixels with ’No Data”\n'majority'—The most frequently occurring value\n'median'—The median value\n\nSee the documentation for the complete list. Additionally, the zonal_stats function accepts user-defined functions for calculating any custom statistics.\nTo count occurrences of categorical raster values within polygons, we can use masking (see Section…) combined with np.unique, as follows:\n\nout_image, out_transform = rasterio.mask.mask(\n    src_nlcd, \n    zion['geometry'].to_crs(src_nlcd.crs), \n    crop=False, \n    nodata=9999\n)\ncounts = np.unique(out_image, return_counts=True)\n\n/usr/local/lib/python3.11/site-packages/rasterio/mask.py:88: UserWarning: shapes are outside bounds of raster. Are they in different coordinate reference systems?\n  warnings.warn('shapes are outside bounds of raster. '\n\n\nAccording to the result, for example, pixel value 2 (“Developed” class) appears in 4205 pixels within the Zion polygon:\n\ncounts\n\n(array([15], dtype=uint8), array([1458207]))\n\n\nRaster to polygon extraction is illustrated in Figure 5.3.\n\nfig, axes = plt.subplots(ncols=2, figsize=(6,4))\nshow(src_srtm, ax=axes[0])\nzion.plot(ax=axes[0], color='none', edgecolor='black')\nshow(src_nlcd, ax=axes[1], cmap='Set3')\nzion.to_crs(src_nlcd.crs).plot(ax=axes[1], color='none', edgecolor='black')\naxes[0].set_title('Continuous data extraction')\naxes[1].set_title('Categorical data extraction');\n\n\n\n\nFigure 5.3: Area used for continuous (left) and categorical (right) raster extraction."
  },
  {
    "objectID": "06-raster-vector.html#sec-rasterization",
    "href": "06-raster-vector.html#sec-rasterization",
    "title": "5  Raster-vector interactions",
    "section": "5.5 Rasterization",
    "text": "5.5 Rasterization\n\n5.5.1 Rasterizing points\nRasterization is the conversion of vector objects into their representation in raster objects. Usually, the output raster is used for quantitative analysis (e.g., analysis of terrain) or modeling. As we saw in Chapter 1, the raster data model has some characteristics that make it conducive to certain methods. Furthermore, the process of rasterization can help simplify datasets because the resulting values all have the same spatial resolution: rasterization can be seen as a special type of geographic data aggregation.\nThe rasterio package contains the rasterio.features.rasterize function for doing this work. To make it happen, we need to have the “template” grid definition, i.e., the “template” raster defining the extent, resolution and CRS of the output, in the form of out_shape (the dimensions) and transform (the transform). In case we have an existing template raster, we simply need to query its out_shape and transform. In case we create a custom template, e.g., covering the vector layer extent with specified resolution, there is some extra work to calculate the out_shape and transform (see next example).\nFurthermore, the rasterio.features.rasterize function requires the input shapes in the form for a generator of (geom, value) tuples, where:\n\ngeom is the given geometry (shapely)\nvalue is the value to be “burned” into pixels coinciding with the geometry (int or float)\n\nAgain, this will be made clear in the next example.\nThe geographic resolution of the “template” raster has a major impact on the results: if it is too low (cell size is too large), the result may miss the full geographic variability of the vector data; if it is too high, computational times may be excessive. There are no simple rules to follow when deciding an appropriate geographic resolution, which is heavily dependent on the intended use of the results. Often the target resolution is imposed on the user, for example when the output of rasterization needs to be aligned to the existing raster.\nTo demonstrate rasterization in action, we will use a template raster that has the same extent and CRS as the input vector data cycle_hire_osm_projected (a dataset on cycle hire points in London is illustrated in Figure 5.4) and spatial resolution of 1000 meters. First, we obtain the vector layer:\n\ncycle_hire_osm_projected = cycle_hire_osm.to_crs(27700)\ncycle_hire_osm_projected\n\n\n\n\n\n  \n    \n      \n      osm_id\n      name\n      capacity\n      cyclestreets_id\n      description\n      geometry\n    \n  \n  \n    \n      0\n      108539\n      Windsor Terrace\n      14.0\n      NaN\n      NaN\n      POINT (532353.838 182857.655)\n    \n    \n      1\n      598093293\n      Pancras Road, King's Cross\n      NaN\n      NaN\n      NaN\n      POINT (529848.350 183337.175)\n    \n    \n      2\n      772536185\n      Clerkenwell, Ampton Street\n      11.0\n      NaN\n      NaN\n      POINT (530635.620 182608.992)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      537\n      5121513755\n      NaN\n      5.0\n      NaN\n      NaN\n      POINT (532531.241 178805.391)\n    \n    \n      538\n      5188912370\n      NaN\n      NaN\n      NaN\n      NaN\n      POINT (538723.338 180836.279)\n    \n    \n      539\n      5188912371\n      NaN\n      NaN\n      NaN\n      NaN\n      POINT (538413.214 180774.335)\n    \n  \n\n540 rows × 6 columns\n\n\n\nNext, we need to calculate the out_shape and transform of out template raster. To calculate the transform, we combine the top-left corner of the cycle_hire_osm_projected bounding box with the required resolution (e.g., 1000 \\(m\\)):\n\nbounds = cycle_hire_osm_projected.total_bounds\nbounds\n\narray([523038.61452275, 174934.65727249, 538723.33812747, 184971.40854298])\n\n\n\nres = 1000\ntransform = rasterio.transform.from_origin(\n    west=bounds[0], \n    north=bounds[3], \n    xsize=res, \n    ysize=res\n)\ntransform\n\nAffine(1000.0, 0.0, 523038.61452275474,\n       0.0, -1000.0, 184971.40854297992)\n\n\nTo calculate the out_shape, we divide the x-axis and y-axis extent by the resolution:\n\nrows = math.ceil((bounds[3] - bounds[1]) / res)\ncols = math.ceil((bounds[2] - bounds[0]) / res)\nshape = (rows, cols)\nshape\n\n(11, 16)\n\n\nNow, we can rasterize. Rasterization is a very flexible operation: the results depend not only on the nature of the template raster, but also on the type of input vector (e.g., points, polygons), the pixel “activation” method, and the function applied when there is more than one match.\nTo illustrate this flexibility we will try three different approaches to rasterization. First, we create a raster representing the presence or absence of cycle hire points (known as presence/absence rasters). In this case, we transfer the value of 1 to all pixels where at least one point falls in. To transform the point GeoDataFrame into a generator of shapely geometries and the (fixed) values, we use the following expression:\n\n((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list())\n\n<generator object <genexpr> at 0x7fa1e10d72a0>\n\n\nTherefore, the rasterizing expression is:\n\nch_raster1 = rasterio.features.rasterize(\n    ((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform\n)\n\nThe result is a numpy array with the burned values of 1, and 0 in unaffected “pixels”:\n\nch_raster1\n\narray([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n\n\nTo count the number of stations, we can use the fixed value of 1 combined with the merge_alg=rasterio.enums.MergeAlg.add, which means that multiple values burned into the same pixel are summed, rather than replaced keeping last (the default):\n\nch_raster2 = rasterio.features.rasterize(\n    ((g, 1) for g in cycle_hire_osm_projected['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform,\n    merge_alg=rasterio.enums.MergeAlg.add\n)\n\nHere is the resulting array of counts:\n\nch_raster2\n\narray([[ 0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  1,  3,  3],\n       [ 0,  0,  0,  1,  3,  3,  5,  5,  8,  9,  1,  3,  2,  6,  7,  0],\n       [ 0,  0,  0,  8,  5,  4, 11, 10, 12,  9, 11,  4,  8,  5,  4,  0],\n       [ 0,  1,  4, 10, 10, 11, 18, 16, 13, 12,  8,  6,  5,  2,  3,  0],\n       [ 3,  3,  9,  3,  5, 14, 10, 15,  9,  9,  5,  8,  0,  0, 12,  2],\n       [ 4,  5,  9, 11,  6,  7,  7,  3, 10,  9,  4,  0,  0,  0,  0,  0],\n       [ 4,  0,  7,  8,  8,  4, 11, 10,  7,  3,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  3,  0,  0,  1,  4,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  1,  0,  1,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  1,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n      dtype=uint8)\n\n\nThe new output, ch_raster2, shows the number of cycle hire points in each grid cell. The cycle hire locations have different numbers of bicycles described by the capacity variable, raising the question, what’s the capacity in each grid cell? To calculate that, we must sum the field (\"capacity\") rather than the fixed values of 1. This requires using an expanded generator of geometries and values, where we (1) extract both geometries and attribute values, and (2) filter out “No Data” values, as follows:\n\nch_raster3 = rasterio.features.rasterize(\n    ((g, v) for g, v in cycle_hire_osm_projected[['geometry', 'capacity']] \\\n        .dropna(subset='capacity')\n        .to_numpy() \\\n        .tolist()),\n    out_shape=shape,\n    transform=transform,\n    merge_alg=rasterio.enums.MergeAlg.add\n)\n\nHere is the code to illustrate the input point layer cycle_hire_osm_projected and the three variants of rasterizing it ch_raster1, ch_raster2, and ch_raster3 (Figure 5.4):\n\nfig, axes = plt.subplots(2, 2, figsize=(8.5, 6.5))\ncycle_hire_osm_projected.plot(ax=axes[0][0], column='capacity')\nshow(ch_raster1, transform=transform, ax=axes[0][1])\nshow(ch_raster2, transform=transform, ax=axes[1][0])\nshow(ch_raster3, transform=transform, ax=axes[1][1])\naxes[0][0].set_title('Points')\naxes[0][1].set_title('Presence/Absence')\naxes[1][0].set_title('Count')\naxes[1][1].set_title('Aggregated capacity');\n\n\n\n\nFigure 5.4: Examples of point rasterization\n\n\n\n\n\n\n5.5.2 Rasterizing lines and polygons\nAnother dataset based on California’s polygons and borders (created below) illustrates rasterization of lines. There are three preliminary steps. First, we subset the California polygon:\n\ncalifornia = us_states[us_states['NAME'] == \"California\"]\ncalifornia\n\n\n\n\n\n  \n    \n      \n      GEOID\n      NAME\n      REGION\n      ...\n      total_pop_10\n      total_pop_15\n      geometry\n    \n  \n  \n    \n      26\n      06\n      California\n      West\n      ...\n      36637290.0\n      38421464.0\n      MULTIPOLYGON (((-118.60337 33.4...\n    \n  \n\n1 rows × 7 columns\n\n\n\nSecond, we “cast” the polygon into a \"MultiLineString\" geometry, using the .boundary property that GeoSeries have:\n\ncalifornia_borders = california['geometry'].boundary\ncalifornia_borders\n\n26    MULTILINESTRING ((-118.60337 33...\ndtype: geometry\n\n\nThird, we create a template raster with a resolution of a 0.5 degree:\n\nbounds = california_borders.total_bounds\nres = 0.5\ntransform = rasterio.transform.from_origin(\n    west=bounds[0], \n    north=bounds[3], \n    xsize=res, \n    ysize=res\n)\nrows = math.ceil((bounds[3] - bounds[1]) / res)\ncols = math.ceil((bounds[2] - bounds[0]) / res)\nshape = (rows, cols)\nshape\n\n(19, 21)\n\n\nFinally, we rasterize california_borders based on the calculated template shape and transform. When considering line or polygon rasterization, one useful additional argument is all_touched. By default it is False, but when changed to True—all cells that are touched by a line or polygon border get a value. Line rasterization with all_touched=True is demonstrated in the code below (Figure 5.5, left). We are also using fill=np.nan to set “background” values as “No Data”:\n\ncalifornia_raster1 = rasterio.features.rasterize(\n    ((g, 1) for g in california_borders.to_list()),\n    out_shape=shape,\n    transform=transform,\n    all_touched=True,\n    fill=np.nan\n)\n\nCompare it to a polygon rasterization, with all_touched=False by default, which selects only raster cells whose centroids are inside the selector polygon, as illustrated in Figure 5.5 (right):\n\ncalifornia_raster2 = rasterio.features.rasterize(\n    ((g, 1) for g in california['geometry'].to_list()),\n    out_shape=shape,\n    transform=transform,\n    fill=np.nan\n)\n\nThe results are shown in Figure 5.5. To illustrate which raster pixels are actually selected as part of rasterization, we also show them as points, which also requires the “Raster to points” code section, as explained in Section 5.6:\n\n# Raster to points\nids = california_raster1.copy()\nids = np.arange(0, california_raster1.size).reshape(california_raster1.shape).astype(np.int32)\nshapes = rasterio.features.shapes(ids, transform=transform)\npol = list(shapes)\npnt = [shapely.geometry.shape(i[0]).centroid for i in pol]\npnt = gpd.GeoSeries(pnt, crs=california.crs)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(8.5, 6.5))\nshow(california_raster1, transform=transform, ax=axes[0], cmap='Set3')\ngpd.GeoSeries(california_borders, crs=california.crs).plot(ax=axes[0], edgecolor='darkgrey', linewidth=1)\npnt.plot(ax=axes[0], color='black', markersize=1)\nshow(california_raster2, transform=transform, ax=axes[1], cmap='Set3')\ncalifornia.plot(ax=axes[1], color='none', edgecolor='darkgrey', linewidth=1)\npnt.plot(ax=axes[1], color='black', markersize=1)\naxes[0].set_title('Line rasterization')\naxes[1].set_title('Polygon rasterization');\n\n\n\n\nFigure 5.5: Examples of line and polygon rasterization"
  },
  {
    "objectID": "06-raster-vector.html#sec-spatial-vectorization",
    "href": "06-raster-vector.html#sec-spatial-vectorization",
    "title": "5  Raster-vector interactions",
    "section": "5.6 Spatial vectorization",
    "text": "5.6 Spatial vectorization\nSpatial vectorization is the counterpart of rasterization (Section 5.5), but in the opposite direction. It involves converting spatially continuous raster data into spatially discrete vector data such as points, lines or polygons.\nThere are three standard methods to convert a raster to a vector layer:\n\nRaster to polygons\nRaster to points\nRaster to contours\n\nThe most straightforward form of vectorization is the first one, converting raster cells to polygons, where each pixel is represented by a rectangular polygon. The second method, raster to points, has the additional step of calculating polygon centroids. The third method, raster to contours, is somewhat unrelated. Let us demonstrate the three in the given order.\nThe rasterio.features.shapes function can be used to access to the raster pixel as polygon geometries, as well as raster values. The returned object is a generator, which yields geometry,value pairs. The additional transform argument is used to yield true spatial coordinates of the polygons, which is usually what we want.\nFor example, the following expression returns a generator named shapes, referring to the pixel polygons:\n\nshapes = rasterio.features.shapes(\n    src_grain.read(), \n    transform=src_grain.transform\n)\nshapes\n\n<generator object shapes at 0x7fa1ded2dad0>\n\n\nWe can generate all shapes at once, into a list named pol, as follows:\n\npol = list(shapes)\n\nEach element in pol is a tuple of length 2, containing:\n\nThe GeoJSON-like dict representing the polygon geometry\nThe value of the pixel(s) which comprise the polygon\n\nFor example:\n\npol[0]\n\n({'type': 'Polygon',\n  'coordinates': [[(-1.5, 1.5),\n    (-1.5, 1.0),\n    (-1.0, 1.0),\n    (-1.0, 1.5),\n    (-1.5, 1.5)]]},\n 1.0)\n\n\nNote that each raster cell is converted into a polygon consisting of five coordinates, all of which are stored in memory (explaining why rasters are often fast compared with vectors!).\nTo transform the list into a GeoDataFrame, we need few more steps of data reshaping:\n\n# Create 'GeoSeries' with the polygons\ngeom = [shapely.geometry.shape(i[0]) for i in pol]\ngeom = gpd.GeoSeries(geom, crs=src_grain.crs)\n# Create 'Series' with the values\nvalues = [i[1] for i in pol]\nvalues = pd.Series(values)\n# Combine the 'Series' and 'GeoSeries' into a 'DataFrame'\nresult = gpd.GeoDataFrame({'value': values, 'geometry': geom})\nresult\n\n\n\n\n\n  \n    \n      \n      value\n      geometry\n    \n  \n  \n    \n      0\n      1.0\n      POLYGON ((-1.50000 1.50000, -1....\n    \n    \n      1\n      0.0\n      POLYGON ((-1.00000 1.50000, -1....\n    \n    \n      2\n      1.0\n      POLYGON ((-0.50000 1.50000, -0....\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      11\n      2.0\n      POLYGON ((0.00000 -0.50000, 0.5...\n    \n    \n      12\n      0.0\n      POLYGON ((0.50000 -1.00000, 0.5...\n    \n    \n      13\n      2.0\n      POLYGON ((1.00000 -1.00000, 1.0...\n    \n  \n\n14 rows × 2 columns\n\n\n\nThe resulting polygon layer is shown in Figure 5.6. As shown using the edgecolor='black' option, neighboring pixels sharing the same raster value are dissolved into larger polygons. The rasterio.features.shapes function does not offer a way to avoid this type of dissolving. One way to work around that is to convert an array with consecutive IDs, instead of the real values, to polygons, then extract the real values from the raster (similarly to the “raster to points” example, see below).\n\nresult.plot(column='value', edgecolor='black', legend=True);\n\n\n\n\nFigure 5.6: grain.tif converted to a polygon layer\n\n\n\n\nTo transform raster to points, we can use rasterio.features.shapes, as in conversion to polygons, only with the addition of the .centroid method to go from polygons to their centroids. However, to avoid dissolving nearby pixels, we will actually convert a raster with consecutive IDs, then extract the “true” values by point (it is not strictly necessary in this example, since the values of elev.tif are all unique):\n\n# Prepare IDs array\nr = src_elev.read(1)\nids = r.copy()\nids = np.arange(0, r.size).reshape(r.shape).astype(np.int32)\nids\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]], dtype=int32)\n\n\n\n# IDs raster to points\nshapes = rasterio.features.shapes(ids, transform=src_elev.transform)\npol = list(shapes)\ngeom = [shapely.geometry.shape(i[0]).centroid for i in pol]\ngeom = gpd.GeoSeries(geom, crs=src_elev.crs)\nresult = gpd.GeoDataFrame(geometry=geom)\n\n\n# Extract values to points\nresult['value'] = rasterstats.point_query(\n    result, \n    r, \n    nodata = src_elev.nodata, \n    affine = src_elev.transform,\n    interpolate='nearest'\n)\n\n/usr/local/lib/python3.11/site-packages/rasterstats/io.py:313: UserWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\"Setting nodata to -999; specify nodata explicitly\")\n\n\nThe result is shown in Figure 5.7.\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nresult.plot(column='value', legend=True, ax=axes[0])\nshow(src_elev, transform=src_elev.transform, ax=axes[0])\nresult.plot(column='value', legend=True, ax=axes[1])\nshow(src_elev, cmap='Greys', ax=axes[1]);\n\n\n\n\nFigure 5.7: Raster and point representation of the elev.tif.\n\n\n\n\nAnother common type of spatial vectorization is the creation of contour lines representing lines of continuous height or temperatures (isotherms) for example. We will use a real-world digital elevation model (DEM) because the artificial raster elev produces parallel lines (task for the reader: verify this and explain why this happens). Plotting contour lines is straightforward, using the contour=True option of rasterio.plot.show (Figure 5.8):\n\nfig, ax = plt.subplots(1)\nshow(src_dem, ax=ax)\nshow(src_dem, ax=ax, contour=True, levels = np.arange(0,1200,50), colors='black');\n\n\n\n\nFigure 5.8: Displaying raster contours\n\n\n\n\nUnfortunately, rasterio does not provide any way of extracting the contour lines in the form of a vector layer, for uses other than plotting. There are two possible workarounds:\n\nUsing gdal_contour on the command line (see below), or through its Python interface osgeo\nWriting a custom function to export contour coordinates generated by, e.g., matplotlib or skimage\n\nWe hereby demonstrate the first and easiest approach, using gdal_contour. Although we deviate from exclusively using the Python language, the benefit of gdal_contour is the proven algorithm, customized to spatial data, and with many relevant options. gdal_contour (along with other GDAL programs) should already be installed on your system since this is a dependency of rasterio. For example, generating 50 \\(m\\) contours of the dem.tif file can be done as follows:\n\nos.system('gdal_contour -a elev data/dem.tif output/dem_contour.gpkg -i 50.0')\n\nsh: 1: gdal_contour: not found\n\n\n32512\n\n\nNote that we ran the gdal_contour command through os.system, in order to remain in the Python environment. You can also run the standalone command in the command line interface you are using, such as the Anaconda Prompt:\ngdal_contour -a elev data/dem.tif output/dem_contour.gpkg -i 50.0\nLike all GDAL programs, gdal_contour works with files. Here:\n\nThe input is the data/dem.tif file\nThe result is exported to the output/dem_contour.gpkg file\n\nTo illustrate the result, let’s read the result back into the Python environment. Note that the layer contains an arrtibute named elev (as specified using -a elev) with the contour elevation values:\n\ncontours = gpd.read_file('output/dem_contour.gpkg')\ncontours\n\n\n\n\n\n  \n    \n      \n      ID\n      elev\n      geometry\n    \n  \n  \n    \n      0\n      0\n      750.0\n      LINESTRING (795382.355 8935384....\n    \n    \n      1\n      1\n      800.0\n      LINESTRING (795237.703 8935384....\n    \n    \n      2\n      2\n      650.0\n      LINESTRING (798098.379 8935384....\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      29\n      29\n      450.0\n      LINESTRING (795324.083 8931774....\n    \n    \n      30\n      30\n      450.0\n      LINESTRING (795488.616 8931774....\n    \n    \n      31\n      31\n      450.0\n      LINESTRING (795717.420 8931774....\n    \n  \n\n32 rows × 3 columns\n\n\n\nHere is a plot of the contour layer in dem_contour.gpkg (Figure 5.9):\n\nfig, ax = plt.subplots()\nshow(src_dem, ax=ax)\ncontours.plot(ax=ax, edgecolor='black');\n\n\n\n\nFigure 5.9: Raster contours calculated with the gdal_contour program"
  },
  {
    "objectID": "06-raster-vector.html#exercises",
    "href": "06-raster-vector.html#exercises",
    "title": "5  Raster-vector interactions",
    "section": "5.7 Exercises",
    "text": "5.7 Exercises"
  },
  {
    "objectID": "07-reproj.html#prerequisites",
    "href": "07-reproj.html#prerequisites",
    "title": "6  Reprojecting geographic data",
    "section": "6.1 Prerequisites",
    "text": "6.1 Prerequisites\nLet’s import the required packages:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport shapely.geometry\nimport geopandas as gpd\nimport rasterio\nimport rasterio.warp\nfrom rasterio.plot import show\nimport pyproj\n\nand load the sample data:\n\nsrc_srtm = rasterio.open('data/srtm.tif')\nsrc_nlcd = rasterio.open('data/nlcd.tif')\nzion = gpd.read_file('data/zion.gpkg')\nworld = gpd.read_file('data/world.gpkg')"
  },
  {
    "objectID": "07-reproj.html#introduction",
    "href": "07-reproj.html#introduction",
    "title": "6  Reprojecting geographic data",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nSection 6.3 introduced coordinate reference systems (CRSs), with a focus on the two major types: geographic (‘lon/lat’, with units in degrees longitude and latitude) and projected (typically with units of meters from a datum) coordinate systems. This chapter builds on that knowledge and goes further. It demonstrates how to set and transform geographic data from one CRS to another and, furthermore, highlights specific issues that can arise due to ignoring CRSs that you should be aware of, especially if your data is stored with lon/lat coordinates.\nIn many projects there is no need to worry about, let alone convert between, different CRSs. It is important to know if your data is in a projected or geographic coordinate system, and the consequences of this for geometry operations. However, if you know the CRS of your data and the consequences for geometry operations (covered in the next section), CRSs should just work behind the scenes: people often suddenly need to learn about CRSs when things go wrong. Having a clearly defined project CRS that all project data is in, plus understanding how and why to use different CRSs, can ensure that things don’t go wrong. Furthermore, learning about coordinate systems will deepen your knowledge of geographic datasets and how to use them effectively.\nThis chapter teaches the fundamentals of CRSs, demonstrates the consequences of using different CRSs (including what can go wrong), and how to ‘reproject’ datasets from one coordinate system to another. In the next section we introduce CRSs in Python, followed by Section 6.4 which shows how to get and set CRSs associated with spatial objects. Section Section 6.5 demonstrates the importance of knowing what CRS your data is in with reference to a worked example of creating buffers. We tackle questions of when to reproject and which CRS to use in Section Section 6.6 and Section Section 6.7, respectively. We cover reprojecting vector and raster objects in sections Section 6.8 and Section 6.9 and modifying map projections in Section 6.10."
  },
  {
    "objectID": "07-reproj.html#sec-coordinate-reference-systems",
    "href": "07-reproj.html#sec-coordinate-reference-systems",
    "title": "6  Reprojecting geographic data",
    "section": "6.3 Coordinate Reference Systems",
    "text": "6.3 Coordinate Reference Systems\nMost modern geographic tools that require CRS conversions, including Python packages and desktop GIS software such as QGIS, interface with PROJ, an open source C++ library that “transforms coordinates from one coordinate reference system (CRS) to another”. CRSs can be described in many ways, including the following.\n\nSimple yet potentially ambiguous statements such as “it’s in lon/lat coordinates”.\nFormalized yet now outdated ‘proj4 strings’ such as +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs.\nWith an identifying ‘authority:code’ text string such as EPSG:4326.\n\nEach refers to the same thing: the ‘WGS84’ coordinate system that forms the basis of Global Positioning System (GPS) coordinates and many other datasets. But which one is correct?\nThe short answer is that the third way to identify CRSs is correct: EPSG:4326 is understood by geopandas and rasterio packages covered in this book, plus many other software projects for working with geographic data including QGIS and PROJ. EPSG:4326 is future-proof. Furthermore, although it is machine readable, unlike the proj-string representation “EPSG:4326” is short, easy to remember and highly ‘findable’ online (searching for EPSG:4326 yields a dedicated page on the website epsg.io, for example). The more concise identifier 4326 is also understood by geopandas and rasterio, but we recommend the more explicit AUTHORITY:CODE representation to prevent ambiguity and to provide context.\nThe longer answer is that none of the three descriptions are sufficient and more detail is needed for unambiguous CRS handling and transformations: due to the complexity of CRSs, it is not possible to capture all relevant information about them in such short text strings. For this reason, the Open Geospatial Consortium (OGC, which also developed the simple features specification that the sf package implements) developed an open standard format for describing CRSs that is called WKT (Well Known Text). This is detailed in a 100+ page document that “defines the structure and content of a text string implementation of the abstract model for coordinate reference systems described in ISO 19111:2019” (Open Geospatial Consortium 2019…to add citation!). The WKT representation of the WGS84 CRS, which has the identifier EPSG:4326 is as follows:\n\ncrs = pyproj.CRS.from_string('EPSG:4326') # or '.from_epsg(4326)'\nprint(crs.to_wkt(pretty=True))\n\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nThe output of the command shows how the CRS identifier (also known as a Spatial Reference Identifier or SRID) works: it is simply a look-up, providing a unique identifier associated with a more complete WKT representation of the CRS. This raises the question: what happens if there is a mismatch between the identifier and the longer WKT representation of a CRS? On this point Open Geospatial Consortium (2019… to add citation!) is clear, the verbose WKT representation takes precedence over the identifier:\n\nShould any attributes or values given in the cited identifier be in conflict with attributes or values given explicitly in the WKT description, the WKT values shall prevail.\n\nThe convention of referring to CRSs identifiers in the form AUTHORITY:CODE, which is also used by geographic software written in other languages, allows a wide range of formally defined coordinate systems to be referred to.26 The most commonly used authority in CRS identifiers is EPSG, an acronym for the European Petroleum Survey Group which published a standardized list of CRSs (the EPSG was taken over by the oil and gas body the Geomatics Committee of the International Association of Oil & Gas Producers (…to add citation!) in 2005). Other authorities can be used in CRS identifiers. ESRI:54030, for example, refers to ESRI’s implementation of the Robinson projection, which has the following WKT string:\n\ncrs = pyproj.CRS.from_string('ESRI:54030')\nprint(crs.to_wkt(pretty=True))\n\nPROJCRS[\"World_Robinson\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"World_Robinson\",\n        METHOD[\"Robinson\"],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Not known.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"ESRI\",54030]]\n\n\nWKT strings are exhaustive, detailed, and precise, allowing for unambiguous CRSs storage and transformations. They contain all relevant information about any given CRS, including its datum and ellipsoid, prime meridian, projection, and units.\nRecent PROJ versions (6+) still allow use of proj-strings to define coordinate operations, but some proj-string keys (+nadgrids, +towgs84, +k, +init=epsg:) are either no longer supported or are discouraged. Additionally, only three datums (i.e., WGS84, NAD83, and NAD27) can be directly set in proj-string. Longer explanations of the evolution of CRS definitions and the PROJ library can be found in Bivand (2021), Chapter 2 of Pebesma and Bivand (2022), and a blog post by Floris Vanderhaeghe (…to add citations!). As outlined in the PROJ documentation there are different versions of the WKT CRS format including WKT1 and two variants of WKT2, the latter of which (WKT2, 2018 specification) corresponds to the ISO 19111:2019 (Open Geospatial Consortium 2019…to add citations!)."
  },
  {
    "objectID": "07-reproj.html#sec-querying-and-setting-coordinate-systems",
    "href": "07-reproj.html#sec-querying-and-setting-coordinate-systems",
    "title": "6  Reprojecting geographic data",
    "section": "6.4 Querying and setting coordinate systems",
    "text": "6.4 Querying and setting coordinate systems\nLet’s look at how CRSs are stored in Python spatial objects and how they can be queried and set. First we will look at getting and setting CRSs in vector geographic data objects. Consider the GeoDataFrame object named world, imported from a file world.gpkg. The object world represents countries worldwide. Its CRS can be retrieved using the .crs property:\n\nworld.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe output specifies the following pieces of information:\n\nThe CRS type (Geographic 2D CRS) and EPSG code (EPSG:4326)\nThe CRS name (WGS 84)\nThe axes (latitude, longitude) and their units (degree)\nThe applicable area name (World) and bounding box ((-180.0, -90.0, 180.0, 90.0))\nThe datum (WGS 84)\n\nThe WKT representation, which is internally used when saving the object to a file or doing any coordinate operations, can be extracted using .crs.to_wkt() as shown above (Section 6.3). Above, we can see that the world object has the WGS84 ellipsoid, uses the Greenwich prime meridian, and the latitude and longitude axis order. We also have the suitable suitable area specification for the use of this CRS, and CRS identifier: EPSG:4326.\nThe CRS specification object, such as world.crs, has several other useful properties and methods to retrieve additional information about the used CRS. For example, try to run:\n\nworld.crs.is_geographic to check is the CRS is geographic or not\nworld.crs.axis_info[0].unit_name and world.crs.axis_info[1].unit_name to find out the CRS units of both axes (typically having identical units)\nworld.crs.to_authority() extracts the authority (e.g., EPSG) and the identifier (e.g., 4326)\nworld.crs.to_proj4() returns the proj-string representation\n\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .set_crs method can be used on a GeoSeries or a GeoDataFrame to set it. The CRS can be specified using an EPSG code as the first argument. In case the object already has a different CRS definition, we must also specify allow_override=True to replace it (otherwise we get an error). For example, here we set the EPSG:4326 CRS, which has no effect because world already has that exact CRS definition:\n\nworld2 = world.set_crs(4326)\n\nand here we replace the definition from the existing EPSG:4326, to a new definition EPSG:3857:\n\nworld3 = world.set_crs(3857, allow_override=True)\n\nA number is interpreted as an EPSG code. We can also use strings, as in 'EPSG:4326', which is useful to make the code more clear and when using other authorities:\n\nworld4 = world.set_crs('ESRI:54009', allow_override=True)\n\nIn rasterio, the CRS information is stored as part of a raster file connection metadata (Section 1.3.2). Replacing the CRS definition for a rasterio file connection is typically not necessary, because it is not considered in any operation, only the transformation matrix and coordinates are. One exception is when writing the raster, in which case we need to construct the metadata of the raster file to be written, and therein specify the CRS anyway (Section 1.3.3). However, if we do for some reason need to change the CRS definition in the file connection metadata, we can do that when opening the file in r+ (reading and writing) mode:\n\nsrc_nlcd2 = rasterio.open('data/nlcd.tif', 'r+')\nsrc_nlcd2.crs\n\nCRS.from_epsg(3857)\n\n\n\nsrc_nlcd2.crs = 3857\nsrc_nlcd2.crs\n\nCRS.from_epsg(3857)\n\n\nImportantly, the .set_crs (for vector layers) or the assignment to .crs (for rasters), as shown above, do not alter coordinates’ values or geometries. Their role is only to set a metadata information about the object CRS. Consequently, the objects we created, world3, world4, and src_nlcd2 are “incorrect”, in the sense that the geometries are in fact given in a different CRS than specified in the associated CRS definition.\nIn some cases the CRS of a geographic object is unknown, as is the case in the london dataset created in the code chunk below, building on the example of London introduced in Section 1.2.6:\n\nlnd_point = shapely.geometry.Point(0.1, 51.5)\nlnd_geom = gpd.GeoSeries([lnd_point])\nlnd_layer = gpd.GeoDataFrame({'geometry': lnd_geom})\nlnd_layer\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POINT (0.10000 51.50000)\n    \n  \n\n\n\n\n\nlnd_layer.crs\n\nNothing is printed as a result of the last expression, because the value of the CRS definition in .crs is None. This implies that geopandas does not know what the CRS is and is unwilling to guess. Unless a CRS is manually specified or is loaded from a source that has CRS metadata, geopandas does not make any explicit assumptions about which coordinate systems, other than to say “I don’t know”. This behavior makes sense given the diversity of available CRSs but differs from some approaches, such as the GeoJSON file format specification, which makes the simplifying assumption that all coordinates have a lon/lat CRS: EPSG:4326.\nA CRS can be added to GeoSeries or GeoDataFrame objects using the .set_crs method, as mentioned above. Since the definition is missing, we do not need to specify allow_override=True, as there is nothing to override. For example:\n\nlnd_layer = lnd_layer.set_crs(4326)\nlnd_layer.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIn general, all geographic coordinates have a coordinate system and software can only make good decisions around plotting and and geometry operations if it knows what type of CRS it is working with. When working with geopandas and rasterio, datasets without a specified CRS are not an issue in most workflows, since only the coordinates are considered. It is up to the user to make sure that, when working with more than one layer, all of the coordinates are given in the same CRS (whether specified or not). When exporting the results, though, it is important to keep the CRS definition in place, because other software typically do use, and require, the CRS definition in calculation. It should also be mentioned that, in some cases the CRS specification is left unspecified on purpose, for example when working with layers in arbitrary or non-geographic space (simulations, internal building plans, analysis of plot-scale ecological patterns, etc.)."
  },
  {
    "objectID": "07-reproj.html#sec-geometry-operations-on-projected-and-unprojected-data",
    "href": "07-reproj.html#sec-geometry-operations-on-projected-and-unprojected-data",
    "title": "6  Reprojecting geographic data",
    "section": "6.5 Geometry operations on projected and unprojected data",
    "text": "6.5 Geometry operations on projected and unprojected data"
  },
  {
    "objectID": "07-reproj.html#sec-when-to-reproject",
    "href": "07-reproj.html#sec-when-to-reproject",
    "title": "6  Reprojecting geographic data",
    "section": "6.6 When to reproject?",
    "text": "6.6 When to reproject?"
  },
  {
    "objectID": "07-reproj.html#sec-which-crs-to-use",
    "href": "07-reproj.html#sec-which-crs-to-use",
    "title": "6  Reprojecting geographic data",
    "section": "6.7 Which CRS to use?",
    "text": "6.7 Which CRS to use?"
  },
  {
    "objectID": "07-reproj.html#sec-reprojecting-vector-geometries",
    "href": "07-reproj.html#sec-reprojecting-vector-geometries",
    "title": "6  Reprojecting geographic data",
    "section": "6.8 Reprojecting vector geometries",
    "text": "6.8 Reprojecting vector geometries\nChapter 1 demonstrated how vector geometries are made-up of points, and how points form the basis of more complex objects such as lines and polygons. Reprojecting vectors thus consists of transforming the coordinates of these points, which form the vertices of lines and polygons.\nSection 6.5 contains an example in which at least one GeoDataFrame object must be transformed into an equivalent object with a different CRS to calculate the distance between two objects (?).\n…"
  },
  {
    "objectID": "07-reproj.html#sec-reprojecting-raster-geometries",
    "href": "07-reproj.html#sec-reprojecting-raster-geometries",
    "title": "6  Reprojecting geographic data",
    "section": "6.9 Reprojecting raster geometries",
    "text": "6.9 Reprojecting raster geometries\nThe projection concepts described in the previous section apply equally to rasters. However, there are important differences in reprojection of vectors and rasters: transforming a vector object involves changing the coordinates of every vertex but this does not apply to raster data. Rasters are composed of rectangular cells of the same size (expressed by map units, such as degrees or meters), so it is usually impracticable to transform coordinates of pixels separately. Raster reprojection involves creating a new raster object, often with a different number of columns and rows than the original. The attributes must subsequently be re-estimated, allowing the new pixels to be ‘filled’ with appropriate values. In other words, raster reprojection can be thought of as two separate spatial operations: a vector reprojection of the raster extent to another CRS (Section 6.8), and computation of new pixel values through resampling (Section 4.4.4). Thus in most cases when both raster and vector data are used, it is better to avoid reprojecting rasters and reproject vectors instead.\n\n\n\n\n\n\nNote\n\n\n\nReprojection of the regular rasters is also known as warping. Additionally, there is a second similar operation called “transformation”. Instead of resampling all of the values, it leaves all values intact but recomputes new coordinates for every raster cell, changing the grid geometry. For example, it could convert the input raster (a regular grid) into a curvilinear grid. The rasterio, like common raster file formats (such as GeoTIFF), does not support curvilinear grids (?).\n\n\nThe raster reprojection process is done using two functions from the rasterio.warp sub-package:\n\nrasterio.warp.calculate_default_transform\nrasterio.warp.reproject\n\nThe first function, calculate_default_transform, is used to calculate the new transformation matrix in the destination CRS, according to the source raster dimensions and bounds. Alternatively, the destination transformation matrix can be obtained from an existing raster; this is common practice when we need to align one raster with another, for instance to be able to combine them in raster algebra operations (Section 3.4.3) (see below). The second function rasterio.warp.reproject then actually calculates cell values in the destination grid, using the user-selected resampling method (such as nearest neighbor, or bilinear).\nLet’s take a look at two examples of raster transformation: using categorical and continuous data. Land cover data are usually represented by categorical maps. The nlcd.tif file provides information for a small area in Utah, USA obtained from National Land Cover Database 2011 in the NAD83 / UTM zone 12N CRS, as shown in the output of the code chunk below (only first line of output shown). We already created a connection to the nlcd.tif file, named src_nlcd:\n\nsrc_nlcd\n\n<open DatasetReader name='data/nlcd.tif' mode='r'>\n\n\nRecall that the raster transformation matrix and dimensions are accessible from the file connection as follows. This information will be required to calculate the destination transformation matrix (hereby printed collectively in a tuple):\n\nsrc_nlcd.transform, src_nlcd.width, src_nlcd.height\n\n(Affine(31.530298224786595, 0.0, 301903.344386758,\n        0.0, -31.52465870178793, 4154086.47216415),\n 1073,\n 1359)\n\n\nFirst, let’s define the destination CRS. In this case, we choose WGS84 (EPSG code 4326):\n\ndst_crs = 'EPSG:4326'\n\nNow, we are ready to claculate the destination raster transformation matrix (dst_transform), and the destination dimensions (dst_width, dst_height), as follows:\n\ndst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n    src_nlcd.crs,\n    dst_crs,\n    src_nlcd.width,\n    src_nlcd.height,\n    *src_nlcd.bounds\n)\ndst_transform, dst_width, dst_height\n\n(Affine(0.00025326212175780043, 0.0, 2.7120438858942424,\n        0.0, -0.00025326212175780043, 34.92789350789737),\n 1200,\n 1248)\n\n\nNote that *, in *src_nlcd.bounds, is used to unpack src_nlcd.bounds to four separate arguments, which calculate_default_transform requires:\n\nsrc_nlcd.bounds\n\nBoundingBox(left=301903.344386758, bottom=4111244.46098842, right=335735.354381954, top=4154086.47216415)\n\n\nNext, we will create the metadata file used for writing the reprojected raster to file. For convenience, we are taking the metadata of the source raster (src_nlcd.meta), making a copy (dst_kwargs), and then updating those specific properties that need to be changed. Note that the reprojection process typically creates “No Data” pixels, even when there were none in the input raster, since the raster orientation changes and the edges need to be “filled” to get back a rectangular extent. We need to specify a “No Data” value of our choice, if there is none, or use the existing source raster setting, such as 255 in this case:\n\ndst_kwargs = src_nlcd.meta.copy()\ndst_kwargs.update({\n    'crs': dst_crs,\n    'transform': dst_transform,\n    'width': dst_width,\n    'height': dst_height\n})\ndst_kwargs\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1200,\n 'height': 1248,\n 'count': 1,\n 'crs': 'EPSG:4326',\n 'transform': Affine(0.00025326212175780043, 0.0, 2.7120438858942424,\n        0.0, -0.00025326212175780043, 34.92789350789737)}\n\n\nWe are ready to create the reprojected raster. Here, reprojection takes place between two file connections, meaning that the raster value arrays are not being read into memory at once. It is also possible to reproject into an in-memory ndarray object, see the documentation.\nTo write the reprojected raster, we first create a destination file connection dst_nlcd, pointing at the output file path of our choice (output/nlcd_4326.tif), using the updated metadata object created earlier (dst_kwargs):\n\ndst_nlcd = rasterio.open('output/nlcd_4326.tif', 'w', **dst_kwargs)\n\nThen, we use the rasterio.warp.reproject function to calculate and write the reprojection result into the dst_nlcd file connection. Note that the source and destination accept a “band” object, created using rasterio.band. In this case, there is just one band. If there were more bands, we would have to repeat the procedure for each band, using i instead of 1 inside a loop:\n\nrasterio.warp.reproject(\n    source=rasterio.band(src_nlcd, 1),\n    destination=rasterio.band(dst_nlcd, 1),\n    src_transform=src_nlcd.transform,\n    src_crs=src_nlcd.crs,\n    dst_transform=dst_transform,\n    dst_crs=dst_crs,\n    resampling=rasterio.enums.Resampling.nearest\n)\n\n(Band(ds=<open DatasetWriter name='output/nlcd_4326.tif' mode='w'>, bidx=1, dtype='uint8', shape=(1248, 1200)),\n Affine(0.00025326212175780043, 0.0, 2.7120438858942424,\n        0.0, -0.00025326212175780043, 34.92789350789737))\n\n\nFinally, we close the file connection so that the data are actually written:\n\ndst_nlcd.close()\n\nMany properties of the new object differ from the previous one, including the number of columns and rows (and therefore number of cells), resolution (transformed from meters into degrees), and extent, as summarized again below (note that the number of categories increases from 8 to 9 because of the addition of NA values, not because a new category has been created — the land cover classes are preserved).\n\nsrc_nlcd.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1073,\n 'height': 1359,\n 'count': 1,\n 'crs': CRS.from_epsg(3857),\n 'transform': Affine(31.530298224786595, 0.0, 301903.344386758,\n        0.0, -31.52465870178793, 4154086.47216415)}\n\n\n\nsrc_nlcd_4326 = rasterio.open('output/nlcd_4326.tif')\nsrc_nlcd_4326.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1200,\n 'height': 1248,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.00025326212175780043, 0.0, 2.7120438858942424,\n        0.0, -0.00025326212175780043, 34.92789350789737)}\n\n\nExamining the unique raster values tells us that the new raster has the same categories, plus the value 255 representing “No Data”:\n\nnp.unique(src_nlcd.read(1))\n\narray([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint8)\n\n\n\nnp.unique(src_nlcd_4326.read(1))\n\narray([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint8)\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nshow(src_nlcd, ax=axes[0], cmap='Set3')\nshow(src_nlcd_4326, ax=axes[1], cmap='Set3')\naxes[0].set_title('Original (EPSG:26912)')\naxes[1].set_title('Reprojected (EPSG:4326)');\n\n\n\n\nFigure 6.1: Reprojecting a categorical raster using nearest neighbor resampling\n\n\n\n\nIn the above example, we automatically calculated an optimal (i.e., most information preserving) destination grid using rasterio.warp.calculate_default_transform. This is appropriate when there are no specific requirements for the destination raster spatial properties. Namely, we are not required to otain a specific origin and resolution, but just wish to preserve the raster values as much as possible. To do that, calculate_default_transform “tries” to keep the extent and resolution of the destination raster as similar as possible to the source. In other situations, however, we need to reproject a raster into a specific “template”, so that it corresponds, for instance, with other rasters we use in the analysis. In the following code section, we reproject the nlcd.tif raster, again, buit this time using the nlcd_4326.tif reprojection result as the “template” to demonstrate this alternative workflow.\nFirst, we create a connection to our “template” raster to read its metadata:\n\ntemplate = rasterio.open('output/nlcd_4326.tif')\ntemplate.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 255.0,\n 'width': 1200,\n 'height': 1248,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.00025326212175780043, 0.0, 2.7120438858942424,\n        0.0, -0.00025326212175780043, 34.92789350789737)}\n\n\nThen, we create a write-mode connection to our destination raster, using this metadata, meaning that as the resampling result is going to have identical metadata as the “template”:\n\ndst_nlcd_2 = rasterio.open('output/nlcd_4326_2.tif', 'w', **template.meta)\n\nNow, we can resample and write the result:\n\nrasterio.warp.reproject(\n    source=rasterio.band(src_nlcd, 1),\n    destination=rasterio.band(dst_nlcd_2, 1),\n    src_transform=src_nlcd.transform,\n    src_crs=src_nlcd.crs,\n    dst_transform=dst_nlcd_2.transform,\n    dst_crs=dst_nlcd_2.crs,\n    resampling=rasterio.enums.Resampling.nearest\n)\n\n(Band(ds=<open DatasetWriter name='output/nlcd_4326_2.tif' mode='w'>, bidx=1, dtype='uint8', shape=(1248, 1200)),\n Affine(0.00025326212175780043, 0.0, 2.7120438858942424,\n        0.0, -0.00025326212175780043, 34.92789350789737))\n\n\n\ndst_nlcd_2.close()\n\nNaturally, in this case, the outputs nlcd_4326.tif and nlcd_4326_2.tif are identical, as we used the same “template” and the same source data:\n\nd = rasterio.open('output/nlcd_4326.tif').read(1) == rasterio.open('output/nlcd_4326_2.tif').read(1)\nd\n\narray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])\n\n\n\nnp.all(d)\n\nTrue\n\n\nThe difference is that in the first example we calculate the template automatically, using rasterio.warp.calculate_default_transform, while in the second example we used an existing raster as the “template”.\nImportantly, when the template raster has much more “coarse” resolution than the source raster, the:\n\nrasterio.enums.Resampling.average (for continuous rasters), or\nrasterio.enums.Resampling.mode (for categorical rasters)\n\nresampling method should be used, instead of rasterio.enums.Resampling.nearest. Otherwise, much of the data will be lost, as the “nearest” method can capture one pixel value only for each destination raster pixel.\nReprojecting continuous rasters (with numeric or, in this case, integer values) follows an almost identical procedure. This is demonstrated below with srtm.tif from the Shuttle Radar Topography Mission (SRTM), which represents height in meters above sea level (elevation) with the WGS84 CRS.\nWe will reproject this dataset into a projected CRS, but not with the nearest neighbor method which is appropriate for categorical data. Instead, we will use the bilinear method which computes the output cell value based on the four nearest cells in the original raster. The values in the projected dataset are the distance-weighted average of the values from these four cells: the closer the input cell is to the center of the output cell, the greater its weight. The following code section create a text string representing WGS 84 / UTM zone 12N, and reproject the raster into this CRS, using the bilinear method. The code is practically the same, except for changing the source and destination file names, and replacing nearest with bilinear:\n\ndst_crs = 'EPSG:32612'\ndst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n    src_srtm.crs,\n    dst_crs,\n    src_srtm.width,\n    src_srtm.height,\n    *src_srtm.bounds\n)\ndst_kwargs = src_srtm.meta.copy()\ndst_kwargs.update({\n    'crs': dst_crs,\n    'transform': dst_transform,\n    'width': dst_width,\n    'height': dst_height\n})\ndst_srtm = rasterio.open('output/srtm_32612.tif', 'w', **dst_kwargs)\nrasterio.warp.reproject(\n    source=rasterio.band(src_srtm, 1),\n    destination=rasterio.band(dst_srtm, 1),\n    src_transform=src_srtm.transform,\n    src_crs=src_srtm.crs,\n    dst_transform=dst_transform,\n    dst_crs=dst_crs,\n    resampling=rasterio.enums.Resampling.bilinear\n)\ndst_srtm.close()\n\nFigure 6.2 shows the input and the reprojected SRTM rasters.\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\nshow(src_srtm, ax=axes[0])\nshow(rasterio.open('output/srtm_32612.tif'), ax=axes[1])\naxes[0].set_title('Original (EPSG:4326)')\naxes[1].set_title('Reprojected (EPSG:32612)');\n\n\n\n\nFigure 6.2: Reprojecting a continuous raster using bilinear resampling"
  },
  {
    "objectID": "07-reproj.html#sec-custom-map-projections",
    "href": "07-reproj.html#sec-custom-map-projections",
    "title": "6  Reprojecting geographic data",
    "section": "6.10 Custom map projections",
    "text": "6.10 Custom map projections"
  },
  {
    "objectID": "07-reproj.html#exercises",
    "href": "07-reproj.html#exercises",
    "title": "6  Reprojecting geographic data",
    "section": "6.11 Exercises",
    "text": "6.11 Exercises"
  },
  {
    "objectID": "08-read-write-plot.html#prerequisites",
    "href": "08-read-write-plot.html#prerequisites",
    "title": "7  Geographic data I/O",
    "section": "7.1 Prerequisites",
    "text": "7.1 Prerequisites\nThis chapter requires the following packages, used in previous chapters:\n\nimport fiona\nimport geopandas as gpd\nimport shapely\nimport rasterio"
  },
  {
    "objectID": "08-read-write-plot.html#introduction",
    "href": "08-read-write-plot.html#introduction",
    "title": "7  Geographic data I/O",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nThis chapter is about reading and writing geographic data. Geographic data import is essential for geocomputation: real-world applications are impossible without data. Data output is also vital, enabling others to use valuable new or improved datasets resulting from your work. Taken together, these processes of import/output can be referred to as data I/O.\nGeographic data I/O is often done with few lines of code at the beginning and end of projects. It is often overlooked as a simple one step process. However, mistakes made at the outset of projects (e.g. using an out-of-date or in some way faulty dataset) can lead to large problems later down the line, so it is worth putting considerable time into identifying which datasets are available, where they can be found and how to retrieve them. These topics are covered in Section 7.3, which describes various geoportals, which collectively contain many terabytes of data, and how to use them. To further ease data access, a number of packages for downloading geographic data have been developed, as described in Section 7.4.\nThere are many geographic file formats, each of which has pros and cons, described in Section 7.6. The process of reading and writing files in formats efficiently is covered in Sections Section 7.7 and Section 7.8, respectively. The final Section Section 7.9 demonstrates methods for saving visual outputs (maps), in preparation for Chapter 8 on visualization."
  },
  {
    "objectID": "08-read-write-plot.html#sec-retrieving-open-data",
    "href": "08-read-write-plot.html#sec-retrieving-open-data",
    "title": "7  Geographic data I/O",
    "section": "7.3 Retrieving open data",
    "text": "7.3 Retrieving open data"
  },
  {
    "objectID": "08-read-write-plot.html#sec-geographic-data-packages",
    "href": "08-read-write-plot.html#sec-geographic-data-packages",
    "title": "7  Geographic data I/O",
    "section": "7.4 Geographic data packages",
    "text": "7.4 Geographic data packages"
  },
  {
    "objectID": "08-read-write-plot.html#geographic-web-services",
    "href": "08-read-write-plot.html#geographic-web-services",
    "title": "7  Geographic data I/O",
    "section": "7.5 Geographic web services",
    "text": "7.5 Geographic web services"
  },
  {
    "objectID": "08-read-write-plot.html#sec-file-formats",
    "href": "08-read-write-plot.html#sec-file-formats",
    "title": "7  Geographic data I/O",
    "section": "7.6 File formats",
    "text": "7.6 File formats\nGeographic datasets are usually stored as files or in spatial databases. File formats can either store vector or raster data, while spatial databases such as PostGIS can store both. The large variety of file formats may seem bewildering, but there has been much consolidation and standardization since the beginnings of GIS software in the 1960s when the first widely distributed program (SYMAP) for spatial analysis was created at Harvard University [@coppock_history_1991].\nGDAL (which should be pronounced “goo-dal”, with the double “o” making a reference to object-orientation), the Geospatial Data Abstraction Library, has resolved many issues associated with incompatibility between geographic file formats since its release in 2000. GDAL provides a unified and high-performance interface for reading and writing of many raster and vector data formats. Many open and proprietary GIS programs, including GRASS, ArcGIS and QGIS, use GDAL behind their GUIs for doing the legwork of ingesting and spitting out geographic data in appropriate formats.\nGDAL provides access to more than 200 vector and raster data formats. Table 7.1 presents some basic information about selected and often used spatial file formats.\n\n\nTable 7.1: Commonly used spatial data file formats\n\n\n\n\n\n\n\n\n\nName\nExtension\nInfo\nType\nModel\n\n\n\n\nESRI Shapefile\n.shp (the main file)\nPopular format consisting of at least three files. No support for: files > 2GB;mixed types; names > 10 chars; cols > 255.\nVector\nPartially open\n\n\nGeoJSON\n.geojson\nExtends the JSON exchange format by including a subset of the simple feature representation; mostly used for storing coordinates in longitude and latitude; it is extended by the TopoJSON format\nVector\nOpen\n\n\nKML\n.kml\nXML-based format for spatial visualization, developed for use with Google Earth. Zipped KML file forms the KMZ format.\nVector\nOpen\n\n\nGPX\n.gpx\nXML schema created for exchange of GPS data.\nVector\nOpen\n\n\nFlatGeobuf\n.fgb\nSingle file format allowing for quick reading and writing of vector data. Has streaming capabilities.\nVector\nOpen\n\n\nGeoTIFF\n.tif/.tiff\nPopular raster format. A TIFF file containing additional spatial metadata.\nRaster\nOpen\n\n\nArc ASCII\n.asc\nText format where the first six lines represent the raster header, followed by the raster cell values arranged in rows and columns.\nRaster\nOpen\n\n\nSQLite/SpatiaLite\n.sqlite\nStandalone relational database, SpatiaLite is the spatial extension of SQLite.\nVector and raster\nOpen\n\n\nESRI FileGDB\n.gdb\nSpatial and nonspatial objects created by ArcGIS. Allows: multiple feature classes; topology. Limited support from GDAL.\nVector and raster\nProprietary\n\n\nGeoPackage\n.gpkg\nLightweight database container based on SQLite allowing an easy and platform-independent exchange of geodata\nVector and (very limited) raster\nOpen\n\n\n\n\nAn important development ensuring the standardization and open-sourcing of file formats was the founding of the Open Geospatial Consortium (OGC) in 1994. Beyond defining the simple features data model (see Section 1.2.4), the OGC also coordinates the development of open standards, for example as used in file formats such as KML and GeoPackage. Open file formats of the kind endorsed by the OGC have several advantages over proprietary formats: the standards are published, ensure transparency and open up the possibility for users to further develop and adjust the file formats to their specific needs.\nESRI Shapefile is the most popular vector data exchange format; however, it is not an open format (though its specification is open). It was developed in the early 1990s and has a number of limitations. First of all, it is a multi-file format, which consists of at least three files. It only supports 255 columns, column names are restricted to ten characters and the file size limit is 2 GB. Furthermore, ESRI Shapefile does not support all possible geometry types, for example, it is unable to distinguish between a polygon and a multipolygon. Despite these limitations, a viable alternative had been missing for a long time. In the meantime, GeoPackage emerged, and seems to be a more than suitable replacement candidate for ESRI Shapefile. GeoPackage is a format for exchanging geospatial information and an OGC standard. The GeoPackage standard describes the rules on how to store geospatial information in a tiny SQLite container. Hence, GeoPackage is a lightweight spatial database container, which allows the storage of vector and raster data but also of non-spatial data and extensions. Aside from GeoPackage, there are other geospatial data exchange formats worth checking out (Table 7.1).\nThe GeoTIFF format seems to be the most prominent raster data format. It allows spatial information, such as the CRS definition and the transformation matrix (see Section 1.3.2), to be embedded within a TIFF file. Similar to ESRI Shapefile, this format was firstly developed in the 1990s, but as an open format. Additionally, GeoTIFF is still being expanded and improved. One of the most significant recent addition to the GeoTIFF format is its variant called COG (Cloud Optimized GeoTIFF). Raster objects saved as COGs can be hosted on HTTP servers, so other people can read only parts of the file without downloading the whole file (see Sections 8.6.2 and 8.7.2…).\nThere is also a plethora of other spatial data formats that we do not explain in detail or mention in Table 7.1 due to the book limits. If you need to use other formats, we encourage you to read the GDAL documentation about vector and raster drivers. Additionally, some spatial data formats can store other data models (types) than vector or raster. It includes LAS and LAZ formats for storing lidar point clouds, and NetCDF and HDF for storing multidimensional arrays.\nFinally, spatial data is also often stored using tabular (non-spatial) text formats, including CSV files or Excel spreadsheets. This can be convenient to share spatial datasets with people who, or software that, struggle with spatial data formats."
  },
  {
    "objectID": "08-read-write-plot.html#sec-data-input",
    "href": "08-read-write-plot.html#sec-data-input",
    "title": "7  Geographic data I/O",
    "section": "7.7 Data input (I)",
    "text": "7.7 Data input (I)\nExecuting commands such as geopandas.read_file (the main function we use for loading vector data) or rasterio.open+.read (the main functions used for loading raster data) silently sets off a chain of events that reads data from files. Moreover, there are many Python packages containing a wide range of geographic data or providing simple access to different data sources. All of them load the data into the Python environment or, more precisely, assign objects to your workspace, stored in RAM and accessible within the Python session.\n\n7.7.1 Vector data\nSpatial vector data comes in a wide variety of file formats. Most popular representations such as .shp, .geojson, and .gpkg files can be imported and exported with geopandas functions read_file and to_file (covered in Section @ref(sec-data-output)), respectively.\ngeopandas uses GDAL to read and write data, via fiona (the default) or pyogrio packages (a recently developed alternative to fiona). After fiona is imported, the command fiona.supported_drivers can be used to list drivers available to GDAL, including whether they can (r), append (a), or write (w) data, or all three:\n\nfiona.supported_drivers\n\n{'DXF': 'rw',\n 'CSV': 'raw',\n 'OpenFileGDB': 'raw',\n 'ESRIJSON': 'r',\n 'ESRI Shapefile': 'raw',\n 'FlatGeobuf': 'raw',\n 'GeoJSON': 'raw',\n 'GeoJSONSeq': 'raw',\n 'GPKG': 'raw',\n 'GML': 'rw',\n 'OGR_GMT': 'rw',\n 'GPX': 'rw',\n 'MapInfo File': 'raw',\n 'DGN': 'raw',\n 'S57': 'r',\n 'SQLite': 'raw',\n 'TopoJSON': 'r'}\n\n\nOther, less common, drivers can be “activated” by manually supplementing fiona.supported_drivers. The first argument of the geopandas versatile data import function gpd.read_file is filename, which is typically a string, but can also be a file connection. The content of a string could vary between different drivers. In most cases, as with the ESRI Shapefile (.shp) or the GeoPackage format (.gpkg), the filename argument would be a path or a URL to an actual file, such as geodata.gpkg. The driver is automatically selected based on the file extension, as demonstrated for a .gpkg file below:\n\nworld = gpd.read_file('data/world.gpkg')\nworld\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.960000\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n    \n      1\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163000\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      2\n      EH\n      Western Sahara\n      Africa\n      ...\n      NaN\n      NaN\n      MULTIPOLYGON (((-8.66559 27.656...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      174\n      XK\n      Kosovo\n      Europe\n      ...\n      71.097561\n      8698.291559\n      MULTIPOLYGON (((20.59025 41.855...\n    \n    \n      175\n      TT\n      Trinidad and Tobago\n      North America\n      ...\n      70.426000\n      31181.821196\n      MULTIPOLYGON (((-61.68000 10.76...\n    \n    \n      176\n      SS\n      South Sudan\n      Africa\n      ...\n      55.817000\n      1935.879400\n      MULTIPOLYGON (((30.83385 3.5091...\n    \n  \n\n177 rows × 11 columns\n\n\n\nFor some drivers, such as a File Geodatabase (OpenFileGDB), filename could be provided as a folder name. GeoJSON string can also be read from a character string:\n\ngpd.read_file('{\"type\":\"Point\",\"coordinates\":[34.838848,31.296301]}')\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POINT (34.83885 31.29630)\n    \n  \n\n\n\n\nAlternatively, the gpd.read_postgis function can be used to read a vector layer from a PostGIS database.\nSome vector formats, such as GeoPackage, can store multiple data layers. By default, gpd.read_file automatically reads the first layer of the file specified in filename. However, using the layer argument you can specify any other layer.\nThe gpd.read_file function also allows for reading just parts of the file into RAM with two possible mechanisms. The first one is related to the where argument, which allows specifying what part of the data to read using an SQL WHERE expression. An example below extracts data for Tanzania only (Figure …). It is done by specifying that we want to get all rows for which name_long equals to \"Tanzania\":\n\ntanzania = gpd.read_file('data/world.gpkg', where='name_long=\"Tanzania\"')\ntanzania\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      TZ\n      Tanzania\n      Africa\n      ...\n      64.163\n      2402.099404\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n  \n\n1 rows × 11 columns\n\n\n\nIf you do not know the names of the available columns, a good approach is to just read one row of the data using the rows argument, which can be used to read the first N rows, then use the .columns property to examine the column names:\n\ngpd.read_file('data/world.gpkg', rows=1).columns\n\nIndex(['iso_a2', 'name_long', 'continent', 'region_un', 'subregion', 'type',\n       'area_km2', 'pop', 'lifeExp', 'gdpPercap', 'geometry'],\n      dtype='object')\n\n\nThe second mechanism uses the mask argument to filter data based on intersection with an existing geometry. This argument expects a geometry (GeoDataFrame, GeoSeries, or shapely) representing the area where we want to extract the data. Let’s try it using a small example—we want to read polygons from our file that intersect with the buffer of 50,000 \\(m\\) of Tanzania’s borders. To do it, we need to (a) transform the geometry to a projected CRS (such as EPSG:32736), (b) prepare our “filter” by creating the buffer (Section 4.3.3), and (c) transform back to the original CRS to be used as a mask:\n\ntanzania_buf = tanzania.to_crs(32736).buffer(50000).to_crs(4326)\ntanzania_buf.iloc[0]\n\n\n\n\nNow, we can apply this “filter” using the mask argument.\n\ntanzania_neigh = gpd.read_file('data/world.gpkg', mask=tanzania_buf)\ntanzania_neigh\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      MZ\n      Mozambique\n      Africa\n      ...\n      57.099\n      1079.823866\n      MULTIPOLYGON (((34.55999 -11.52...\n    \n    \n      1\n      ZM\n      Zambia\n      Africa\n      ...\n      60.775\n      3632.503753\n      MULTIPOLYGON (((30.74001 -8.340...\n    \n    \n      2\n      MW\n      Malawi\n      Africa\n      ...\n      61.932\n      1090.367208\n      MULTIPOLYGON (((32.75938 -9.230...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6\n      BI\n      Burundi\n      Africa\n      ...\n      56.688\n      803.172837\n      MULTIPOLYGON (((30.46967 -2.413...\n    \n    \n      7\n      UG\n      Uganda\n      Africa\n      ...\n      59.224\n      1637.275081\n      MULTIPOLYGON (((33.90371 -0.950...\n    \n    \n      8\n      RW\n      Rwanda\n      Africa\n      ...\n      66.188\n      1629.868866\n      MULTIPOLYGON (((30.41910 -1.134...\n    \n  \n\n9 rows × 11 columns\n\n\n\nOur result, shown in Figure 7.1, contains Tanzania and every country within its 50,000 \\(m\\) buffer. Note that the last two expressions are used to add text labels with the name_long of each country, placed at the country centroid:\n\nfig, axes = plt.subplots(ncols=2, figsize=(9,5))\ntanzania.plot(ax=axes[0], color='lightgrey', edgecolor='grey')\ntanzania_neigh.plot(ax=axes[1], color='lightgrey', edgecolor='grey')\ntanzania_buf.plot(ax=axes[1], color='none', edgecolor='red')\naxes[0].set_title('where')\naxes[1].set_title('mask')\ntanzania.apply(lambda x: axes[0].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\ntanzania_neigh.apply(lambda x: axes[1].annotate(text=x['name_long'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);\n\n\n\n\nFigure 7.1: Reading a subset of the vector data using a where query (left) and a mask (right)\n\n\n\n\nOften we need to read CSV files (or other tabular formats) which have x and y coordinate columns, and turn them into a GeoDataFrame with point geometries. To do that, we can import the file using pandas (e.g., pd.read_csv or pd.read_excel), then go from DataFrame to GeoDataFrame using the gpd.points_from_xy function, as shown earlier in the book (See Section 1.2.6 and Section 3.3.4). For example, the table cycle_hire_xy.csv, where the coordinates are stored in the X and Y columns in EPSG:4326, can be imported, converted to a GeoDataFrame, and plotted, as follows:\n\ncycle_hire = pd.read_csv('data/cycle_hire_xy.csv')\ngeom = gpd.points_from_xy(cycle_hire['X'], cycle_hire['Y'], crs=4326)\ngeom = gpd.GeoSeries(geom)\ncycle_hire_xy = gpd.GeoDataFrame(data=cycle_hire, geometry=geom)\ncycle_hire_xy.plot();\n\n\n\n\nInstead of columns describing ‘XY’ coordinates, a single column can also contain the geometry information. Well-known text (WKT), well-known binary (WKB), and the GeoJSON formats are examples of this. For instance, the world_wkt.csv file has a column named WKT representing polygons of the world’s countries. To import and convert it to a GeoDataFrame, we can apply the shapely.wkt.loads function (Section 1.2.5) on WKT strings, to convert them into shapely geometries:\n\nworld_wkt = pd.read_csv('data/world_wkt.csv')\nworld_wkt['geometry'] = world_wkt['WKT'].apply(shapely.wkt.loads)\nworld_wkt = gpd.GeoDataFrame(world_wkt)\nworld_wkt.plot();\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNot all of the supported vector file formats store information about their coordinate reference system. In these situations, it is possible to add the missing information using the .set_crs function. Please refer also to Section 6.4 for more information.\n\n\nAs a final example, we will show how geopandas also reads KML files. A KML file stores geographic information in XML format—a data format for the creation of web pages and the transfer of data in an application-independent way (Nolan and Lang 2014 …). Here, we access a KML file from the web. First, we need to “activate” the KML driver, which isn’t available by default (see above):\n\nfiona.supported_drivers['KML'] = 'r'\n\nThis file contains more than one layer. To list the available layers, we can use the fiona.listlayers function:\n\nu = 'https://developers.google.com/kml/documentation/KML_Samples.kml'\nfiona.listlayers(u)\n\n['Placemarks',\n 'Highlighted Icon',\n 'Paths',\n 'Google Campus',\n 'Extruded Polygon',\n 'Absolute and Relative']\n\n\nFinally, we can choose the first layer Placemarks and read it, using gpd.read_file with an additional layer argument:\n\ngpd.read_file(u, layer='Placemarks')\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      geometry\n    \n  \n  \n    \n      0\n      Simple placemark\n      Attached to the ground. Intelli...\n      POINT Z (-122.08220 37.42229 0....\n    \n    \n      1\n      Floating placemark\n      Floats a defined distance above...\n      POINT Z (-122.08407 37.42200 50...\n    \n    \n      2\n      Extruded placemark\n      Tethered to the ground by a cus...\n      POINT Z (-122.08577 37.42157 50...\n    \n  \n\n\n\n\n\n\n7.7.2 Raster data\nSimilar to vector data, raster data comes in many file formats with some of them supporting multilayer files. rasterio.open is used to create a file connection to a raster file, which can be subsequently used to read the metadata and/or the values, as shown previously (Section 1.3.2). For example:\n\nsrc = rasterio.open('data/srtm.tif')\nsrc\n\n<open DatasetReader name='data/srtm.tif' mode='r'>\n\n\nAll of the previous examples read spatial information from files stored on your hard drive. However, GDAL also allows reading data directly from online resources, such as HTTP/HTTPS/FTP web resources. The only thing we need to do is to add a /vsicurl/ prefix before the path to the file. Let’s try it by connecting to the global monthly snow probability at 500 m resolution for the period 2000-2012 (T. Hengl 2021 add reference…). Snow probability for December is stored as a Cloud Optimized GeoTIFF (COG) file (see Section 7.6). To read an online file, we just need to provide its URL together with the /vsicurl/ prefix:\n\nurl = \"/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif\"\nsrc = rasterio.open(url)\nsrc\n\n<open DatasetReader name='/vsicurl/https://zenodo.org/record/5774954/files/clm_snow.prob_esacci.dec_p.90_500m_s0..0cm_2000..2012_v2.0.tif' mode='r'>\n\n\nIn the example above rasterio.open creates a connection to the file without obtaining any values, as we did for the local srtm.tif file. The values can read, into an ndarray, using the .read method of the file connection (Section 1.3.2). This allows us also to just read a small portion of the data without downloading the entire file. This is very useful when working with large datasets hosted online from resource-constrained computing environments such as laptops.\nAnother option is to extract raster values at particular points, directly from the file connection, using the .sample method (see Section 3.4.1). For example, we can get the snow probability for December in Reykjavik (70%) by specifying its coordinates and applying .sample:\n\nvalues = src.sample([(-21.94, 64.15)])\nlist(values)\n\n[array([70], dtype=uint8)]\n\n\nThe example above efficiently extracts and downloads a single value instead of the entire GeoTIFF file, saving valuable resources. The /vsicurl/ prefix also works for vector file formats, enabling you to import datasets from online storage with geopandas just by adding it before the vector file URL.\nImportantly, /vsicurl/ is not the only prefix provided by GDAL—many more exist, such as /vsizip/ to read spatial files from ZIP archives without decompressing them beforehand or /vsis3/ for on-the-fly reading files available in AWS S3 buckets. You can learn more about it at https://gdal.org/user/virtual_file_systems.html."
  },
  {
    "objectID": "08-read-write-plot.html#sec-data-output",
    "href": "08-read-write-plot.html#sec-data-output",
    "title": "7  Geographic data I/O",
    "section": "7.8 Data output (O)",
    "text": "7.8 Data output (O)\nWriting geographic data allows you to convert from one format to another and to save newly created objects for permanent storage. Depending on the data type (vector or raster), object class (e.g., GeoDataFrame), and type and amount of stored information (e.g., object size, range of values), it is important to know how to store spatial files in the most efficient way. The next two sections will demonstrate how to do this.\n\n7.8.1 Vector data\nThe counterpart of gpd.read_file is the .to_file method that a GeoDataFrame has. It allows you to write GeoDataFrame objects to a wide range of geographic vector file formats, including the most common, such as .geojson, .shp and .gpkg. Based on the file name, .to_file decides automatically which driver to use. The speed of the writing process depends also on the driver.\n\nworld.to_file('output/world.gpkg')\n\nNote: if you try to write to the same data source again, the function will overwrite the file:\n\nworld.to_file('output/world.gpkg')\n\nInstead of overwriting the file, we could add a new layer to the file with mode='a' (“append” mode, as opposed to the default mode='w' for “write” mode). Appending is supported by several spatial formats, including GeoPackage. For example:\n\nworld.to_file('output/world_many_features.gpkg')\nworld.to_file('output/world_many_features.gpkg', mode='a')\n\nHere, world_many_features.gpkg will contain a polygonal layer named world with two “copies” of each country (that is 177×2=354 features, whereas the world layer has 177 features).\nAlternatively, you can create another, separate, layer, within the same file. The GeoPackage format also supports multiple layers within one file. For example:\n\nworld.to_file('output/world_many_layers.gpkg')\nworld.to_file('output/world_many_layers.gpkg', layer='world2')\n\nIn this case, world_many_layers.gpkg has two “layers”, world_many_layers (same as the file name, when layer is unspecified) and world2. Incidentally, the contents of the two layers is identical, but this doesn’t have to be so. Each layer from such a file can be imported separately, as in:\n\ngpd.read_file('output/world_many_layers.gpkg', layer='world_many_layers').head(1)\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.96\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n  \n\n1 rows × 11 columns\n\n\n\n\ngpd.read_file('output/world_many_layers.gpkg', layer='world2').head(1)\n\n\n\n\n\n  \n    \n      \n      iso_a2\n      name_long\n      continent\n      ...\n      lifeExp\n      gdpPercap\n      geometry\n    \n  \n  \n    \n      0\n      FJ\n      Fiji\n      Oceania\n      ...\n      69.96\n      8222.253784\n      MULTIPOLYGON (((-180.00000 -16....\n    \n  \n\n1 rows × 11 columns\n\n\n\n…\n\n\n7.8.2 Raster data\n…"
  },
  {
    "objectID": "08-read-write-plot.html#sec-visual-outputs",
    "href": "08-read-write-plot.html#sec-visual-outputs",
    "title": "7  Geographic data I/O",
    "section": "7.9 Visual outputs",
    "text": "7.9 Visual outputs"
  },
  {
    "objectID": "08-read-write-plot.html#exercises",
    "href": "08-read-write-plot.html#exercises",
    "title": "7  Geographic data I/O",
    "section": "7.10 Exercises",
    "text": "7.10 Exercises"
  },
  {
    "objectID": "09-mapping.html#introduction",
    "href": "09-mapping.html#introduction",
    "title": "8  Making maps with Python",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\n\nGeopandas explore has been used in previous chapters.\nWhen to focus on visualisation? At the end of geographic data processing workflows.\n\n\n\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\nimport rasterio.plot\nnz = gpd.read_file(\"data/nz.gpkg\")"
  },
  {
    "objectID": "09-mapping.html#static-maps",
    "href": "09-mapping.html#static-maps",
    "title": "8  Making maps with Python",
    "section": "8.2 Static maps",
    "text": "8.2 Static maps\n\nFocus on matlibplot\nFirst example: NZ with fill and borders\nScary matplotlib code here…\n\n\nnz.plot(color=\"grey\");\nnz.plot(color=\"none\", edgecolor=\"blue\");\nnz.plot(color=\"grey\", edgecolor=\"blue\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs covered in Chapter 2, you can plot raster datasets as follows:\n\nnz_elev = rasterio.open('data/nz_elev.tif')\nrasterio.plot.show(nz_elev);\n\n\n\n\n\nYou can combine the raster and vector plotting methods shown above into a single visualisation with multiple layers as follows:\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nrasterio.plot.show(nz_elev, ax=ax)\nnz.to_crs(nz_elev.crs).plot(ax=ax, facecolor='none', edgecolor='r');\n\n\n\n\n\n8.2.1 Palettes\n\n\n8.2.2 Layers\n\n\n8.2.3 Faceted maps\n\n\n8.2.4 Exporting maps as images"
  },
  {
    "objectID": "09-mapping.html#interactive-maps",
    "href": "09-mapping.html#interactive-maps",
    "title": "8  Making maps with Python",
    "section": "8.3 Interactive maps",
    "text": "8.3 Interactive maps\n\nWhen are interactive maps useful\n\nAn interactive map is an important way to understand and interpret complex geographical information. A good interactive map enables movement across the map area, change the area of interest and provide additional context or text information. In this section we will look an interactive map based of national public transport access nodes (NaPTAN), the UK Department for Transport repository of public transport point-of-interest in England, Scotland and Wales consisting of: - bus stops and railway stations - tram, metro and underground stops - airports and ferry terminals\nWe will show how to create this may restricted to railway stations, tram stops and ferry terminals in Yorkshire. This will also match data to the National Rail customer reservation code (CRS) and timing point location (TIPLOC) attributes used in the the national rail timetable.\nIn the first code block we define a function get_databuffer that uses the requests library to download the NaPTAN data-set in CSV format to a StringIO buffer.\n\nimport io\nimport requests\n\ndef get_databuffer(uri, encoding='UTF-8'):\n    \"\"\"Download data from URI and returns as an StringIO buffer\"\"\"\n    r = requests.get(uri, timeout=10)\n    return io.StringIO(str(r.content, encoding))\n\n# NaPTAN data service\nURI='https://multiple-la-generator-dot-dft-add-naptan-prod.ew.r.appspot.com/v1/access-nodes?dataFormat=csv'\nBUFFER = get_databuffer(URI)\n\nWe then read the in-memory string-buffer into a Panda data-frame, treating the buffer as if it were a CSV file. We then extract the location data into a numpy two-dimensional array.\n\nimport pandas as pd\n\nDF1 = pd.read_csv(BUFFER, low_memory=False)\nDATA = DF1[['Longitude', 'Latitude']].values\n\nWe then convert the \\(transposed data-array\\) into a GeoSeries and use this to create a GeoDataFrame. Which we then tidy by dropping any columns that only contain invalid (pd.NA) values.\n\nimport geopandas as gpd\n\nPOINTS = gpd.points_from_xy(*DATA.T, crs='WGS84')\nNaPTAN = gpd.GeoDataFrame(data=DF1, geometry=POINTS)\n\nNaPTAN = NaPTAN.dropna(how='all', axis=1)\n\nThe next step is to create the timing-point TIPLOC data based on the StopType and a subset of the ATCOCode columns.\n\nNaPTAN['TIPLOC'] = ''\n# Heavy railway stations\nIDX1 = NaPTAN['StopType'] == 'RLY'\nNaPTAN.loc[IDX1, 'TIPLOC'] = NaPTAN['ATCOCode'].str[4:]\n\n# Ferrys\nIDX1 = NaPTAN['StopType'] == 'FER'\nNaPTAN.loc[IDX1, 'TIPLOC'] = NaPTAN['ATCOCode'].str[4:]\n\n# Metro and trams\nIDX1 = NaPTAN['StopType'] == 'MET'\nNaPTAN.loc[IDX1, 'TIPLOC'] = NaPTAN['ATCOCode'].str[6:]\n\nWe extract the heavy and light rail, or ferry locationsFrom the 435,298 rows in the NaPTAN data-frame.\n\nIDX1 = NaPTAN['StopType'].isin(['RLY', 'FER', 'MET'])\nSTATIONS = NaPTAN[IDX1]\n\nFilter columns and drop points within Yorkshire.\n\nFIELDS = ['ATCOCode', 'CommonName', 'ShortCommonName', 'LocalityName',\n          'StopType', 'Status', 'TIPLOC', 'geometry']\n\n# Clean up data-frame columns\nSTATIONS = STATIONS[FIELDS]\n\nYORKSHIRE = gpd.read_file('data/yorkshire.json').iloc[0, 0]\nIDX = STATIONS.within(YORKSHIRE)\n\nSTATIONS = STATIONS[IDX]\n\n# Write to GeoJSON\nSTATIONS.to_file('stations.geojson', driver='GeoJSON')\n# Write file to GeoPackage\n\nOUTPUT = STATIONS.copy()\nCRS = 'EPSG:32630'\nOUTPUT['geometry'] = OUTPUT['geometry'].to_crs(CRS)\nOUTPUT.to_file('stations.gpkg', driver='GPKG', layer='stations')\n\n\nHoloviews: facetted plotting\nPanel: allows you to create applications/dashboards\n\n\n8.3.1 GeoPandas explore\n\n\n8.3.2 Layers\n\n\n8.3.3 Publishing interactive maps\n\n\n8.3.4 Linking geographic and non-geographic visualisations"
  },
  {
    "objectID": "09-mapping.html#exercises",
    "href": "09-mapping.html#exercises",
    "title": "8  Making maps with Python",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises"
  }
]